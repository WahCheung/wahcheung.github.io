<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Apollo Routing路径导航]]></title>
    <url>%2F2020%2F04%2F07%2Fapollo-routing%2F</url>
    <content type="text"><![CDATA[Routing模块概述Routing模块给自动驾驶系统提供全局的路径导航功能，类似于百度地图/高德地图的导航。在收到路由请求之后，为了完成导航功能，Routing模块使用了专用的路由地图routing_map。最终输出的是自动驾驶车辆在从出发点到目的地的过程中经过的所有路段。 路由请求的形式Apollo路由请求实现的功能与百度地图/高德地图的导航类似，都是完成从起点到目的地的行驶路段规划，但Apollo Routing模块支持添加多个必经点(Waypoint/航点)，最终输出的路由结果会依次经过这些航点。举个例子，你想从深圳(起点)开车到广州(终点)，你只需要两个航点深圳-&gt;广州；如果你想先途径东莞再到广州，你需要输入的航点的顺序是深圳-&gt;东莞-&gt;广州。Apollo中的航点指定需要经过某条Lane的指定位置。Apollo中路由请求的具体形式如下： 123456789101112131415161718192021222324252627282930message LaneWaypoint &#123; // Note: waypoint所在的Lane的Id optional string id = 1; // Note: waypoint相对于Lane起点的距离 optional double s = 2; // Note: waypoint的坐标 optional apollo.common.PointENU pose = 3;&#125;message LaneSegment &#123; // Note: 所在Lane的Id optional string id = 1; // Note: Segment起始位置的s(相对于所在Lane的起点) optional double start_s = 2; // Note: Segment结束位置的s(相对于所在Lane的起点) optional double end_s = 3;&#125;message RoutingRequest &#123; optional apollo.common.Header header = 1; // at least two points. The first is start point, the end is final point. // The routing must go through each point in waypoint. repeated LaneWaypoint waypoint = 2; // Note: LaneSegment黑名单, 表示这些LaneSegment不能通过 repeated LaneSegment blacklisted_lane = 3; // Note: Road黑名单, 表示这些Road中的所有Lane都不能通过 repeated string blacklisted_road = 4; optional bool broadcast = 5 [default = true]; optional apollo.hdmap.ParkingSpace parking_space = 6;&#125; 导航器(Navigator)Navigator是路由搜索的实施类，完成具体的路径搜索。Navigator对象在构造时，会从routing_map文件中加载routing地图，然后对原始的routing地图信息进行一些加工&amp;整合，形成TopoGraph(对应Navigator中的graph_成员变量)。 1234567891011121314151617181920// Note: 加载原始(origin)拓扑地图Navigator::Navigator(const std::string&amp; topo_file_path) &#123; Graph graph; if (!cyber::common::GetProtoFromFile(topo_file_path, &amp;graph)) &#123; AERROR &lt;&lt; "Failed to read topology graph from " &lt;&lt; topo_file_path; return; &#125; graph_.reset(new TopoGraph()); // Note: 对base_map信息进行加工&amp;整合，形成拓扑图TopoGraph if (!graph_-&gt;LoadGraph(graph)) &#123; AINFO &lt;&lt; "Failed to init navigator graph failed! File path: " &lt;&lt; topo_file_path; return; &#125; black_list_generator_.reset(new BlackListRangeGenerator); result_generator_.reset(new ResultGenerator); is_ready_ = true; AINFO &lt;&lt; "The navigator is ready.";&#125; 拓扑图(TopoGraph)routing_map的数据结构类型为Graph(数据结构定义在topo_graph.proto中)，其中的Node表示一条语义地图中完整的Lane，而Edge表示Lane之间的拓扑关系。Routing模块并不直接使用Graph进行路由寻径，而使用了细节信息更丰富的TopoGraph。TopoGraph的全部信息都来源于Graph，但由于对原始的节点(Node)和连接边(Edge)信息进行了加工和整合，提供了许多额外的数据结构来方便后续的查找和搜索。通过TopoGraph，你可以查找一个Lane ID对应的TopoNode，可以查找某个TopoNode的OutEdge(InEdge)有那些，这些OutEdge(InEdge)的另一边又是连接到哪条Lane上的，还可以查找某个Road ID对应的一系列TopoNode等。这些TopoNode之间的拓扑关系表示的就是地图中Lane之间的关系。举个例子，在TopoGraph中TopoNode A(对应Lane A)有一条类型为LEFT的OutEdge连接到TopoNode B(对应Lane B)，表示在地图中Lane A的左边紧邻的就是Lane B。 拓扑图(TopoGraph)加载流程TopoGraph头文件如下： 123456789101112131415161718192021222324252627282930313233class TopoGraph &#123; public: TopoGraph() = default; ~TopoGraph() = default; bool LoadGraph(const Graph&amp; filename); const std::string&amp; MapVersion() const; const std::string&amp; MapDistrict() const; const TopoNode* GetNode(const std::string&amp; id) const; void GetNodesByRoadId( const std::string&amp; road_id, std::unordered_set&lt;const TopoNode*&gt;* const node_in_road) const; private: void Clear(); bool LoadNodes(const Graph&amp; graph); bool LoadEdges(const Graph&amp; graph); private: std::string map_version_; std::string map_district_; // Note: 存放所有从routing_map加载来的TopoNode // lane_id对应的TopoNode的下标可以通过lane_id查表node_index_map_得到 std::vector&lt;std::shared_ptr&lt;TopoNode&gt; &gt; topo_nodes_; // Note: TopoEdge列表 std::vector&lt;std::shared_ptr&lt;TopoEdge&gt; &gt; topo_edges_; // Note: lane_id对应的TopoNode在topo_nodes_中的下标 std::unordered_map&lt;std::string, int&gt; node_index_map_; // Note: road_id对应的TopoNode的集合 std::unordered_map&lt;std::string, std::unordered_set&lt;const TopoNode*&gt; &gt; road_node_map_;&#125;; 加载TopoGraph的过程比较简单，直接看代码和注释： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// Note: 加载拓扑地图(origin TopoNode和origin TopoEdge)bool TopoGraph::LoadGraph(const Graph&amp; graph) &#123; Clear(); map_version_ = graph.hdmap_version(); map_district_ = graph.hdmap_district(); if (!LoadNodes(graph)) &#123; AERROR &lt;&lt; "Failed to load nodes from topology graph."; return false; &#125; if (!LoadEdges(graph)) &#123; AERROR &lt;&lt; "Failed to load edges from topology graph."; return false; &#125; AINFO &lt;&lt; "Load Topo data successful."; return true;&#125;// Note: TopoNode初始化时会获取锚点, 对变道区间根据start_s进行排序// 计算最长的边界range, 并分别判断左右两边的最大变道区间是否足够用于变道(长度超过设定值)// TopoNode初始化时，还没有对有交集的变道区间进行合并bool TopoGraph::LoadNodes(const Graph&amp; graph) &#123; if (graph.node().empty()) &#123; AERROR &lt;&lt; "No nodes found in topology graph."; return false; &#125; for (const auto&amp; node : graph.node()) &#123; node_index_map_[node.lane_id()] = static_cast&lt;int&gt;(topo_nodes_.size()); std::shared_ptr&lt;TopoNode&gt; topo_node; // Note: TopoNode的构造函数做了不少东西，需要注意一下 topo_node.reset(new TopoNode(node)); road_node_map_[node.road_id()].insert(topo_node.get()); topo_nodes_.push_back(std::move(topo_node)); &#125; return true;&#125;// Need to execute load_nodes() firstly// Note: 根据拓扑地图中的Edge形成TopoEdge并添加对应TopoNode的InEdge和OutEdgebool TopoGraph::LoadEdges(const Graph&amp; graph) &#123; if (graph.edge().empty()) &#123; AINFO &lt;&lt; "0 edges found in topology graph, but it's fine"; return true; &#125; for (const auto&amp; edge : graph.edge()) &#123; const std::string&amp; from_lane_id = edge.from_lane_id(); const std::string&amp; to_lane_id = edge.to_lane_id(); if (node_index_map_.count(from_lane_id) != 1 || node_index_map_.count(to_lane_id) != 1) &#123; return false; &#125; std::shared_ptr&lt;TopoEdge&gt; topo_edge; TopoNode* from_node = topo_nodes_[node_index_map_[from_lane_id]].get(); TopoNode* to_node = topo_nodes_[node_index_map_[to_lane_id]].get(); topo_edge.reset(new TopoEdge(edge, from_node, to_node)); from_node-&gt;AddOutEdge(topo_edge.get()); to_node-&gt;AddInEdge(topo_edge.get()); topo_edges_.push_back(std::move(topo_edge)); &#125; return true;&#125; Routing搜索流程先说明一下路径搜索的大致流程。 获取waypoints所在的Lane对应的TopoNode，这样就把航点映射到TopoGraph中了。 添加黑名单。Apollo支持在RoutingRequest中添加黑名单，表示那些车道段(LaneSegment)和Road是不能通行的，这样做的意义在于可以避开拥堵路段/施工路段之类的无法通行的区域。 依次搜索前后两个航点之间的路径，最后把这些路径拼接起来。这样可以处理带有多个航点(waypoint)的路径搜索。通俗一点，查找深圳--&gt;东莞--&gt;广州的路径，就是搜索深圳--&gt;东莞和东莞--&gt;广州，然后把两段结果拼接起来。 将路径搜索的结果输出为RoutingResponse格式。 将航点映射到TopoGraph中的TopoNode获取waypoints对应的TopoNode，抽取waypoint在对应Lane中的s： 1234567891011bool Navigator::Init(const RoutingRequest&amp; request, const TopoGraph* graph, std::vector&lt;const TopoNode*&gt;* const way_nodes, std::vector&lt;double&gt;* const way_s) &#123; Clear(); // Note: 获取waypoints对应的TopoNode和waypoint在对应Lane中的s if (!GetWayNodes(request, graph_.get(), way_nodes, way_s)) &#123; AERROR &lt;&lt; "Failed to find search terminal point in graph!"; return false; &#125; // 省略。。。&#125; 1234567891011121314bool GetWayNodes(const RoutingRequest&amp; request, const TopoGraph* graph, std::vector&lt;const TopoNode*&gt;* const way_nodes, std::vector&lt;double&gt;* const way_s) &#123; for (const auto&amp; point : request.waypoint()) &#123; const auto* cur_node = graph-&gt;GetNode(point.id()); if (cur_node == nullptr) &#123; AERROR &lt;&lt; "Cannot find way point in graph! Id: " &lt;&lt; point.id(); return false; &#125; way_nodes-&gt;push_back(cur_node); way_s-&gt;push_back(point.s()); &#125; return true;&#125; 黑名单管理进行路径搜索前会添加在RoutingRequest中指定的黑名单，RoutingRequest指定黑名单的方式有两种： 指定不能通行的车道段(LaneSegment)。LaneSegment表示Lane的一部分。 指定不能通行的Road。该Road Id对应的Road包含的所有Lane都不能通行。 12345678910bool Navigator::Init(const RoutingRequest&amp; request, const TopoGraph* graph, std::vector&lt;const TopoNode*&gt;* const way_nodes, std::vector&lt;double&gt;* const way_s) &#123; // 省略。。。 // Note: 从request中获取黑名单 black_list_generator_-&gt;GenerateBlackMapFromRequest(request, graph_.get(), &amp;topo_range_manager_); return true;&#125; 12345678void BlackListRangeGenerator::GenerateBlackMapFromRequest( const RoutingRequest&amp; request, const TopoGraph* graph, TopoRangeManager* const range_manager) const &#123; AddBlackMapFromLane(request, graph, range_manager); AddBlackMapFromRoad(request, graph, range_manager); // Note: 对range_map_里面的TopoNode的NodeSRange进行排序, 然后进行区间合并 range_manager-&gt;SortAndMerge();&#125; TopoRangeManager管理着所有的黑名单，记录着TopoNode对应的黑名单区间NodeSRange。 12345678910111213141516class TopoRangeManager &#123; public: const std::unordered_map&lt;const TopoNode*, std::vector&lt;NodeSRange&gt;&gt;&amp; RangeMap() const; const std::vector&lt;NodeSRange&gt;* Find(const TopoNode* node) const; void Clear(); void Add(const TopoNode* node, double start_s, double end_s); void SortAndMerge(); private: // Note: TopoNode对应的一系列黑名单NodeSRange(不允许通行) // 在BlackListRangeGenerator中添加的NodeSRange黑名单 // 这里的TopoNode指针还是刚开始加载拓扑地图后的TopoNode的指针 std::unordered_map&lt;const TopoNode*, std::vector&lt;NodeSRange&gt;&gt; range_map_;&#125;; 拓扑子图(SubTopoGraph)进行拓扑图搜索时选择那些能同行的路段，而加入黑名单之后带来了一个新的问题，TopoGraph里面的TopoNode可能是带有黑名单LaneSegment的，不能直接使用了。可以想到的其中一种做法是，过滤黑名单路段，获得地图中可以通行的LaneSegment，根据这些可通行的LaneSegment建立TopoNode，重新考察TopoNode之间的TopoEdge，进而重建整个TopoGraph，在这个重建的TopoGraph中进行拓扑图搜索。这差不多就是最终的思路了，但这样做性能不会太好，假设进行带$N$个航点的路由搜索，没两个航点间的路由搜索都需要重建一次TopoGraph，共计需要重建$N-1$次TopoGraph，这个过程对于大地图非常耗时。Apollo中通过引入拓扑子图(SubTopoGraph)来解决这个问题。SubTopoGraph只关注那些带有黑名单的Lane对应的TopoNode以及搜索起点和搜索终点处的TopoNode，其他拓扑图信息仍然由原TopoGraph提供，省去了大量的工作。具体的方法需要结合后面的代码和说明去理解。 创建拓扑子图(SubTopoGraph)在注释中，我把TopoGraph中的TopoNode称为origin TopoNode，代表的是对应整条Lane的TopoNode，TopoGraph中的TopoEdge称为origin TopoEdge。把拓扑子图(SubTopoGraph)中创建的TopoNode称为子TopoNode，子图中创建的TopoEdge称为子TopoEdge。拓扑子图只管包含黑名单LaneSegment的Lane，将这些Lane中的黑名单剔除，提取出白名单(可供行驶的LaneSegment)，根据白名单创建子TopoNode。需要特别注意的是，在SubTopoGraph中创建的子TopoNode和子TopoEdge信息不会添加到TopoGraph中，TopoGraph的信息自从routing_map中加载完成后就不再改变了。为了将子图SubTopoGraph与TopoGraph联系起来，需要在拓扑子图(SubTopoGraph)中添加从子TopoNode连接到TopoGraph的TopoNode的连接信息。 12345678910111213141516171819202122232425SubTopoGraph::SubTopoGraph( const std::unordered_map&lt;const TopoNode*, std::vector&lt;NodeSRange&gt; &gt;&amp; black_map) &#123; std::vector&lt;NodeSRange&gt; valid_range; for (const auto&amp; map_iter : black_map) &#123; valid_range.clear(); // Note: 过滤TopoNode处于黑名单的NodeSRange, 从而获得TopoNode的有效NodeSRange GetSortedValidRange(map_iter.first, map_iter.second, &amp;valid_range); // Note: 为TopoNode的每个valid range都创建一个对应range的子TopoNode // 如果两个子TopoNode是紧邻的, 则在两个TopoNode之间添加Edge // 使得之前被分割的waypoint起点和终点前后的平行车道的子TopoNode重新连接起来了 InitSubNodeByValidRange(map_iter.first, valid_range); &#125; // Note: 创建黑名单中的origin TopoNode的有效子TopoNode的连接边(子TopoEdge) for (const auto&amp; map_iter : black_map) &#123; InitSubEdge(map_iter.first); &#125; // 与InitSubEdge里面判断能否变道的标准有一些不同, AddPotentialEdge的变道区间下限为3m // 把一些可能在InitSubEdge因变道区间不足而不被添加的边添加进来 for (const auto&amp; map_iter : black_map) &#123; AddPotentialEdge(map_iter.first); &#125;&#125; 拓扑子图(SubTopoGraph)中的TopoNodeSubTopoGraph中的TopoNode通过过滤origin TopoNode中的黑名单LaneSegment得到的白名单(Valid Range)来创建，表示一段可通行的LaneSegment。创建TopoNode时记录了该TopoNode表示的LaneSegment的区间范围([start_s_, end_s_])，还会找该LaneSegment的中心点(AnchorPoint/锚点)，并且计算该TopoNode的可变道区间列表。[注释: 后面通过AnchorPoint之间的距离来估算TopoNode之间的直线距离，作为A*搜索时从节点到终点的Heuristic Cost] 123456789101112131415161718192021222324252627282930313233343536373839404142// Note: TopoGraph从routing_map加载拓扑地图时TopoNode使用这种初始化方式// TopoNode初始化时会获取锚点, 对变道区间根据start_s进行排序// 计算最长的边界range, 并分别判断左右两边的最大变道区间是否足够用于变道// 变道区间此处未进行交集合并处理TopoNode::TopoNode(const Node&amp; node) : pb_node_(node), start_s_(0.0), end_s_(pb_node_.length()) &#123; CHECK(pb_node_.length() &gt; kLenghtEpsilon) &lt;&lt; "Node length is invalid in pb: " &lt;&lt; pb_node_.DebugString(); Init(); // origin_node_是否等于this是判断当前TopoNode是不是SubTopoGraph中的TopoNode的标志 origin_node_ = this;&#125;// Note: 这个构造函数在创建SubTopoGraph时使用TopoNode::TopoNode(const TopoNode* topo_node, const NodeSRange&amp; range) : TopoNode(topo_node-&gt;PbNode()) &#123; origin_node_ = topo_node; start_s_ = range.StartS(); end_s_ = range.EndS(); Init();&#125;// Note: 找TopoNode对应的LaneSegment中心线的中间点作为锚点// 找左右虚线边界最长的range, 并简单地通过长度判断是否足够用于lane changevoid TopoNode::Init() &#123; // Note: 将s range中间的点设置为anchor point if (!FindAnchorPoint()) &#123; AWARN &lt;&lt; "Be attention!!! Find anchor point failed for lane: " &lt;&lt; LaneId(); &#125; ConvertOutRange(pb_node_.left_out(), start_s_, end_s_, &amp;left_out_sorted_range_, &amp;left_prefer_range_index_); is_left_range_enough_ = (left_prefer_range_index_ &gt;= 0) &amp;&amp; left_out_sorted_range_[left_prefer_range_index_].IsEnoughForChangeLane(); ConvertOutRange(pb_node_.right_out(), start_s_, end_s_, &amp;right_out_sorted_range_, &amp;right_prefer_range_index_); is_right_range_enough_ = (right_prefer_range_index_ &gt;= 0) &amp;&amp; right_out_sorted_range_[right_prefer_range_index_] .IsEnoughForChangeLane();&#125; 拓扑子图(SubTopoGraph)中的TopoEdge子图TopoEdge的创建稍微复杂一点，处理逻辑是，对于每一个子TopoNode(简称Sub_A)，其所在的origin TopoNode称为A，TopoGraph中与A相连的TopoNode有多个(B,C,...等)，对每一个与A相连的origin TopoNode(假设是B)，根据变道条件判断Sub_A能否通过变道(或直行)与B的子TopoNode相连(假设为Sub_B)，如果可以连接，则创建子TopoEdge，将子TopoEdge信息添加到Sub_A和Sub_B的InEdge/OutEdge集合中。假如B没有子TopoNode(表示B没有黑名单)，那就判断Sub_A能否通过变道(或直行)与B相连，如果可以连接，则创建子TopoEdge，将子TopoEdge信息添加到Sub_A的InEdge/OutEdge集合中。注意，子TopoEdge信息始终没有添加到origin TopoNode的Edge信息中。 分段进行路径搜索Routing是依次搜索前后两个航点之间的路径，每一次搜索(每两个相邻航点之间的路径搜索)都会重新建立拓扑子图。每轮搜索的起点和终点处的Lane(以及同向车道)都被分割成两部分了，导致了每轮搜索所建立的子图是不一样的。搜索的起点(start)和终点(end)都是子TopoNode，因为way_start和way_end这两个origin TopoNode都通过AddBlackMapFromTerminal添加了一段长度为0的黑名单区间，从而将起点和终点处的TopoNode分割成两个子TopoNode了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182bool Navigator::SearchRouteByStrategy( const TopoGraph* graph, const std::vector&lt;const TopoNode*&gt;&amp; way_nodes, const std::vector&lt;double&gt;&amp; way_s, std::vector&lt;NodeWithRange&gt;* const result_nodes) const &#123; std::unique_ptr&lt;Strategy&gt; strategy_ptr; strategy_ptr.reset(new AStarStrategy(FLAGS_enable_change_lane_in_result)); result_nodes-&gt;clear(); std::vector&lt;NodeWithRange&gt; node_vec; // Note: 分段进行路由搜索 // Note: 在每轮循环中都建立拓扑子图，然后进行路由搜索 for (size_t i = 1; i &lt; way_nodes.size(); ++i) &#123; const auto* way_start = way_nodes[i - 1]; const auto* way_end = way_nodes[i]; double way_start_s = way_s[i - 1]; double way_end_s = way_s[i]; // Note: 黑名单管理 // topo_range_manager_只有从routing request中制定的LaneSegment和Road黑名单 // full_range_manager会在waypoint所在车道&amp;平行车道前后添加新的黑名单 TopoRangeManager full_range_manager = topo_range_manager_; // Note: 在当前Lane和平行车道Lane的 // way_start_s的后面(-1cm)和way_end_s的前面(+1cm)设置黑名单 // 但这里通过AddBlackMapFromTerminal加入的黑名单NodeSRange的长度都是0 // 举个例子, [3.21, 3.21] // Note: way_start这个TopoNode会产生两个子TopoNode， // 分别是[0, way_start_s - 1cm]和[way_start_s - 1cm, length] // 这里单纯就为了将waypoint所在Lane(以及同向车道)分割成两部分子TopoNode， // 从而获得搜索起点start和搜索终点end black_list_generator_-&gt;AddBlackMapFromTerminal( way_start, way_end, way_start_s, way_end_s, &amp;full_range_manager); // Note: 创建子图, 里面包含黑名单origin TopoNode的有效子TopoNode的信息和关联的TopoEdge信息 // 子图与origin TopoNode是相关联的, // 子图里面的TopoNode都是黑名单里面那些valid range生成的TopoNode(称为子TopoNode或sub TopoNode) // 在创建子TopoEdge时, 根据sub TopoNode所在的origin TopoNode连接的所有的connected origin TopoNode来创建, // 如果connected origin TopoNode没有sub TopoNode, // 则直接创建sub TopoEdge连接sub TopoNode和connected origin TopoNode // 如果connected origin TopoNode有sub TopoNode, // 则判断每个sub TopoNode是否可能与当前的sub TopoNode连接上(符合变道或直行的要求) // 总的来说, SubTopoGraph包含了所有的sub TopoNode和所有的sub TopoEdge, // 这些sub TopoEdge连接的其中一端肯定是sub TopoNode, 另一端则有可能是origin/sub TopoNode // 子TopoEdge的信息被添加到子TopoNode中, // 但origin TopoNode不会添加子TopoEdge到InEdge或OutEdge信息中 SubTopoGraph sub_graph(full_range_manager.RangeMap()); // Note: 起点处的子TopoNode // 这个子TopoNode包含了起点waypoint的点 const auto* start = sub_graph.GetSubNodeWithS(way_start, way_start_s); if (start == nullptr) &#123; AERROR &lt;&lt; "Sub graph node is nullptr, origin node id: " &lt;&lt; way_start-&gt;LaneId() &lt;&lt; ", s:" &lt;&lt; way_start_s; return false; &#125; // Note: 终点处的子TopoNode // 这个子TopoNode包含了终点处的waypoint const auto* end = sub_graph.GetSubNodeWithS(way_end, way_end_s); if (end == nullptr) &#123; AERROR &lt;&lt; "Sub graph node is nullptr, origin node id: " &lt;&lt; way_end-&gt;LaneId() &lt;&lt; ", s:" &lt;&lt; way_end_s; return false; &#125; std::vector&lt;NodeWithRange&gt; cur_result_nodes; if (!strategy_ptr-&gt;Search(graph, &amp;sub_graph, start, end, &amp;cur_result_nodes)) &#123; AERROR &lt;&lt; "Failed to search route with waypoint from " &lt;&lt; start-&gt;LaneId() &lt;&lt; " to " &lt;&lt; end-&gt;LaneId(); return false; &#125; node_vec.insert(node_vec.end(), cur_result_nodes.begin(), cur_result_nodes.end()); &#125; // Note: 合并多段waypoint之间的路由结果 // Note: 这里把被middle waypoints分割的LaneSegment重新组合成一段LaneSegment了 if (!MergeRoute(node_vec, result_nodes)) &#123; AERROR &lt;&lt; "Failed to merge route."; return false; &#125; return true;&#125; AStarStrategyApollo Routing模块使用经典的A*搜索来进行路径查找，关于A*的讲解网上可以找到非常多的资料。节点的代价函数由$F(x) = G(x) + H(x)$决定，$G$是从起点到达当前节点的实际代价，$H$是估计的从当前节点到终点的代价(通过欧式距离估算)。基本策略就是维持一个节点的优先队列，每次将代价最小($F$最小)的TopoNode出列。为了能够在搜索到终点时重建搜索的路径，需要将当前节点的来源节点记录下来(代码中的came_from_)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177// Note: 搜索两个waypoint(TopoNode)之间的可行路径// graph: 拓扑图// sub_graph: 子拓扑图(子图)// src_node: 起点waypoint所在的子TopoNode,// 一般来说，src_node区间范围是[start_s - 1cm, length]// dest_node: 终点waypoint所在的子TopoNode,// 一般来说，dest_node区间范围是[0, end_s + 1cm]// 最终输出从src_node到dest_node的NodeWithRange序列// 这个result_nodes记录的是从src_node到dest_node(可能)经过的LaneSegment// result_nodes中的NodeWithRange里面的TopoNode都是origin TopoNodebool AStarStrategy::Search(const TopoGraph* graph, const SubTopoGraph* sub_graph, const TopoNode* src_node, const TopoNode* dest_node, std::vector&lt;NodeWithRange&gt;* const result_nodes) &#123; Clear(); AINFO &lt;&lt; "Start A* search algorithm."; std::priority_queue&lt;SearchNode&gt; open_set_detail; SearchNode src_search_node(src_node); // Note: f = g + h, 起点的g = 0 src_search_node.f = HeuristicCost(src_node, dest_node); open_set_detail.push(src_search_node); open_set_.insert(src_node); g_score_[src_node] = 0.0; enter_s_[src_node] = src_node-&gt;StartS(); SearchNode current_node; // Note: 所有从当前node出发的edge std::unordered_set&lt;const TopoEdge*&gt; next_edge_set; std::unordered_set&lt;const TopoEdge*&gt; sub_edge_set; while (!open_set_detail.empty()) &#123; current_node = open_set_detail.top(); const auto* from_node = current_node.topo_node; // Note: 搜索到终点 if (current_node.topo_node == dest_node) &#123; // Note: 根据搜索的中间信息输出路径规划结果 if (!Reconstruct(came_from_, from_node, result_nodes)) &#123; AERROR &lt;&lt; "Failed to reconstruct route."; return false; &#125; return true; &#125; open_set_.erase(from_node); open_set_detail.pop(); if (closed_set_.count(from_node) != 0) &#123; // if showed before, just skip... continue; &#125; closed_set_.emplace(from_node); // if residual_s is less than FLAGS_min_length_for_lane_change, only move // forward // 根据剩余距离是否足够变道来筛选下一个节点 const auto&amp; neighbor_edges = (GetResidualS(from_node) &gt; FLAGS_min_length_for_lane_change &amp;&amp; change_lane_enabled_) ? from_node-&gt;OutToAllEdge() : from_node-&gt;OutToSucEdge(); double tentative_g_score = 0.0; next_edge_set.clear(); for (const auto* edge : neighbor_edges) &#123; sub_edge_set.clear(); // Note: 本质上只是获取从from_node出发的Edge sub_graph-&gt;GetSubInEdgesIntoSubGraph(edge, &amp;sub_edge_set); next_edge_set.insert(sub_edge_set.begin(), sub_edge_set.end()); &#125; // Note: 所有从当前node出发的edge for (const auto* edge : next_edge_set) &#123; const auto* to_node = edge-&gt;ToNode(); // Note: 已经访问过, 访问过的node的cost不会比当前的cost高 if (closed_set_.count(to_node) == 1) &#123; continue; &#125; if (GetResidualS(edge, to_node) &lt; FLAGS_min_length_for_lane_change) &#123; continue; &#125; tentative_g_score = g_score_[current_node.topo_node] + GetCostToNeighbor(edge); // Note: 如果是变道类型的Edge，两条道不是都走完全程的，节点自身的cost只取一半 // 通俗一点的说法就是，我从A变道到B，我的行驶路程并没有Lane A + Lane B的总长度这么长 // 大概就是一半这样子 if (edge-&gt;Type() != TopoEdgeType::TET_FORWARD) &#123; tentative_g_score -= (edge-&gt;FromNode()-&gt;Cost() + edge-&gt;ToNode()-&gt;Cost()) / 2; &#125; double f = tentative_g_score + HeuristicCost(to_node, dest_node); // Note: 如果发现to_node已经处于搜索边界且已知的cost比从当前节点出发的cost要小 if (open_set_.count(to_node) != 0 &amp;&amp; f &gt;= g_score_[to_node]) &#123; continue; &#125; // Note: 估计进入to_node时的位置 // if to_node is reached by forward, reset enter_s to start_s if (edge-&gt;Type() == TopoEdgeType::TET_FORWARD) &#123; enter_s_[to_node] = to_node-&gt;StartS(); &#125; else &#123; // else, add enter_s with FLAGS_min_length_for_lane_change // Note: 估算变道进入to_node的位置 double to_node_enter_s = (enter_s_[from_node] + FLAGS_min_length_for_lane_change) / from_node-&gt;Length() * to_node-&gt;Length(); // enter s could be larger than end_s but should be less than length to_node_enter_s = std::min(to_node_enter_s, to_node-&gt;Length()); // if enter_s is larger than end_s and to_node is dest_node if (to_node_enter_s &gt; to_node-&gt;EndS() &amp;&amp; to_node == dest_node) &#123; continue; &#125; enter_s_[to_node] = to_node_enter_s; &#125; g_score_[to_node] = f; SearchNode next_node(to_node); next_node.f = f; open_set_detail.push(next_node); // Note: 记录父节点 came_from_[to_node] = from_node; if (open_set_.count(to_node) == 0) &#123; open_set_.insert(to_node); &#125; &#125; &#125; AERROR &lt;&lt; "Failed to find goal lane with id: " &lt;&lt; dest_node-&gt;LaneId(); return false;&#125;搜索到终点后需要根据父节点信息重构完整的路径，这个Reconstruct的过程对变道(lane change)的节点做了调整，调整的意义在于选择更长的节点(Lane)作为变道的节点。``` cpp// Note: 在发生变道的路由段做处理，选择更长的节点作为变道区间bool AdjustLaneChange(std::vector&lt;const TopoNode*&gt;* const result_node_vec) &#123; if (result_node_vec-&gt;size() &lt; 3) &#123; return true; &#125; // Note: 从后往前检查，尝试将变道的地点往终点方向移动到更合适的位置(更长的Lane) if (!AdjustLaneChangeBackward(result_node_vec)) &#123; AERROR &lt;&lt; "Failed to adjust lane change backward"; return false; &#125; // Note: 从前往后检查，尝试将变道的地点往起点方向移动到更合适的位置(更长的Lane) if (!AdjustLaneChangeForward(result_node_vec)) &#123; AERROR &lt;&lt; "Failed to adjust lane change backward"; return false; &#125; return true;&#125;// Note: 根据came_from信息抽取完整的行车路径// Note: 对发生变道的路由段做处理，选择前方/后方更长的节点作为变道区间bool Reconstruct( const std::unordered_map&lt;const TopoNode*, const TopoNode*&gt;&amp; came_from, const TopoNode* dest_node, std::vector&lt;NodeWithRange&gt;* result_nodes) &#123; std::vector&lt;const TopoNode*&gt; result_node_vec; result_node_vec.push_back(dest_node); auto iter = came_from.find(dest_node); while (iter != came_from.end()) &#123; result_node_vec.push_back(iter-&gt;second); iter = came_from.find(iter-&gt;second); &#125; std::reverse(result_node_vec.begin(), result_node_vec.end()); // Note: 对发生变道的路由段做处理，选择前方/后方更长的节点作为变道区间 if (!AdjustLaneChange(&amp;result_node_vec)) &#123; AERROR &lt;&lt; "Failed to adjust lane change"; return false; &#125; result_nodes-&gt;clear(); for (const auto* node : result_node_vec) &#123; // Note: result_nodes中的TopoNode都是origin TopoNode result_nodes-&gt;emplace_back(node-&gt;OriginNode(), node-&gt;StartS(), node-&gt;EndS()); &#125; return true;&#125; Routing结果输出123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960bool Navigator::SearchRoute(const RoutingRequest&amp; request, RoutingResponse* const response) &#123; // Note: 打印routing request信息 if (!ShowRequestInfo(request, graph_.get())) &#123; SetErrorCode(ErrorCode::ROUTING_ERROR_REQUEST, "Error encountered when reading request point!", response-&gt;mutable_status()); return false; &#125; if (!IsReady()) &#123; SetErrorCode(ErrorCode::ROUTING_ERROR_NOT_READY, "Navigator is not ready!", response-&gt;mutable_status()); return false; &#125; std::vector&lt;const TopoNode*&gt; way_nodes; std::vector&lt;double&gt; way_s; // Note: 将routing request的waypoints信息转化为origin TopoNode和对应的s // 实际上就是去graph_里面去找waypoint的(Lane)id对应的TopoNode // 还添加了request指定的黑名单 if (!Init(request, graph_.get(), &amp;way_nodes, &amp;way_s)) &#123; SetErrorCode(ErrorCode::ROUTING_ERROR_NOT_READY, "Failed to initialize navigator!", response-&gt;mutable_status()); return false; &#125; std::vector&lt;NodeWithRange&gt; result_nodes; if (!SearchRouteByStrategy(graph_.get(), way_nodes, way_s, &amp;result_nodes)) &#123; SetErrorCode(ErrorCode::ROUTING_ERROR_RESPONSE, "Failed to find route with request!", response-&gt;mutable_status()); return false; &#125; if (result_nodes.empty()) &#123; SetErrorCode(ErrorCode::ROUTING_ERROR_RESPONSE, "Failed to result nodes!", response-&gt;mutable_status()); return false; &#125; // 把起点和终点的前后1cm的多余距离抹除了 result_nodes.front().SetStartS(request.waypoint().begin()-&gt;s()); result_nodes.back().SetEndS(request.waypoint().rbegin()-&gt;s()); // Note: result_nodes中的NodeWithRange的TopoNode都用的是原TopoGraph中的origin TopoNode // Note: 注意，这里传入的是routing request中黑名单建立的黑名单管理器 // 将SearchRouteByStrategy的搜索结果转化为RoutingResponse形式 if (!result_generator_-&gt;GeneratePassageRegion( graph_-&gt;MapVersion(), request, result_nodes, topo_range_manager_, response)) &#123; SetErrorCode(ErrorCode::ROUTING_ERROR_RESPONSE, "Failed to generate passage regions based on result lanes", response-&gt;mutable_status()); return false; &#125; SetErrorCode(ErrorCode::OK, "Success!", response-&gt;mutable_status()); PrintDebugData(result_nodes); return true;&#125; GeneratePassageRegion的过程比较繁琐，大致过程就是先将路径分割成passage，然后对passage进行扩展，最后将passage结果转化成RoutingResponse需要的形式。passage在这里的概念就是一系列头尾相连的Lane，例如A--&gt;B--&gt;C--&gt;D，中间不涉及换道。用短横线-代表Lane，下图描述了SearchRouteByStrategy的结果，结果中包含了4个passage，第一个passage包含6条头尾连接的Lane，第一个passage(的最后一条Lane)右转进入第二个passage(包含11条Lane)，第二个passage(的最后一条Lane)右转进入第三个passage(包含12条Lane),第三个passage(的最后一条Lane)左转进入第四个passage(包含10条Lane)。 12345------ ----------- ---------- ------------行驶方向&gt;&gt; 在SearchRouteByStrategy中对拓扑图进行搜索时，变道就是一个TopoNode通过LEFT/RIGHT类型的TopoEdge连接到下一个TopoNode，这种形式的连接导致了变道的产生，但也规定了变道只能从passage的最后一个Lane开始，然后变道到下一个passage的第一条Lane，就如同上面简单画图表现的结果。我们希望可以将变道区间进行扩展，可以尝试根据将passage的前后进行扩展(ResultGenerator类的ExtendPassages方法)。当然了，在passage尾部扩展的Lane必须是能变道到下一个passage的，而在passage头部扩展的Lane必须是可以从上一passage变道进入的。扩展完成后的结果可能是下面这样子，经过扩展的那些passage的Lane数量增加了。 123------- ------------ ---------- -------------- 最终输出到RoutingResponse的形式也有一点讲究，先看看RoutingResponse的proto定义， 1234567891011121314151617181920212223message Passage &#123; repeated LaneSegment segment = 1; // Note: whether the passage can lead to another passage in routing optional bool can_exit = 2; // Note: 当前passage如何进入下一个passage optional ChangeLaneType change_lane_type = 3 [default = FORWARD];&#125;message RoadSegment &#123; optional string id = 1; repeated Passage passage = 2;&#125;message RoutingResponse &#123; optional apollo.common.Header header = 1; repeated RoadSegment road = 2; optional Measurement measurement = 3; optional RoutingRequest routing_request = 4; // the map version which is used to build road graph optional bytes map_version = 5; optional apollo.common.StatusPb status = 6;&#125; RoutingResponse包含一个RoadSegment数组，RoadSegment包含一个Passage数组，而Passage包含一个LaneSegment数组。路径中目前的passage形式是如何转化为RoutingResponse中的RoadSegment形式的呢。代码可能略显复杂，但表达的意思比较直观，就是按照橫截面变化来对passage进行分割，然后形成RoadSegment。[创建路由结果由ResultGenerator类完成，代码在modules/routing/core目录下。] 123456|-----|--| |--|-------|---| |--|--------| |---|---------|--||R1 |R2|R3 |R4 |R5 |R6|R7 |行驶方向&gt;&gt; 上面的结果共形成了7块RoadSegment。实际上Apollo Routing模块最终输出的结果与上图现实稍有区别，上面的结果是为了方便理解其结果输出形式。不同之处在于，passage中那些直行的Lane都单独作为RoadSegment输出，如下， 123456|-|-|-|-|-|--| |--|-|-|-|-|-|-|-|---| |--|-|-|-|-|-|-|-|-| |---|-|-|-|-|-|-|-|-|-|--|| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |共32个RoadSegment行驶方向&gt;&gt; 以上就是Routing模块的输出形式。]]></content>
      <categories>
        <category>Apollo自动驾驶</category>
      </categories>
      <tags>
        <tag>Apollo</tag>
        <tag>Routing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apollo Routing地图]]></title>
    <url>%2F2020%2F02%2F19%2Fapollo-routing-map%2F</url>
    <content type="text"><![CDATA[Routing地图的作用Routing地图是从高精度语义地图抽象出来的初步的拓扑结构，这个拓扑结构的节点(Node)是可供行驶的Lane，而边(Edge)则表示Lane之间的拓扑关系。这个拓扑关系具体指的是，举个例子，Lane B是Lane A的右邻同向车道(B是A的right_neighbor_forward_lane)，则会有一条Edge从A指向B，这条Edge的DirectionType是RIGHT。Routing地图是Routing模块的基本信息来源，用于进行全局的路径搜索，类似于百度地图/高德地图的导航功能。 生成Routing地图 Routing地图(routing_map)是Routing导航的基础信息来源，根据高精度语义地图(base_map)中车道的拓扑结构转化而来。Routing地图通过脚本scripts/generate_routing_topo_graph.sh来创建，实际功能由GraphCreator类来完成(将base_map转为routing_map)，主体逻辑代码位于modules/routing/topo_creator目录。 Routing地图的结构Routing地图(routing_map)的数据结构定义(topo_graph.proto)如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475syntax = "proto2";package apollo.routing;import "modules/map/proto/map_geometry.proto";message CurvePoint &#123; // Note: 相对于Lane中心线起始位置的s距离 optional double s = 1;&#125;message CurveRange &#123; optional CurvePoint start = 1; optional CurvePoint end = 2;&#125;message Node &#123; // Note: Lane的id optional string lane_id = 1; // Note: Lane的长度 optional double length = 2; // Note: Lane的left_boundary的虚线部分的区间范围(变道区间) // 每一段CurveRange都是一个虚线区间 // 这些虚线区间是相对于道路的实线边界而言的， // 道路的实线边界不允许压线，而虚线边界是可以变道的 repeated CurveRange left_out = 3; // Note: Lane的right_boundary的虚线部分的区间范围(变道区间) // 每一段CurveRange都是一个虚线区间 // 这些虚线区间是相对于道路的实线边界而言的， // 道路的实线边界不允许压线，而虚线边界是可以变道的 repeated CurveRange right_out = 4; // Note: 代价由Lane的长度、speed_limit大小和turn类型决定 // 1. Lane越长耗时越多(也更消耗资源)，因此Lane长则cost大 // 2. speed limit越高cost越小, 同样里程的情况下， // 限速100km/h比限速30km/h的道路通过时间更快，cost因此更小 // 3. 不同类型的转弯车道根据routing_config有固定的cost // 左转/右转/U型转弯车道额外添加了固定的cost optional double cost = 5; // Note: Lane中心线 optional apollo.hdmap.Curve central_curve = 6; // Note: 虚拟的Lane形成的Node(十字路口的虚拟Lane), 这个字段无实际用途 optional bool is_virtual = 7 [default = true]; // Note: Lane所在的Road的id optional string road_id = 8;&#125;message Edge &#123; // Note: DirectionType指的是from_lane_id-&gt;to_lane_id的方向 enum DirectionType &#123; // Note: to_lane_id在from_lane_id的successor_id中 FORWARD = 0; // Note: to_lane_id在from_lane_id的left_neighbor_forward_lane_id中 LEFT = 1; // Note: to_lane_id在from_lane_id的right_neighbor_forward_lane_id中 RIGHT = 2; &#125; // Note: Edge起点 optional string from_lane_id = 1; // Note: Edge终点 optional string to_lane_id = 2; // Note: FORWARD类型的Edge的cost为0 // LEFT/RIGHT类型的Edge的cost与变道区间总长度呈负相关 optional double cost = 3; // Note: DirectionType指的是from_lane_id-&gt;to_lane_id的方向 optional DirectionType direction_type = 4;&#125;// Note: Routing地图由代表Lane的Node和表示Lane之间的连接关系的Edge组成message Graph &#123; optional string hdmap_version = 1; optional string hdmap_district = 2; repeated Node node = 3; repeated Edge edge = 4;&#125; 从语义地图创建Routing地图GraphCreator类完成创建Routing地图的工作，其主要的成员如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class GraphCreator &#123; public: GraphCreator(const std::string&amp; base_map_file_path, const std::string&amp; dump_topo_file_path, const RoutingConfig&amp; routing_conf); ~GraphCreator() = default; bool Create(); private: void InitForbiddenLanes(); std::string GetEdgeID(const std::string&amp; from_id, const std::string&amp; to_id); void AddEdge( const Node&amp; from_node, const ::google::protobuf::RepeatedPtrField&lt;hdmap::Id&gt;&amp; to_node_vec, const Edge::DirectionType&amp; type); static bool IsValidUTurn(const hdmap::Lane&amp; lane, const double radius); private: // Note: base_map的文件路径, base_map是创建Routing地图的信息来源 std::string base_map_file_path_; // Note: Routing地图的输出路径 std::string dump_topo_file_path_; // Note: base_map的内容 hdmap::Map pbmap_; // Note: 最终要输出的Routing地图的内容 Graph graph_; // Note: lane_id对应的Node的下标, 即Lane对应Node在graph_.node的下标 std::unordered_map&lt;std::string, int&gt; node_index_map_; // Note: 从地图中直接获取的所有Lane的id及其对应的Road的id, &lt;lane_id, road_id&gt; // 这里包含所有的Lane(含禁止通行的非机动车道等) // 用于查询Lane所在的Road std::unordered_map&lt;std::string, std::string&gt; road_id_map_; // Note: 已经添加到graph_中的Edge的id, 用于避免重复添加Edge std::unordered_set&lt;std::string&gt; showed_edge_id_set_; // Note: Lane's type is not CITY_DRIVING which is not allowed for driving std::unordered_set&lt;std::string&gt; forbidden_lane_id_set_; // Note: Routing地图的配置信息, 主要是一些对Node和Edge的cost做出规定的配置 const RoutingConfig&amp; routing_conf_;&#125;; GraphCreator主要通过Create成员函数来完成创建routing_map的功能： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114bool GraphCreator::Create() &#123; // Note: 加载base_map if (absl::EndsWith(base_map_file_path_, ".xml")) &#123; if (!hdmap::adapter::OpendriveAdapter::LoadData(base_map_file_path_, &amp;pbmap_)) &#123; AERROR &lt;&lt; "Failed to load base map file from " &lt;&lt; base_map_file_path_; return false; &#125; &#125; else &#123; if (!cyber::common::GetProtoFromFile(base_map_file_path_, &amp;pbmap_)) &#123; AERROR &lt;&lt; "Failed to load base map file from " &lt;&lt; base_map_file_path_; return false; &#125; &#125; AINFO &lt;&lt; "Number of lanes: " &lt;&lt; pbmap_.lane_size(); graph_.set_hdmap_version(pbmap_.header().version()); graph_.set_hdmap_district(pbmap_.header().district()); node_index_map_.clear(); road_id_map_.clear(); showed_edge_id_set_.clear(); // Note: 获取lane_id -&gt; road_id对应关系 for (const auto&amp; road : pbmap_.road()) &#123; for (const auto&amp; section : road.section()) &#123; for (const auto&amp; lane_id : section.lane_id()) &#123; road_id_map_[lane_id.id()] = road.id().id(); &#125; &#125; &#125; // Note: 将非CITY_DRIVING的land id添加到禁止通行的lane名单 // 因为这一类lane不允许机动车通行, 例如自行车道 InitForbiddenLanes(); const double min_turn_radius = VehicleConfigHelper::GetConfig().vehicle_param().min_turn_radius(); // Note: 添加拓扑地图的节点 // every valid city driving Lane as a Node // non-city-driving Lane and invalid u-turn Lanes are excluded for (const auto&amp; lane : pbmap_.lane()) &#123; const auto&amp; lane_id = lane.id().id(); if (forbidden_lane_id_set_.find(lane_id) != forbidden_lane_id_set_.end()) &#123; ADEBUG &lt;&lt; "Ignored lane id: " &lt;&lt; lane_id &lt;&lt; " because its type is NOT CITY_DRIVING."; continue; &#125; if (lane.turn() == hdmap::Lane::U_TURN &amp;&amp; !IsValidUTurn(lane, min_turn_radius)) &#123; ADEBUG &lt;&lt; "The u-turn lane radius is too small for the vehicle to turn"; continue; &#125; AINFO &lt;&lt; "Current lane id: " &lt;&lt; lane_id; // Note: Lane对应的Node的下标 node_index_map_[lane_id] = graph_.node_size(); const auto iter = road_id_map_.find(lane_id); if (iter != road_id_map_.end()) &#123; node_creator::GetPbNode(lane, iter-&gt;second, routing_conf_, graph_.add_node()); &#125; else &#123; AWARN &lt;&lt; "Failed to find road id of lane " &lt;&lt; lane_id; node_creator::GetPbNode(lane, "", routing_conf_, graph_.add_node()); &#125; &#125; // Note: 添加拓扑地图的连接边 for (const auto&amp; lane : pbmap_.lane()) &#123; const auto&amp; lane_id = lane.id().id(); if (forbidden_lane_id_set_.find(lane_id) != forbidden_lane_id_set_.end()) &#123; ADEBUG &lt;&lt; "Ignored lane id: " &lt;&lt; lane_id &lt;&lt; " because its type is NOT CITY_DRIVING."; continue; &#125; const auto&amp; from_node = graph_.node(node_index_map_[lane_id]); // Note: 直行的Edge的cost为0 AddEdge(from_node, lane.successor_id(), Edge::FORWARD); if (lane.length() &lt; FLAGS_min_length_for_lane_change) &#123; continue; &#125; // Note: 左变道或者右变道的Edge的cost与变道区间长度有关, // 小于设定值时, 变道区间越短cost越高 if (lane.has_left_boundary() &amp;&amp; IsAllowedToCross(lane.left_boundary())) &#123; AddEdge(from_node, lane.left_neighbor_forward_lane_id(), Edge::LEFT); &#125; if (lane.has_right_boundary() &amp;&amp; IsAllowedToCross(lane.right_boundary())) &#123; AddEdge(from_node, lane.right_neighbor_forward_lane_id(), Edge::RIGHT); &#125; &#125; if (!absl::EndsWith(dump_topo_file_path_, ".bin") &amp;&amp; !absl::EndsWith(dump_topo_file_path_, ".txt")) &#123; AERROR &lt;&lt; "Failed to dump topo data into file, incorrect file type " &lt;&lt; dump_topo_file_path_; return false; &#125; auto type_pos = dump_topo_file_path_.find_last_of(".") + 1; std::string bin_file = dump_topo_file_path_.replace(type_pos, 3, "bin"); std::string txt_file = dump_topo_file_path_.replace(type_pos, 3, "txt"); if (!cyber::common::SetProtoToASCIIFile(graph_, txt_file)) &#123; AERROR &lt;&lt; "Failed to dump topo data into file " &lt;&lt; txt_file; return false; &#125; AINFO &lt;&lt; "Txt file is dumped successfully. Path: " &lt;&lt; txt_file; if (!cyber::common::SetProtoToBinaryFile(graph_, bin_file)) &#123; AERROR &lt;&lt; "Failed to dump topo data into file " &lt;&lt; bin_file; return false; &#125; AINFO &lt;&lt; "Bin file is dumped successfully. Path: " &lt;&lt; bin_file; return true;&#125;]]></content>
      <categories>
        <category>Apollo自动驾驶</category>
      </categories>
      <tags>
        <tag>Apollo</tag>
        <tag>Routing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Samba文件共享]]></title>
    <url>%2F2019%2F09%2F19%2Fsamba-service%2F</url>
    <content type="text"><![CDATA[有时候需要在其他主机（Linux/Windows/Mac）上直接修改Linux服务器的数据，这是使用Samba搭建文件共享服务的笔记。 Samba安装12$ sudo apt-get update$ sudo apt-get install samba Samba用户在Linux主机上单独开设一个系统用户(share)用于Samba（以及vsftpd之类的）共享服务，为安全起见，禁止该用户远程登录机器。 123$ sudo mkdir /home/share$ sudo useradd -d /home/share -s /usr/sbin/nologin share$ sudo passwd share Samba服务维护了一份用户和密码名单，这些用户必须是已有的系统用户，但samba用户的密码与系统用户的密码是不一样的，通过smbpasswd命令设置。 通过下面命令给Samba的share用户设置Samba密码，在别的机器连接Samba共享文件夹时就是使用下面指定的用户和密码进行验证: 1$ sudo smbpasswd -a share Samba配置修改配置前先备份配置文件 1$ sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.bak 在/etc/samba/smb.conf末尾添加以下内容 12345678[share] comment = Share Folder path = /home/share browseable = yes writeable = yes read only = no create mask = 0644 directory mask = 0755 path为共享的目录，根据实际情况修改。对于path指定的目录，需要更改权限（sudo chmod 755 /home/share），否则会导致在其他机器上连接Samba后无法修改或者添加文件。]]></content>
      <tags>
        <tag>Samba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境下vsftpd的安装及配置]]></title>
    <url>%2F2019%2F09%2F17%2Fvsftpd-on-ubuntu%2F</url>
    <content type="text"><![CDATA[笔记，使用VSFTPD搭建FTP服务，只允许用户使用密码登录，支持上传和下载。 安装12$ sudo apt-get update$ sudo apt-get install vsftpd 安装后vsftpd -v检测到版本号为version 3.0.3，配置文件位于/etc/vsftpd.conf。 试过安装后找不到vsftpd.conf配置文件的，sudo apt-get remove --purge vsftpd卸载后重新安装就有了。 新建FTP用户新建一个名为share的用户特供FTP使用，禁止该用户通过telnet登录，指定该用户的主目录，准备将这个目录作为FTP的根目录。 123$ sudo mkdir /home/share$ sudo useradd -d /home/share -s /usr/sbin/nologin share$ sudo passwd share 配置先备份配置文件再做修改: 1$ sudo cp /etc/vsftpd.conf /etc/vsftpd.conf.bak 对vsftpd.conf做以下修改: 123456789101112131415161718192021222324# 新增这一行, 设置FTP根目录local_root=/home/share# 禁止匿名登录anonymous_enable=NO# 允许服务器本地用户登录local_enable=YES# 允许写入(支持上传)write_enable=YES# 目录和文件被创建时的初始权限(755)local_umask=022# 必须，否则提示530 Login incorrect.pam_service_name=ftp# 采用被动模式pasv_enable=YES# 端口范围下界pasv_min_port=5000# 端口范围上界pasv_max_port=6000 上面pam_service_name设置的解释可以参考 vsftpd 530 Login incorrect.如果使用的是阿里云的服务器，需要另外在阿里云服务器的安全组策略中添加对应的开放端口（端口21和pasv_min_port-pasv_max_port之间的端口）。 重启vsFTPd服务: 1$ sudo systemctl restart vsftpd.service 登录验证有几种方式可以验证vsFTPd是否安装配置正确，同时这也是连接FTP服务器的几种方法: 通过ftp命令进行登录验证 1$ ftp ip-address-of-ftp-server 浏览器打开地址ftp://ip-address-of-ftp-server，然后输入用户&amp;密码进行验证 对于Ubuntu系统，打开文件管理器，点击左侧栏的Connect to Server，在Server Address中输入ftp://ip-address-of-ftp-server，接着输入用户名和密码就可以连接到FTP共享目录。]]></content>
      <tags>
        <tag>vsftpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx服务的安装及配置]]></title>
    <url>%2F2019%2F09%2F12%2Fnginx-on-ubuntu%2F</url>
    <content type="text"><![CDATA[安装Nginx参考官方的安装说明少走弯路 Nginx官方安装指引 123$ sudo add-apt-repository "deb http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx"$ sudo apt-get update$ sudo apt-get install nginx nginx -v查看Nginx版本号为nginx/1.16.1。 Nginx服务相关命令查看Nginx服务状态1$ systemctl status nginx 启动/关闭/重启Nginx服务123$ sudo systemctl start nginx$ sudo systemctl stop nginx$ sudo systemctl restart nginx 重新加载Nginx配置修改Nginx服务配置文件后可以通过以下方式重新加载配置使修改生效，避免重启服务 1$ sudo systemctl reload nginx 利用Nginx搭建静态文件服务器Nginx的配置文件为/etc/nginx/nginx.conf，在这个配置文件中会通过include /etc/nginx/conf.d/*.conf;把别的配置项包含进来。笔者安装的版本在/etc/nginx/conf.d/目录下只有default.conf，因此相关配置都在nginx.conf和default.conf中。常用的配置项有几个，如下（注意，中间省略了一些配置）: 123456789101112server &#123; # 端口 listen 80; # 可修改为域名 server_name localhost; location / &#123; # 站点根目录 root /usr/share/nginx/html; index index.html index.htm; &#125;&#125; 在http配置（位于nginx.conf）中添加如下设置（注意，中间省略了已有的配置）: 1234567891011121314151617181920212223242526272829303132333435user nginx;worker_processes 1; error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; autoindex on; autoindex_localtime on; autoindex_exact_size off; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125; 重新加载配置，sudo systemctl reload nginx，浏览器打开http://ip-address:port可以看到Nginx站点根目录下的文件。]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用树莓派搭建自动驾驶小车]]></title>
    <url>%2F2018%2F12%2F25%2Fraspberry-pi-self-driving-car%2F</url>
    <content type="text"><![CDATA[一年多以前得知树莓派这样神奇的卡片式电脑的存在，于是入手了当作家用服务器来用。以前放在客厅里，室友不小心扯到电源线然后整个树莓派盒子掉水箱里了，电吹风吹一吹再自然晾干，竟然满血复活了！通过隔三叉四的知识积累，慢慢就知道了树莓派提供的丰富的GPIO口才是其精华，只当一个小主机来用实在是暴殄天物了，这个卡片式电脑配上那40个GPIO口，可以完成的作品超乎想象。后来萌生了做一个遥控车的想法，然后又想着，既然都能遥控了，为什么不让它自动驾驶呢。于是我买回来了小车底盘、4个直流减速电机（就是小时候玩的四驱车里面的那种小马达）、树莓派专用摄像头等零件，有了这些，我差不多就可以开始“闭门造车”了。 整个造车过程中，需要解决的问题比较多，比较细碎，关键是怎么将硬件和软件控制部分结合起来，收集什么样的样本进行训练，最后的自动驾驶用到的机器学习部分反而是最容易做到的，因为这一块已经有像Scikit-Learn和TensorFlow之类的成熟的机器学习库。 远程遥控由于需要采集无人驾驶小车的驾驶样本，在做一辆能够无人驾驶的小车前，需要先具备有人驾驶的能力，即遥控功能。遥控小车常用的有两种方案，一个是用红外遥控，另一个是使用键盘。目前两种方案都已实现。我先是尝试了红外遥控的方案，这需要一个红外遥控器和一个红外接收器，这个是差不多是整个造车过程中最波折的部分，但效果却差强人意。买回来的红外接收器通电之后直接烧坏了，原因是接线错误。于是又买了两个，运费比零件本身还要贵（红外接收器一个就一块多）。这次问商家要了电路图，总算没有接错线，终于可以开始摆弄软件部分，结果后面发现irw命令没有任何相应。中间停滞了一个星期，尝试了很多方法，还重装了系统却还是没法解决。最后的最后，发现这是树莓派Raspbian Stretch操作系统采用的LIRC驱动软件的问题，好在偶然在国外一个论坛上发现了解决方案。终于把红外遥控功能做出来了。然后将树莓派的网络从网线切换到WiFi，用充电宝给树莓派供电、电池盒给L298N电机驱动模块供电，实现了遥控小车的落地行走。不过由于红外线自身的特性（光波不能透过障碍物），遥控器和红外接收器要对着（跟电视遥控器一样），这使得红外遥控实践起来，操作不是很方便，整个人跟着小车跑，遥控器对着小车，没有了自由奔放的感觉…于是决定采用键盘遥控的方案。我特地买来了一个蓝牙键盘，想着可以通过这块蓝牙键盘直接连接树莓派进行遥控，但却没有成功，使用pygame库编写的程序完全没有接收到键盘的事件响应。其中的原因暂时没有去深究，不过很快想到了另外一种可行性比较高的方法：在树莓派上通过socket起一个服务(Server端)去接收其他主机(Client端)发送过来的指令，从而达到远程遥控的目的。而Client端就用我的笔记本电脑，通过键盘的上下左右键控制小车行进方向。我的蓝牙键盘直接连接笔记本电脑，笔记本电脑这边的程序监听着本地的键盘事件，将其发送到树莓派，树莓派接收到指令并进行响应，这就是Remote Control的整体流程。 硬件设备 设备 作用 树莓派 主控 L298N电机驱动模块 驱动直流电机(马达) 4个直流减速电机 控制轮子转动 带轮子的小车底盘 小车的外壳 充电宝&amp;18650电池盒 给树莓派和电机驱动板供电 树莓派摄像头 提供视野，采集样本 杜邦线若干 线路连接 上面是实现基本的自动驾驶功能所需要的主要零件，不过为了组装和固定，还需要摄像头支架、螺柱、螺丝之类的细碎零件。]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
        <tag>自动驾驶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派红外遥控模块]]></title>
    <url>%2F2018%2F11%2F27%2Fraspberry-pi-infrared-remote-controller%2F</url>
    <content type="text"><![CDATA[从红外遥控模块的安装到投入使用费了不少时间, 在此记录一下。我使用的是HX1838遥控模块, 第一次买回来插线上电, 由于接反了正负极, 红外接收器马上就啪一声烧掉了, 于是又在淘宝买了HX1838红外接收器。得到的教训是, 对于电子元器件还是要好好找店家要电路图, 切勿凭经验、感觉。接线的时候串联一个电阻也能起到保护电路的作用。红外遥控是得到Linux内核支持的, 我们需要安装必要的lirc库(LIRC全称Linux Infrared Remote Control), 然后通过几个文件配置红外遥控的相关参数, 可以认为, 遥控器不同按键发出的信号在接收端就表现为不一样的电信号序列, 我们需要设置这些信号序列对应了遥控器上的哪个键, 除此之外, 我们还可以自定义这些按键的行为。关于LIRC, 官网有很详细的Manual Welcome to the LIRC Manual LIRC安装123$ sudo apt-get update$ sudo apt-get install lirc$ sudo apt-get install liblircclient-dev 如果要使用pylirc, 必须要安装liblircclient-dev, 否则使用pip install python-lirc安装python-lirc时会出现以下错误: 1lirc/lirc.c:276:30: fatal error: lirc/lirc_client.h: No such file or directory 1$ sudo vim /etc/lirc/hardware.conf 添加以下内容: 12345678910111213141516# Arguments which will be used when launching lircdLIRCD_ARGS="--uinput"# Don't start lircmd even if there seems to be a good config file# START_LIRCMD=false# Don't start irexec, even if a good config file seems to exist.# START_IREXEC=false# Try to load appropriate kernel modulesLOAD_MODULES=true# Run "lircd --driver=help" for a list of supported drivers.DRIVER="default"# usually /dev/lirc0 is the correct setting for systems using udevDEVICE="/dev/lirc0"MODULES="lirc_rpi"# Default configuration files for your hardware if anyLIRCD_CONF=""LIRCMD_CONF="" 1$ sudo vim /etc/modules 添加以下两行, gpio_in_pin=18表示红外接收模块的信号从树莓派的GPIO口18Pin处输入, 此处18是采用BCM编码的端口编号。 12lirc_devlirc_rpi gpio_in_pin=18 1$ sudo vim /boot/config.txt 找到下面的设置, 把前面的#去掉, 取消注释。 1#dtoverlay=lirc-rpi 修改/boot/config.txt后需要重启树莓派: 1$ sudo shutdown -r now LIRC测试123$ sudo modprobe lirc_rpi$ sudo /etc/init.d/lircd stop$ sudo mode2 --driver default --device /dev/lirc0 按下一个键命令行会输出很多行, 大致如下: 12345678910111213Using driver default on device /dev/lirc0Trying device: /dev/lirc0Using device: /dev/lirc0Running as regular user pispace 16777215pulse 9135space 4457pulse 586space 549pulse 586space 549pulse 585...此处省略多行 国内外大部分的教程使用的命令都是下面这种, 但在我自己的板子上却不管用: 123$ sudo modprobe lirc_rpi$ sudo /etc/init.d/lirc stop$ sudo mode2 -d /dev/lirc0 我的系统路径/etc/init.d/下没有lirc, 而是lircd, 但这两个实质上没有区别, 实际上有哪个就对应修改一下就好。关键是上面第三行的命令, sudo mode2 -d /dev/lirc0在我的机器上输出的东西跟教程完全不一样, 如下: 12345pi@raspberrypi:~ $ mode2 -d /dev/lirc0Using driver devinput on device /dev/lirc0Trying device: /dev/lirc0Using device: /dev/lirc0Partial read 8 bytes on /dev/lirc0pi@raspberrypi:~ $ 这个在StackOverflow上有网友遇到一模一样的问题, 上面使用sudo mode2 --driver default --device /dev/lirc0的做法就是高票回答中给出的, 参考 LIRC partial read 8 bytes. 设置红外遥控键-值对应关系LIRC中的irrecord是用来生成遥控器配置文件的, 我们需要使用这个工具生成遥控器按键(红外信号)与自定义按键名的对应关系。 按键名不是随意设置的, 可以使用命令irrecord --list-namespace查看支持的按键名. 1$ irrecord -d /dev/lirc0 ~/lircd.conf 在出现提示后需要几次回车确认, 然后进入下面的设置流程: 123456789101112131415161718192021222324252627282930313233Enter name of remote (only ascii, no spaces) :controllerUsing controller.lircd.conf as output filenameHold down an arbitrary key................................................................................Found gap (512 us)Please enter the name for the next button (press &lt;ENTER&gt; to finish recording)KEY_NUMERIC_0Now hold down button "KEY_NUMERIC_0".Please enter the name for the next button (press &lt;ENTER&gt; to finish recording)KEY_NUMERIC_0Button KEY_NUMERIC_0 is already recorded, removing old data.Now hold down button "KEY_NUMERIC_0".Please enter the name for the next button (press &lt;ENTER&gt; to finish recording)KEY_NUMERIC_1Now hold down button "KEY_NUMERIC_1".Please enter the name for the next button (press &lt;ENTER&gt; to finish recording)KEY_NUMERIC_2Now hold down button "KEY_NUMERIC_2".Please enter the name for the next button (press &lt;ENTER&gt; to finish recording)KEY_NUMERIC_3Now hold down button "KEY_NUMERIC_3"....此处省略余下多个键的设置过程 在最后一个按键设置完成后再次按下回车键, 得到下面的设置成功提示: 1Successfully written config file controller.lircd.conf 打开这个controller.lircd.conf文件(controller的前缀就是我在上面设置的remote的名称了), 可以看到主体内容如下: 12345678910111213141516171819202122232425262728293031323334353637begin remote name controller driver devinput bits 56 eps 30 aeps 100 one 0 0 zero 0 0 pre_data_bits 72 pre_data 0x11 gap 512 toggle_bit_mask 0x0 frequency 38000 begin codes KEY_NUMERIC_0 0x69000001000265 KEY_NUMERIC_1 0x6B000001000263 KEY_NUMERIC_2 0x6B000001000262 KEY_NUMERIC_3 0x6D000001000262 KEY_NUMERIC_4 0x8400000100024A KEY_NUMERIC_5 0x6F000001000261 KEY_NUMERIC_6 0x72000001000262 KEY_NUMERIC_7 0x72000001000264 KEY_NUMERIC_8 0x73000001000263 KEY_NUMERIC_9 0x8A000001000263 KEY_NUMERIC_STAR 0x72000001000262 KEY_NUMERIC_POUND 0x71000001000260 KEY_LEFT 0x6F000001000260 KEY_RIGHT 0x6F000001000261 KEY_UP 0x85000001000262 KEY_DOWN 0x6D000001000261 KEY_OK 0x6C000001000261 end codesend remote 先做备份, 然后将刚刚生成的lircd.conf覆盖原来几乎为空的lircd.conf: 12$ sudo cp /etc/lirc/lircd.conf /etc/lirc/lircd.conf.backup$ sudo cp controller.lircd.conf /etc/lirc/lircd.conf 重启lirc服务即可完成所有的配置更新, 红外遥控器也就可以使用了。但是, 这样的红外模块还不能供程序使用, 我们还需要自定义lircrc文件, 这是lirc与程序交互的桥梁。 lircrc文件格式/etc/lirc/目录下的irexec.lircrc是一个设置的样板。 新建文件/etc/lirc/lircrc, 并添加以下内容: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101begin prog = irexec button = KEY_NUMERIC_0 config = echo "KEY_NUMERIC_0"endbegin prog = irexec button = KEY_NUMERIC_1 config = echo "KEY_NUMERIC_1"endbegin prog = irexec button = KEY_NUMERIC_2 config = echo "KEY_NUMERIC_2"endbegin prog = irexec button = KEY_NUMERIC_3 config = echo "KEY_NUMERIC_3"endbegin prog = irexec button = KEY_NUMERIC_4 config = echo "KEY_NUMERIC_4"endbegin prog = irexec button = KEY_NUMERIC_5 config = echo "KEY_NUMERIC_5"endbegin prog = irexec button = KEY_NUMERIC_6 config = echo "KEY_NUMERIC_6"endbegin prog = irexec button = KEY_NUMERIC_7 config = echo "KEY_NUMERIC_7"endbegin prog = irexec button = KEY_NUMERIC_8 config = echo "KEY_NUMERIC_8"endbegin prog = irexec button = KEY_NUMERIC_9 config = echo "KEY_NUMERIC_9"endbegin prog = irexec button = KEY_NUMERIC_STAR config = echo "KEY_NUMERIC_STAR"endbegin prog = irexec button = KEY_NUMERIC_POUND config = echo "KEY_NUMERIC_POUND"endbegin prog = irexec button = KEY_LEFT config = echo "KEY_LEFT"endbegin prog = irexec button = KEY_RIGHT config = echo "KEY_RIGHT"endbegin prog = irexec button = KEY_UP config = echo "KEY_UP"endbegin prog = irexec button = KEY_DOWN config = echo "KEY_DOWN"endbegin prog = irexec button = KEY_OK config = echo "KEY_OK"end 至此, 树莓派的红外安装配置已经完成。 红外遥控程序12 参考文献:[1]: Controlling your Pi with an infrared remote[2]: Raspberry Pi Pinout[3]: LIRC partial read 8 bytes[4]: LIRC Configuration guide[5]: The lircd.conf file format[6]: Raspberry Pi Zero Universal Remote]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
        <tag>红外遥控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派脉冲宽度调制]]></title>
    <url>%2F2018%2F11%2F27%2Fraspberry-pi-pwm%2F</url>
    <content type="text"><![CDATA[在给树莓派小车做加速和减速功能时遇到了脉冲宽度调制的问题，到底什么是脉冲宽度调制，跟智能小车直流电机的转速有什么关系？这几个问题没搞明白的话，感觉小车的控制程序很难继续往下写，于是搜集整理了一下资料。大概这也是不少非电子工程专业的同学在折腾树莓派GPIO时会遇到的问题。 我先（以目前浅显的理解）概括一下脉冲宽度调制（以下简称PWM）的作用：树莓派的GPIO口输出的是电子信号，二值状态的电子信号只有高电平(如3.3v)和低电平(0v)的区分，而我的直流伺服电机（小车轮子的小马达）在额定电压通电后的转速是100rpm（100转/分钟），那么我应该怎么让小车以50rpm的速度跑起来呢？假想你有一辆只能以100km/h速度行驶的小车，却要通过一条限速50km/h的街道，你怎么办？方法挺好理解，你只需要驾车走走停停就行了，例如开车一分钟然后停车一分钟，再开车一分钟然后停车一分钟，循环此操作直到通过街道。这样整体下来你的车速（差不多）就是50km/h了。但一分钟才换挡一次，在你开车的一分钟期间看到你开车路过的行人（或者是测速仪）明显能感觉出来你超速了。于是，你把换挡的间隔缩小到了1秒钟，也就是说，开车1秒停车1秒（周期为2秒），这样子，你就骗过了测速仪，成功地以50km/h的速度通过了街道。如果你手速够快，每秒钟可以完成“开车1秒停车1秒”这样的操作10次（即周期为0.1s，频率为10Hz），那么这天衣无缝的开车技术足以瞒天过海，骗过街边的路人甲乙丙丁。脉冲宽度调制的原理跟你上面开车的方法差不多，通过高电平与低电平的比例来模拟需要的电压。其中最重要的概念是占空比（Duty Cycle），即高电平时间占总时间的百分比（0 &lt;= dc &lt;= 1）。假设电信号恒定的高电平为10v，当占空比为0.25（即25%）时，模拟的就是2.5v的电压，占空比为0.5的话就是模拟5v的电压。通过上面的知识，通过PWM控制树莓派小车的速度的方法就出来了，假设在额定的输入电压情况下，转速为100rpm，那么只需要把PWM占空比设置为50%，就可以实现50rpm的转速。输入电压不变的情况下，电机转速与占空比成正比。 参考文献：[1]: Raspberry Pi Pulse Width Modulation (PWM)]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
        <tag>脉冲宽度调制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LevelDB编译、安装与使用]]></title>
    <url>%2F2018%2F10%2F16%2Fleveldb-installation%2F</url>
    <content type="text"><![CDATA[由于LevelDB现在用CMake编译, 网上看到的教程大都是直接make搞定的, 现在不管用了。从编译、安装到使用, 踩了一些坑, 在此记录下来。 CMake版本太低导致编译失败12345678$ git clone https://github.com/google/leveldb.git$ mkdir -p build &amp;&amp; cd build$ cmake -DCMAKE_BUILD_TYPE=Release .. &amp;&amp; cmake --build .CMake Error at CMakeLists.txt:5 (cmake_minimum_required): CMake 3.9 or higher is required. You are running version 3.5.1-- Configuring incomplete, errors occurred! CMake版本太低导致编译失败, 下面升级CMake。 升级安装CMake12$ cmake --versioncmake version 3.5.1 通过wget下载符合要求的版本(CMake 3.9 or higher): 1$ wget https://cmake.org/files/v3.12/cmake-3.12.3.tar.gz 解压、编译、安装: 1234$ tar -xf cmake-3.12.3.tar.gz$ cd cmake-3.12.3/$ ./configure$ sudo make &amp;&amp; sudo make install 12$ cmake -versioncmake version 3.12.3 编译、安装123$ cd leveldb/$ mkdir -p build &amp;&amp; cd build$ cmake -DCMAKE_BUILD_TYPE=Release .. &amp;&amp; cmake --build . 在build目录下生成了一个静态库libleveldb.a, 我们把这个静态库复制到/usr/local/lib/, 并把leveldb相关的头文件复制到/usr/local/include/ 12$ sudo cp build/libleveldb.a /usr/local/lib/$ sudo cp -r include/leveldb/ /usr/local/include/ 一个例子新建demo.cc: 1234567891011121314151617181920212223242526#include &lt;cassert&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;leveldb/db.h&gt;int main() &#123; leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; leveldb::Status status = leveldb::DB::Open(options, "/tmp/testdb", &amp;db); assert(status.ok()); std::string key = "apple"; std::string value = "A"; std::string get; leveldb::Status s = db-&gt;Put(leveldb::WriteOptions(), key, value); if (s.ok()) s = db-&gt;Get(leveldb::ReadOptions(), key, &amp;get); if (s.ok()) std::cout &lt;&lt; "读取到的与(key=" &lt;&lt; key &lt;&lt; ")对应的(value=" &lt;&lt; get &lt;&lt; ")" &lt;&lt; std::endl; else std::cout &lt;&lt; "读取失败!" &lt;&lt; std::endl; delete db; return 0;&#125; 编译、链接(注: 缺少-pthread选项会报错undefined reference to `pthread_create&#39;): 1$ g++ -o demo demo.cc -pthread -lleveldb -std=c++11 12$ ./demo读取到的与(key=apple)对应的(value=A) 参考文献:[1]: https://github.com/google/leveldb[2]: https://techoverflow.net/2012/12/14/compiling-installing-leveldb-on-linux/[3]: https://github.com/google/leveldb/blob/master/doc/index.md[4]: http://www.cnblogs.com/skynet/p/3372855.html[5]: https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>leveldb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim与YouCompleteMe代码补全]]></title>
    <url>%2F2018%2F09%2F23%2Fvim-as-an-ide%2F</url>
    <content type="text"><![CDATA[默认的Vim配置给人的感觉实在无法称得上是编辑器之神, 在使用简单的Vim配置好久之后, 终于有时间好好折腾一下Vim插件, 一番挣扎之后, 终于顺利为Vim加上代码自动补全的功能。 以下过程均基于Ubuntu 16.04, 不同的Linux发行版可能在安装YouCompleteMe时稍有差异, YouCompleteMe主页 (传送门) 有说明。 安装Vim和Git如果尚未安装git和vim, 使用下面命令安装: 123$ sudo apt-get update$ sudo apt-get install git$ sudo apt-get install vim 安装Vim插件管理工具Vundle使用Vundle作为插件管理工具 (Github Vundle传送门)[https://github.com/VundleVim/Vundle.vim] 1$ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 然后需要进行简单的配置, 参考Vundle的GitHub主页中的Quick Start, 编辑~/.vimrc文件, 在文件开头添加以下内容, 用于设置使用Vundle来管理插件: 1234567891011121314151617181920212223242526272829303132333435363738394041424344set nocompatible " 去除VI一致性,必须filetype off " 必须" 设置包括vundle和初始化相关的runtime pathset rtp+=~/.vim/bundle/Vundle.vimcall vundle#begin()" 另一种选择, 指定一个vundle安装插件的路径"call vundle#begin('~/some/path/here')" 让vundle管理插件版本,必须Plugin 'VundleVim/Vundle.vim'" 以下范例用来支持不同格式的插件安装." 请将安装插件的命令放在vundle#begin和vundle#end之间." Github上的插件" 格式为 Plugin '用户名/插件仓库名'Plugin 'tpope/vim-fugitive'" 来自 http://vim-scripts.org/vim/scripts.html 的插件" Plugin '插件名称' 实际上是 Plugin 'vim-scripts/插件仓库名' 只是此处的用户名可以省略Plugin 'L9'" 由Git支持但不再github上的插件仓库 Plugin 'git clone 后面的地址'Plugin 'git://git.wincent.com/command-t.git'" 本地的Git仓库(例如自己的插件) Plugin 'file:///+本地插件仓库绝对路径'Plugin 'file:///home/gmarik/path/to/plugin'" 插件在仓库的子目录中." 正确指定路径用以设置runtimepath. 以下范例插件在sparkup/vim目录下Plugin 'rstacruz/sparkup', &#123;'rtp': 'vim/'&#125;" 安装L9，如果已经安装过这个插件，可利用以下格式避免命名冲突Plugin 'ascenator/L9', &#123;'name': 'newL9'&#125;" 你的所有插件需要在下面这行之前call vundle#end() " 必须filetype plugin indent on " 必须 加载vim自带和插件相应的语法和文件类型相关脚本" 忽视插件改变缩进,可以使用以下替代:"filetype plugin on"" 简要帮助文档" :PluginList - 列出所有已配置的插件" :PluginInstall - 安装插件,追加 `!` 用以更新或使用 :PluginUpdate" :PluginSearch foo - 搜索 foo ; 追加 `!` 清除本地缓存" :PluginClean - 清除未使用插件,需要确认; 追加 `!` 自动批准移除未使用插件"" 查阅 :h vundle 获取更多细节和wiki以及FAQ" 将你自己对非插件片段放在这行之后 打开vim, 执行:PluginInstall, Vundle就会开始安装插件。 安装YouCompleteMe插件YouCompleteMe Github主页一开始我是通过直接在.vimrc中添加YouCompleteMe的插件行, 然后运行vim再执行:PluginInstall来让Vunble自动安装YouCompleteMe插件的, 但安装YouCompleteMe耗时太长, 看不到进度, 还出了点问题, 就改用下面手动克隆YouCompleteMe仓库的方法了: 12$ cd ~/.vim/bundle/$ git clone https://github.com/Valloric/YouCompleteMe.git 根据YouCompleteMe主页给出的安装指引, Install development tools, CMake, and Python headers: 1$ sudo apt install build-essential cmake python3-dev 更新子模块, 由于依赖比较多, 下面的命令会下载多个git仓库, 耗时比较长。 12$ cd ~/.vim/bundle/YouCompleteMe$ git submodule update --init --recursive YouCompleteMe Github主页给出的做法是直接使用python3 install.py --clang-completer命令进行安装, 但在我的机器上, 这个过程一直都没成功, 中间下载clang的时候time out了。于是手动安装clang: 12$ sudo apt-get update$ sudo apt-get install clang 然后使用YouCompleteMe自带的安装脚本的进行安装: 12$ cd ~/.vim/bundle/YouCompleteMe$ python3 install.py --clang-completer --system-libclang 新加入插件在.vimrc中的添加位置在上面Vundle的配置说明中已经说明白了, 现在添加: 1Plugin 'Valloric/YouCompleteMe' 运行vim, 执行:PluginInstall, 可以在左下角看到安装完成。 随便打开一个cpp文件, 有下面提示: 1NoExtraConfDetected: No .ycm_extra_conf.py file detected, so no compile flags are available. Thus no semantic support for C/C++/ObjC/ObjC++. 参考 YouCompleteMe issues 700, 在.vimrc中添加: 1let g:ycm_global_ycm_extra_conf = '~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/.ycm_extra_conf.py' 然而, 进入~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/, 然后ls -la查看目录文件, 所谓的.ycm_extra_conf.py并不存在! 索性进入YouCompleteMe目录进行文件查找: 12$ cd ~/.vim/bundle/YouCompleteMe$ find . -name ".ycm_extra_conf.py" 竟然找到不少, 如下: 123456789101112131415161718./python/ycm/tests/testdata/.ycm_extra_conf.py./.ycm_extra_conf.py./third_party/ycmd/examples/.ycm_extra_conf.py./third_party/ycmd/cpp/ycm/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/get_doc/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/noflags/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/driver_mode_cl/flag/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/driver_mode_cl/executable/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/client_data/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/test-include/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/clang/testdata/general_fallback/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/testdata/client/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/testdata/extra_conf/project/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/cs/testdata/testy-multiple-solutions/solution-not-named-like-folder/extra-conf-rel/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/cs/testdata/testy-multiple-solutions/solution-not-named-like-folder/extra-conf-abs/.ycm_extra_conf.py./third_party/ycmd/ycmd/tests/cs/testdata/testy-multiple-solutions/solution-not-named-like-folder/extra-conf-bad/testy/.ycm_extra_conf.py./third_party/ycmd/.ycm_extra_conf.py 看了几个, 个人感觉~/.vim/bundle/YouCompleteMe/third_party/ycmd/examples/.ycm_extra_conf.py的配置应该比较全, 就选择了这个, 把它复制到~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/目录下: 1$ cp ~/.vim/bundle/YouCompleteMe/third_party/ycmd/examples/.ycm_extra_conf.py ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/.ycm_extra_conf.py 记得在.vimrc中添加如下一行配置: 1let g:ycm_global_ycm_extra_conf = '~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/.ycm_extra_conf.py' .vimrc文件整个.vimrc文件如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687" 参考Vundle Github主页: https://github.com/VundleVim/Vundle.vim/blob/master/README_ZH_CN.mdset nocompatible " 去除VI一致性,必须filetype off " 必须" 设置包括vundle和初始化相关的runtime pathset rtp+=~/.vim/bundle/Vundle.vimcall vundle#begin()" 另一种选择, 指定一个vundle安装插件的路径"call vundle#begin('~/some/path/here')" 让vundle管理插件版本,必须Plugin 'VundleVim/Vundle.vim'Plugin 'Valloric/YouCompleteMe'" 以下范例用来支持不同格式的插件安装." 请将安装插件的命令放在vundle#begin和vundle#end之间." Github上的插件" 格式为 Plugin '用户名/插件仓库名'"Plugin 'tpope/vim-fugitive'" 来自 http://vim-scripts.org/vim/scripts.html 的插件" Plugin '插件名称' 实际上是 Plugin 'vim-scripts/插件仓库名' 只是此处的用户名可以省略"Plugin 'L9'" 由Git支持但不再github上的插件仓库 Plugin 'git clone 后面的地址'"Plugin 'git://git.wincent.com/command-t.git'" 本地的Git仓库(例如自己的插件) Plugin 'file:///+本地插件仓库绝对路径'"Plugin 'file:///home/gmarik/path/to/plugin'" 插件在仓库的子目录中." 正确指定路径用以设置runtimepath. 以下范例插件在sparkup/vim目录下"Plugin 'rstacruz/sparkup', &#123;'rtp': 'vim/'&#125;" 安装L9，如果已经安装过这个插件，可利用以下格式避免命名冲突"Plugin 'ascenator/L9', &#123;'name': 'newL9'&#125;" 你的所有插件需要在下面这行之前call vundle#end() " 必须filetype plugin indent on " 必须 加载vim自带和插件相应的语法和文件类型相关脚本" 忽视插件改变缩进,可以使用以下替代:"filetype plugin on"" 简要帮助文档" :PluginList - 列出所有已配置的插件" :PluginInstall - 安装插件,追加 `!` 用以更新或使用 :PluginUpdate" :PluginSearch foo - 搜索 foo ; 追加 `!` 清除本地缓存" :PluginClean - 清除未使用插件,需要确认; 追加 `!` 自动批准移除未使用插件"" 查阅 :h vundle 获取更多细节和wiki以及FAQ" 将你自己对非插件片段放在这行之后let g:ycm_global_ycm_extra_conf = '~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/.ycm_extra_conf.py'" 语法高亮syntax on" 显示行号set number" 搜索忽略大小写set ignorecase " 突出显示当前行set cursorline " 搜索高亮set hlsearch " 搜索逐字符高亮set incsearch " 自动缩进set autoindent " 使用C风格的缩进set cindent " 设置Tab键的行为" 将文件的Tab键显示为几个空格, 只做解析显示, 不进行实际转换set tabstop=2" 缩进宽度set shiftwidth=2" 将输入的Tab键转成空格set expandtab" 编辑时, 一个Tab表现为2个空格" Tab键等于2个空格宽度, 退格时把两个空格当作一个Tab删除set softtabstop=2" 高亮显示匹配的括号set showmatch]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Windows 10上运行Linux子系统]]></title>
    <url>%2F2018%2F08%2F28%2Flinux-on-windows%2F</url>
    <content type="text"><![CDATA[Windows 10出现了一类称为WSL(Windows Subsystem for Linux)的应用，使得Windows 10能够通过极其简单的方式安装并运行Linux发行版。详情查看微软官网对此的介绍Windows Subsystem for Linux Documentation 启用“适用于Linux的Windows子系统”功能打开Windows功能面板-&gt;启用或关闭Windows功能:启用“适用于Linux的Windows子系统”： 或者管理员权限打开PowerShell执行以下命令以启用“适用于Linux的Windows子系统”功能(&gt;为命令提示符，无需输入): 1&gt; Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 启用功能后重启系统让改动生效。若未启用此功能，打开ubuntu会提示以下错误: 12345Installing, this may take a few minutes...WslRegisterDistribution failed with error: 0x8007019eThe Windows Subsystem for Linux optional component is not enabled. Please enable it and try again.See https://aka.ms/wslinstall for details.Press any key to continue... 使用Windows应用商店安装Ubuntu在Windows 10应用商店搜索ubuntu或者linux，选择安装ubuntu。安装完成后点击图标启动，就会进入控制台窗口，会提示新建用户并输入密码: 123456789101112Installing, this may take a few minutes...Please create a default UNIX user account. The username does not need to match your Windows username.For more information visit: https://aka.ms/wslusersEnter new UNIX username: cheungEnter new UNIX password:Retype new UNIX password:passwd: password updated successfullyInstallation successful!To run a command as administrator (user "root"), use "sudo &lt;command&gt;".See "man sudo_root" for details.cheung@thinkpad-cheung:~$ 在Windows中的任意一个命令行窗口输入ubuntu都可以进入ubuntu子系统。后来，某次开机之后尝试进入ubuntu子系统，提示WslRegisterDistribution failed with error: 0x800703fa错误，这个问题在微软的GitHub上有对应的issue，见 Error: 0x800703fa - Press any key to continue。我在使用管理员打开Ubuntu之后就正常了，之后不需要管理员权限打开也没有问题。此外，重启也可以解决这个问题。 推荐使用推荐更换国内软件源默认的软件源一般是国外的ip，安装和更新软件的下载速度非常慢，国内用户推荐使用阿里云的软件源。先备份原始的软件源，然后替换为阿里云的软件源。有一点要特别注意，软件源的版本一定要和使用的Linux版本一致，如果在Ubuntu 18.x上使用了Ubuntu 16.x的软件源，在安装或更新软件时将会导致软件版本过高或过低而无法安装等问题。 推荐使用CmderWindows的命令行被诟病已久，界面也不怎么样，强烈推荐使用Cmder作为Windows命令行的替代品。Cmder支持在选中文本的情况下Ctrl+C进行复制，Ctrl+V粘贴，可以更换窗口背景图片、调整透明度等，感兴趣的去官网看看吧，传送门 Cmder网站，Cmder Github 后话这个Linux子系统与普通过的发行版还有有一点区别的，其体积也小很多，因此有一些正式发行版会自带的软件在这里却没有默认安装，例如我使用的这个版本，默认是没有gcc和g++的，需要的软件使用apt-get命令安装就好。这样轻量的处理好处也是明显的，占用的系统资源非常小，启动速度极其快，这也是相对于在Windows上使用Linux虚拟机的一大优势。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的VPS都用来干嘛了]]></title>
    <url>%2F2018%2F08%2F20%2Fcool-things-to-do-with-a-vps%2F</url>
    <content type="text"><![CDATA[目前在VPS上搭建了VPN服务器，Git服务器和FTP服务器，通过自己的VPN服务器实现了不限流量的科学上网。因为有很方便的开源工具，这几个服务的搭建都很简单。 搭建VPN服务器采用setup-ipsec-vpn安装过程非常简单，项目的主页也给出了非常详细的安装和配置过程: IPsec VPN 服务器一键安装脚本 1$ apt-get update &amp;&amp; apt-get dist-upgrade 1234$ wget https://git.io/vpnsetup -O vpnsetup.sh$ nano -w vpnsetup.sh[Replace with your own values: YOUR_IPSEC_PSK, YOUR_USERNAME and YOUR_PASSWORD]$ sudo sh vpnsetup.sh 搭建Git服务器我新开了一个用户(git)专门给Git使用的: 12$ groupadd git$ useradd -m git -g git 创建/repo目录，专门用于存放各种Git仓库: 12$ mkdir /repo$ chown git:git /repo 切换到git用户并新建仓库: 123$ su git$ cd /repo$ git init --bare leetcode.git 搭建FTP服务器12$ sudo apt-get update$ sudo apt-get install vsftpd 12$ ps -ef |grep vsftpdroot 14505 1 0 17:25 ? 00:00:00 /usr/sbin/vsftpd /etc/vsftpd.conf 1$ sudo vim /etc/vsftpd.conf 12anon_root=/mnt/ftp_home # 设置ftp主目录anonymous_enable=YES # 允许匿名用户登陆 修改设置后要重启服务: 1$ sudo service vsftpd restart]]></content>
      <categories>
        <category>自娱自乐</category>
      </categories>
      <tags>
        <tag>VPS</tag>
        <tag>VPN</tag>
        <tag>Git</tag>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL用户授权与访问控制]]></title>
    <url>%2F2018%2F08%2F14%2Fmysql-installation-and-access-privilege-system%2F</url>
    <content type="text"><![CDATA[这本来是一篇讲述怎么在Linux上完成MySQL的安装、新建用户并授权的博文，后来查阅了不少资料，看到了不少有趣的文章，就想着要把更多的细节作为笔记分享出来。 1mysql&gt; FLUSH PRIVILEGES; 也许你看到大多数讲解MySQL授权的文章最后都让你使用上面的命令来刷新MySQL的权限，但很多情况下可能都是毫无意义的(文末讲这个问题)。不求甚解的求知习惯是危险的，如果对一项技术有追求，应该花时间去了解背后的原理和逻辑。 MySQL安装参考 https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04 1$ sudo apt-get update 1$ sudo apt-get install mysql-server 扯远了，MySQL安装完之后一般就启动了，通过下面的命令检查MySQL服务的状态: 123456789$ systemctl status mysql.service● mysql.service - MySQL Community Server Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2018-08-14 10:14:28 CST; 2min 0s ago Process: 990 ExecStartPost=/usr/share/mysql/mysql-systemd-start post (code=exited, status=0/SUCCESS) Process: 976 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 989 (mysqld) CGroup: /system.slice/mysql.service └─989 /usr/sbin/mysqld 若MySQL没启动，通过以下命令启动: 1$ sudo systemctl start mysql 通过mysql_secure_installation安全向导修改root用户密码，设置是否允许root远程登陆以及决定是否删除匿名账号和测试数据库等。[MySQL开发者文档的传送门]: 1$ sudo mysql_secure_installation 在生产环境中，安全起见，建议设置root用户不允许远程登陆，如果只是学习测试用就无所谓了。此外，匿名账号最好删掉，不然容易出现奇奇怪怪的问题，如StackOverflow中提到的这个已授权的用户本地登陆后却出现MySQL Access Denied，高票答案中对这个问题有很详细的解答，感兴趣的读者可以打开看看。后来在MySQL的开发者文档中也看到了关于这个问题的说明: 传送门。这个问题总结起来就是：匿名账号由于指定了具体的host(localhost)，比使用通配符(%)的账号拥有了优先权(在user表中排在前面因此先被检查)，导致实名用户在本地登陆时实际上是以匿名用户身份登陆的。host是特定值的账号之所以在user表中排在前面，是因为这张表遵循了以下排序规则Access Control, Stage 1: Connection Verification: The server uses sorting rules that order rows with the most-specific Host values first. Literal host names and IP addresses are the most specific. (The specificity of a literal IP address is not affected by whether it has a netmask, so 198.51.100.13 and 198.51.100.0/255.255.255.0 are considered equally specific.) The pattern ‘%’ means “any host” and is least specific. The empty string ‘’ also means “any host” but sorts after ‘%’. Rows with the same Host value are ordered with the most-specific User values first (a blank User value means “any user” and is least specific). For rows with equally-specific Host and User values, the order is nondeterministic. 创建新用户并授权我们直接看看数据库中的user表的结构: 12345678910111213141516mysql&gt; describe mysql.user;+------------------------+-----------------------------------+------+-----+-----------------------+-------+| Field | Type | Null | Key | Default | Extra |+------------------------+-----------------------------------+------+-----+-----------------------+-------+| Host | char(60) | NO | PRI | | || User | char(32) | NO | PRI | | |......[此处省略多行]......| max_connections | int(11) unsigned | NO | | 0 | || max_user_connections | int(11) unsigned | NO | | 0 | || plugin | char(64) | NO | | mysql_native_password | || authentication_string | text | YES | | NULL | || password_expired | enum('N','Y') | NO | | N | || password_last_changed | timestamp | YES | | NULL | || password_lifetime | smallint(5) unsigned | YES | | NULL | || account_locked | enum('N','Y') | NO | | N | |+------------------------+-----------------------------------+------+-----+-----------------------+-------+ 由Host和User是主键可知，一个MySQL账号是由Host和User共同决定的。参考 MySQL开发者文档：Adding User Accounts，下面是开发者文档中给出的创建用户并授权的例子，虽然在下面的例子中，授权总是紧跟着创建用户的命令，你完全可以先创建用户但暂时不做任何授权。 12345678mysql&gt; CREATE USER 'finley'@'localhost' IDENTIFIED BY 'password';mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'finley'@'localhost' -&gt; WITH GRANT OPTION;mysql&gt; CREATE USER 'finley'@'%' IDENTIFIED BY 'password';mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'finley'@'%' -&gt; WITH GRANT OPTION;mysql&gt; CREATE USER 'admin'@'localhost' IDENTIFIED BY 'password';mysql&gt; GRANT RELOAD,PROCESS ON *.* TO 'admin'@'localhost'; 解释一下上面例子中的字符含义: localhost表示这个账号只能在本机登陆，%是通配符，表示可以在任意host登陆。由于我们已经知道一个账号是由User和Host共同决定的，因此上面例子中创建的‘finley’@’localhost’和‘finley’@’%’其实是两个不同的账号。*.*表示任意数据库的所有权限，WITH GRANT OPTION表示这个账号可以将具有的权限授予其他账号。可见，上面例子创建的这两个名为finley的用户都是超级用户。官方文档还给出了几个具体授权的例子: 123456789101112mysql&gt; CREATE USER 'custom'@'localhost' IDENTIFIED BY 'password';mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP -&gt; ON bankaccount.* -&gt; TO 'custom'@'localhost';mysql&gt; CREATE USER 'custom'@'host47.example.com' IDENTIFIED BY 'password';mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP -&gt; ON expenses.* -&gt; TO 'custom'@'host47.example.com';mysql&gt; CREATE USER 'custom'@'%.example.com' IDENTIFIED BY 'password';mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP -&gt; ON customer.* -&gt; TO 'custom'@'%.example.com'; SHOW GRANTS命令查看用户的权限 1mysql&gt; SHOW GRANTS FOR 'admin'@'localhost'; [注: SHOW GRANTS FOR CURRENT_USER可以查看当前用户的权限] 创建一个名为cheung的MySQL用户并授权: 12mysql&gt; CREATE USER 'cheung'@'%' IDENTIFIED BY 'password-for-cheung';mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'cheung'@'%'; 命令行可成功登陆: 1$ mysql -u cheung -p 尝试在Windows中通过Navicat远程连接，报错2003 - Can&#39;t connect to MySQL server on &#39;192.168.118.123&#39; (10038)如果在Linux上远程连接会得到如下错误提示[192.168.118.123是要连接的数据库所在的host]: 12$ mysql -h 192.168.118.123 -u cheung -pERROR 2003 (HY000): Can't connect to MySQL server on '192.168.118.123' (111) 上面的命令实际上无需在另外一台Linux服务器上运行，就在MySQL所在的服务器运行也会得到一样的错误结果。因为使用ip值192.168.118.123作为host已经不是本地连接[经过了路由]。通过mysql -h 127.0.0.1 -u cheung -p或者mysql -h localhost -u cheung -p则可以成功连接并登陆。由于&#39;cheung&#39;@&#39;%&#39;，基本可以确定这是MySQL只绑定了本机回环地址(loopback address)导致的问题。 12cheung@ubuntu:~$ netstat -anp |grep 3306tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN - 123$ cd /etc/mysql$ grep -rn "127.0.0.1"mysql.conf.d/mysqld.cnf:43:bind-address = 127.0.0.1 通过上面的grep命令查找到了MySQL的监听设置，bind-address设置为127.0.0.1的话，就只能从本机连接MySQL了，所有远程连接都不被允许，因此注释掉这一行: 123# Instead of skip-networking the default is now to listen only on# localhost which is more compatible and is not less secure.#bind-address = 127.0.0.1 重启MySQL服务: 1$ sudo service mysql restart 再次查看3306端口: 12$ netstat -anp |grep 3306tcp6 0 0 :::3306 :::* LISTEN - 这个问题终于解决了，网上所有的教程都是这么做的。如果你了解Linux的回环地址和socket连接的原理的话，几乎可以马上反应过来，我们还可以有别的修改方式！我们可以将bind-address的值更换为本机IP(我这里具体就是192.168.118.123) [注: 如果机器的ip地址变了，就要再次修改bind-address了，因此这种方法还是有缺陷的。] 12#bind-address = 127.0.0.1bind-address = 192.168.118.123 这样的话，通过mysql -u cheung -p/mysql -h 192.168.118.123 -u cheung -p/mysql -h localhost -u cheung -p都可以成功连接，但使用mysql -h 127.0.0.1 -u cheung -p则会报错ERROR 2003 (HY000): Can’t connect to MySQL server on ‘127.0.0.1’ (111)最近双十一阿里云打折力度很大, 于是入手了一台ECS服务器, 在折腾MySQL的时候又遇到了10038错误, 又翻出这篇笔记, 一样的套路, 注释掉bind-address, 并且确保使用的user是具有远程连接的权限之后, 还是没能远程连接到MySQL. telnet server-ip 3306不同, 怀疑是防火墙的问题, service iptables status发现防火墙是inactive (dead)的, 也就排除了防火墙的问题. 最后发现是阿里云的安全组配置导致的, 需要到阿里云控制台打开需要开放的端口, 否则除基本的22端口外, 其他端口都是不通的.导致错误的原因真的太多, 还是要好好排查原因, 多想想为什么… 删除用户1&gt; DROP USER 'smithj'@'localhost'; 或者 1&gt; DROP USER IF EXISTS 'smithj'@'localhost'; 不要滥用FLUSH PRIVILEGESFLUSH PRIVILEGES的作用是Reload grant tables滥用FLUSH PRIVILEGES命令的教程太多，一篇国外的文章对此做了详细的说明，传送门: Stop using FLUSH PRIVILEGES总结一句就是，除非涉及直接更改grant tables(如使用INSERT, UPDATE, or DELETE对这张表进行修改)，其他情况下使用FLUSH PRIVILEGES都是毫无意义的。使用GRANT, REVOKE, SET PASSWORD, or RENAME USER等命令时，MySQL server都会立即监听到这些变化并重新加载grant tables到内存。 需要使用FLUSH PRIVILEGES的一个例子，参见: ERROR 1698 (28000): Access denied for user ‘root’@’localhost’ 123mysql&gt; USE mysql;mysql&gt; UPDATE user SET plugin='mysql_native_password' WHERE User='root';mysql&gt; FLUSH PRIVILEGES; 参考文献:[1] MySQL Documentation[2] MySQL用户授权不当引起的问题[3] Stop using FLUSH PRIVILEGES[4] Privileges Provided by MySQL]]></content>
      <categories>
        <category>开发环境</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性方程组与向量方程、矩阵方程的联系]]></title>
    <url>%2F2018%2F08%2F09%2Flinear-equations-in-linear-algebra%2F</url>
    <content type="text"><![CDATA[再读《Linear Algebra and Its Applications》，作为日后温故知新的笔记。 通过行变换求解线性方程组的原理解线性方程组的基本策略是 “to replace one system with an equivalent system(i.e., one with the same solution set) that is easier to solve.” 有三种基本的行变换(Elementary Row Operation, 基本行变换)可以用于简化线性方程组，如下(Replacement, Interchange, Scaling): 显然，上面三种行变换每一种都是可逆的。如果矩阵A可以通过一系列的基本行变换转为矩阵$B$(由于基本行变换可逆，$B$也可以变换得到$A$)，那么我们称$A$和$B$是行等价的(row equivalent)。此外，对于一个线性方程组(以及它对应的增广矩阵)，基本行变换不改变原方程组的解，这一性质是使用基本行变换解线性方程组的根基，明白这一点至关重要！ 通过上面的方程组(称这个方程组为$A$)思考一下: 如果$A$通过其中一种基本行变换转为$B$，分别考察三种基本行变换，可以看到，方程组A的任意解必然是方程组$B$的解($A$的解代入方程组$B$必然满足)。反过来，$B$通过刚才$A$使用的基本行变换的逆变换(这还是一个基本行变换)也可以得到$A$，即$B$通过基本行变换可以转为$A$，因此$B$的任意解必然也是方程组A的解。我们得到结论: 如果两个线性方程组的增广矩阵是行等价的(即可以通过一系列的基本行变换互相转换)，那么这两个方程组有同样的解。 那么，我们只要不断地通过基本行变换把增广矩阵转换成更简单的形式，就会让方程的解更加显而易见。当转换成简化阶梯型矩阵时，方程的解就一目了然了。 向量的线性组合 通过一个求解一个向量是否是另外两个已知向量的线性组合的例子，我们发现这实际上就是求解线性方程组，且其增广矩阵由这些向量组成: 至此，我们知道，求解线性方程组的解也就是求解增广矩阵最后一个column与其他column之间的线性组合关系。 对于向量的线性组合还有以下定义: 矩阵与向量相乘$Ax$的定义 这个定义直接将向量线性组合与矩阵(×向量)乘积联系在一起了，由这个定义直接得知，$Ax=b$有解当且仅当$b$是$A$的列向量的线性组合。 另一方面，联系这个定义和上面提到的求解线性方程组也就是求解增广矩阵最后一个column与其他column之间的线性组合关系，我们立即可以得到下面的定理： 这个定理将线性方程组、向量方程和矩阵方程这三个方面联系起来了，感觉是一个伟大的壮举。 $Ax$的计算不少线性代数的教材开篇就直接写$Ax$怎么计算，但我们却看得一头雾水，不知道为什么是这样计算的，这样计算的来源依据是什么。上一节我们定义了矩阵$A$与向量$x$相乘的含义，就是$A$的列向量依次以$x$的各个元素为权重的线性组合。这就是我们计算$Ax$的原始依据，看下面的例子： 观察上面的计算结果，发现有下面的规律： 这就是我们手算$Ax$时使用的方法！]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tinyhttpd源码阅读]]></title>
    <url>%2F2018%2F08%2F04%2Ftiny-httpd-annotation%2F</url>
    <content type="text"><![CDATA[Tiny HTTPd项目地址: Tiny HTTPd作者J. David Blackstone对这个项目的Summary如下: tinyhttpd is a relatively simple webserver I wrote for a school project. While exceedingly simple, tinyhttpd is threaded and handles basic CGI scripts! This is an educational tool to demonstrate the concepts behind http. Tiny HTTPd源码阅读tinyhttpd是一个基于教学目的而开发的轻量级的http server，读者可以从中体会http协议的原理。读这个源码的过程中做了一些注解，贴在下面: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514/* J. David's webserver *//* This is a simple webserver. * Created November 1999 by J. David Blackstone. * CSE 4344 (Network concepts), Prof. Zeigler * University of Texas at Arlington *//* This program compiles for Sparc Solaris 2.6. * To compile for Linux: * 1) Comment out the #include &lt;pthread.h&gt; line. * 2) Comment out the line that defines the variable newthread. * 3) Comment out the two lines that run pthread_create(). * 4) Uncomment the line that runs accept_request(). * 5) Remove -lsocket from the Makefile. */#include &lt;stdio.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;ctype.h&gt;#include &lt;strings.h&gt;#include &lt;string.h&gt;#include &lt;sys/stat.h&gt;//#include &lt;pthread.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdlib.h&gt;#define ISspace(x) isspace((int)(x))#define SERVER_STRING "Server: jdbhttpd/0.1.0\r\n"void accept_request(int);void bad_request(int);void cat(int, FILE *); /* 将文件的全部内容发送到client socket */void cannot_execute(int); /* 向client socket发送500(服务器错误)消息 */void error_die(const char *);void execute_cgi(int, const char *, const char *, const char *);int get_line(int, char *, int); /* 从socket读取一行存放到第二个参数中,并返回该行的长度(包括统一的换行符\n) */void headers(int, const char *); /* 把http headers信息写入client socket */void not_found(int); /* 向client socket发送404(资源找不到)消息 */void serve_file(int, const char *); /* 把http headers和文件内容发送到client socket */int startup(u_short *);void unimplemented(int); /* 向client socket发送501(方法不支持)消息 *//**********************************************************************//* A request has caused a call to accept() on the server port to * return. Process the request appropriately. * Parameters: the socket connected to the client *//**********************************************************************/void accept_request(int client)&#123; char buf[1024]; int numchars; char method[255]; char url[255]; char path[512]; size_t i, j; struct stat st; int cgi = 0; /* becomes true if server decides this is a CGI * program */ char *query_string = NULL; /* CGI查询字符串 */ numchars = get_line(client, buf, sizeof(buf)); i = 0; j = 0; /* 获取Method */ while (!ISspace(buf[j]) &amp;&amp; (i &lt; sizeof(method) - 1)) &#123; method[i] = buf[j]; i++; j++; &#125; method[i] = '\0'; /* 只支持GET和POST */ if (strcasecmp(method, "GET") &amp;&amp; strcasecmp(method, "POST")) /* 忽略大小写比较字符串,若method既不是GET也不是POST,method未被支持 */ &#123; unimplemented(client); /* 向client socket发送501(方法不支持)消息 */ return; &#125; if (strcasecmp(method, "POST") == 0) cgi = 1; /* 获取URL */ i = 0; while (ISspace(buf[j]) &amp;&amp; (j &lt; sizeof(buf))) j++; while (!ISspace(buf[j]) &amp;&amp; (i &lt; sizeof(url) - 1) &amp;&amp; (j &lt; sizeof(buf))) &#123; url[i] = buf[j]; i++; j++; &#125; url[i] = '\0'; if (strcasecmp(method, "GET") == 0) &#123; /* 从URL中获取查询字符串 */ query_string = url; while ((*query_string != '?') &amp;&amp; (*query_string != '\0')) query_string++; if (*query_string == '?') &#123; cgi = 1; *query_string = '\0'; /* url的新结尾,之后url就只包含文件路径了,查询字符串则被query_string保存 */ query_string++; &#125; &#125; sprintf(path, "htdocs%s", url); if (path[strlen(path) - 1] == '/') strcat(path, "index.html"); if (stat(path, &amp;st) == -1) &#123; /* 通过文件名获取文件信息,文件信息保存在st中。若没有这个文件,则向client socket发送404消息 */ /* 直到无内容可读或者读到空行,这个while循环是为了丢弃client socket的headers */ while ((numchars &gt; 0) &amp;&amp; strcmp("\n", buf)) /* read &amp; discard headers */ numchars = get_line(client, buf, sizeof(buf)); not_found(client); /* 404 */ &#125; else &#123; if ((st.st_mode &amp; S_IFMT) == S_IFDIR) /* 若path是一个目录 */ strcat(path, "/index.html"); if ((st.st_mode &amp; S_IXUSR) || (st.st_mode &amp; S_IXGRP) || (st.st_mode &amp; S_IXOTH) ) /* 判断是否具有x(执行)权限 */ cgi = 1; if (!cgi) serve_file(client, path); /* 把http headers和文件内容发送到client socket */ else execute_cgi(client, path, method, query_string); &#125; close(client);&#125;/**********************************************************************//* Inform the client that a request it has made has a problem. * Parameters: client socket *//**********************************************************************/void bad_request(int client)&#123; char buf[1024]; sprintf(buf, "HTTP/1.0 400 BAD REQUEST\r\n"); send(client, buf, sizeof(buf), 0); sprintf(buf, "Content-type: text/html\r\n"); send(client, buf, sizeof(buf), 0); sprintf(buf, "\r\n"); send(client, buf, sizeof(buf), 0); sprintf(buf, "&lt;P&gt;Your browser sent a bad request, "); send(client, buf, sizeof(buf), 0); sprintf(buf, "such as a POST without a Content-Length.\r\n"); send(client, buf, sizeof(buf), 0);&#125;/**********************************************************************//* Put the entire contents of a file out on a socket. This function * is named after the UNIX "cat" command, because it might have been * easier just to do something like pipe, fork, and exec("cat"). * Parameters: the client socket descriptor * FILE pointer for the file to cat *//**********************************************************************/void cat(int client, FILE *resource)&#123; char buf[1024]; fgets(buf, sizeof(buf), resource); while (!feof(resource)) &#123; send(client, buf, strlen(buf), 0); fgets(buf, sizeof(buf), resource); &#125;&#125;/**********************************************************************//* Inform the client that a CGI script could not be executed. * Parameter: the client socket descriptor. *//**********************************************************************/void cannot_execute(int client)&#123; char buf[1024]; sprintf(buf, "HTTP/1.0 500 Internal Server Error\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "Content-type: text/html\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;P&gt;Error prohibited CGI execution.\r\n"); send(client, buf, strlen(buf), 0);&#125;/**********************************************************************//* Print out an error message with perror() (for system errors; based * on value of errno, which indicates system call errors) and exit the * program indicating an error. *//**********************************************************************/void error_die(const char *sc)&#123; perror(sc); exit(1);&#125;/**********************************************************************//* Execute a CGI script. Will need to set environment variables as * appropriate. * Parameters: client socket descriptor * path to the CGI script *//**********************************************************************/void execute_cgi(int client, const char *path, const char *method, const char *query_string)&#123; char buf[1024]; int cgi_output[2]; int cgi_input[2]; pid_t pid; int status; int i; char c; int numchars = 1; int content_length = -1; buf[0] = 'A'; buf[1] = '\0'; if (strcasecmp(method, "GET") == 0) /* GET */ while ((numchars &gt; 0) &amp;&amp; strcmp("\n", buf)) /* read &amp; discard headers */ numchars = get_line(client, buf, sizeof(buf)); else /* POST */ &#123; numchars = get_line(client, buf, sizeof(buf)); while ((numchars &gt; 0) &amp;&amp; strcmp("\n", buf)) /* 直到buff为空行,是为了读取并丢弃headers */ &#123; buf[15] = '\0'; if (strcasecmp(buf, "Content-Length:") == 0) content_length = atoi(&amp;(buf[16])); /* 从header中获取报文主体的长度 */ numchars = get_line(client, buf, sizeof(buf)); /* 逐行获取http headers */ &#125; if (content_length == -1) &#123; bad_request(client); return; &#125; &#125; sprintf(buf, "HTTP/1.0 200 OK\r\n"); send(client, buf, strlen(buf), 0); if (pipe(cgi_output) &lt; 0) &#123; /* 创建管道cig_output */ cannot_execute(client); return; &#125; if (pipe(cgi_input) &lt; 0) &#123; /* 创建管道cig_input */ cannot_execute(client); return; &#125; if ( (pid = fork()) &lt; 0 ) &#123; cannot_execute(client); return; &#125; if (pid == 0) /* child: CGI script */ /* 子进程 */ &#123; char meth_env[255]; char query_env[255]; char length_env[255]; dup2(cgi_output[1], 1); /* stdout被重定向,stdout的内容会写入cgi_output管道 */ dup2(cgi_input[0], 0); /* stdin被重定向,stdin会从cig_input管道读取信息 */ close(cgi_output[0]); close(cgi_input[1]); sprintf(meth_env, "REQUEST_METHOD=%s", method); putenv(meth_env); /* change or add an environment variable */ if (strcasecmp(method, "GET") == 0) &#123; sprintf(query_env, "QUERY_STRING=%s", query_string); putenv(query_env); &#125; else &#123; /* POST */ sprintf(length_env, "CONTENT_LENGTH=%d", content_length); putenv(length_env); &#125; execl(path, path, NULL); /* 执行path路径指定的文件 */ exit(0); &#125; else &#123; /* parent */ /* 父进程 */ close(cgi_output[1]); close(cgi_input[0]); if (strcasecmp(method, "POST") == 0) for (i = 0; i &lt; content_length; i++) &#123; recv(client, &amp;c, 1, 0); /* 逐个字符接收http报文主体 */ write(cgi_input[1], &amp;c, 1); /* 往cig_input的输入端写入http报文主体内容 */ &#125; while (read(cgi_output[0], &amp;c, 1) &gt; 0) /* 从cig_output读取内容并发送到client socket */ send(client, &amp;c, 1, 0); close(cgi_output[0]); close(cgi_input[1]); waitpid(pid, &amp;status, 0); /* 等待子进程结束 */ &#125;&#125;/**********************************************************************//* Get a line from a socket, whether the line ends in a newline, * carriage return, or a CRLF combination. Terminates the string read * with a null character. If no newline indicator is found before the * end of the buffer, the string is terminated with a null. If any of * the above three line terminators is read, the last character of the * string will be a linefeed and the string will be terminated with a * null character. * Parameters: the socket descriptor * the buffer to save the data in * the size of the buffer * Returns: the number of bytes stored (excluding null) *//**********************************************************************//* 从描述符sock指定的socket读取一行存放到buf中,然后返回这一行的长度(包含换行符\n) */int get_line(int sock, char *buf, int size)&#123; int i = 0; char c = '\0'; int n; while ((i &lt; size - 1) &amp;&amp; (c != '\n')) &#123; n = recv(sock, &amp;c, 1, 0); /* 从socket接收一个字符 */ /* DEBUG printf("%02X\n", c); */ if (n &gt; 0) /* recv成功 */ &#123; if (c == '\r') /* 发现回车符,到达行尾了,处理一下不同系统的换行 */ &#123; /* 要读懂这几行代码需要了解一下回车与换行的区别 */ n = recv(sock, &amp;c, 1, MSG_PEEK); /* 读取一个字符,但不消耗socket的buffer */ /* DEBUG printf("%02X\n", c); */ if ((n &gt; 0) &amp;&amp; (c == '\n')) /* 发现是换行符便消耗掉 */ recv(sock, &amp;c, 1, 0); else c = '\n'; &#125; buf[i] = c; i++; &#125; else c = '\n'; &#125; buf[i] = '\0'; return(i);&#125;/**********************************************************************//* Return the informational HTTP headers about a file. *//* Parameters: the socket to print the headers on * the name of the file *//**********************************************************************//* 把http headers信息写入client socket */void headers(int client, const char *filename)&#123; char buf[1024]; (void)filename; /* could use filename to determine file type */ strcpy(buf, "HTTP/1.0 200 OK\r\n"); send(client, buf, strlen(buf), 0); strcpy(buf, SERVER_STRING); send(client, buf, strlen(buf), 0); sprintf(buf, "Content-Type: text/html\r\n"); send(client, buf, strlen(buf), 0); strcpy(buf, "\r\n"); send(client, buf, strlen(buf), 0);&#125;/**********************************************************************//* Give a client a 404 not found status message. *//**********************************************************************/void not_found(int client)&#123; char buf[1024]; sprintf(buf, "HTTP/1.0 404 NOT FOUND\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, SERVER_STRING); send(client, buf, strlen(buf), 0); sprintf(buf, "Content-Type: text/html\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;HTML&gt;&lt;TITLE&gt;Not Found&lt;/TITLE&gt;\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;BODY&gt;&lt;P&gt;The server could not fulfill\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "your request because the resource specified\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "is unavailable or nonexistent.\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;/BODY&gt;&lt;/HTML&gt;\r\n"); send(client, buf, strlen(buf), 0);&#125;/**********************************************************************//* Send a regular file to the client. Use headers, and report * errors to client if they occur. * Parameters: a pointer to a file structure produced from the socket * file descriptor * the name of the file to serve *//**********************************************************************//* 把http headers和文件内容发送到client socket */void serve_file(int client, const char *filename)&#123; FILE *resource = NULL; int numchars = 1; char buf[1024]; buf[0] = 'A'; buf[1] = '\0'; while ((numchars &gt; 0) &amp;&amp; strcmp("\n", buf)) /* read &amp; discard headers *//* 读取并抛弃headers内容 */ numchars = get_line(client, buf, sizeof(buf)); resource = fopen(filename, "r"); if (resource == NULL) not_found(client); /* 找不到filename对应的资源,返回404信息 */ else &#123; headers(client, filename); /* 将header信息添加到client socket */ cat(client, resource); /* 把文件resource的内容全部send到client socket */ &#125; fclose(resource);&#125;/**********************************************************************//* This function starts the process of listening for web connections * on a specified port. If the port is 0, then dynamically allocate a * port and modify the original port variable to reflect the actual * port. * Parameters: pointer to variable containing the port to connect on * Returns: the socket *//**********************************************************************/int startup(u_short *port)&#123; int httpd = 0; /* socket描述符 */ struct sockaddr_in name; /* socket地址 */ httpd = socket(PF_INET, SOCK_STREAM, 0); /* 创建socket,返回socket描述符 */ if (httpd == -1) error_die("socket"); memset(&amp;name, 0, sizeof(name)); name.sin_family = AF_INET; /* 采用AF_INET地址族 */ name.sin_port = htons(*port); /* 指定端口。Host to Network Short int,将整型变量从主机字节顺序转变成网络字节顺序,即将整数在地址空间存储方式变为高位字节存放在内存的低地址处 */ name.sin_addr.s_addr = htonl(INADDR_ANY); /* Host to Network Long int,INADDR_ANY就是0.0.0.0,泛指本机地址 */ if (bind(httpd, (struct sockaddr *)&amp;name, sizeof(name)) &lt; 0) /* 绑定socket的地址 */ error_die("bind"); if (*port == 0) /* if dynamically allocating a port */ &#123; socklen_t namelen = sizeof(name); if (getsockname(httpd, (struct sockaddr *)&amp;name, &amp;namelen) == -1) /* 获取socket绑定的地址(name) */ error_die("getsockname"); *port = ntohs(name.sin_port); /* 获取socket实际端口,port是通过指针传进来的,startup函数的调用者也就获得了socket的实际端口 */ &#125; if (listen(httpd, 5) &lt; 0) /* 开始监听并且指定socket的最大连接数为5 */ error_die("listen"); return(httpd); /* 返回socket的描述符 */&#125;/**********************************************************************//* Inform the client that the requested web method has not been * implemented. * Parameter: the client socket *//**********************************************************************/void unimplemented(int client)&#123; char buf[1024]; sprintf(buf, "HTTP/1.0 501 Method Not Implemented\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, SERVER_STRING); send(client, buf, strlen(buf), 0); sprintf(buf, "Content-Type: text/html\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;Method Not Implemented\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;/TITLE&gt;&lt;/HEAD&gt;\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;BODY&gt;&lt;P&gt;HTTP request method not supported.\r\n"); send(client, buf, strlen(buf), 0); sprintf(buf, "&lt;/BODY&gt;&lt;/HTML&gt;\r\n"); send(client, buf, strlen(buf), 0);&#125;/**********************************************************************//* 掌握一些关于socket和网络通讯的基本知识再来看代码会比较得心应手 * 推荐阅读: https://blog.csdn.net/hguisu/article/details/7445768 *//**********************************************************************/int main(void)&#123; int server_sock = -1; u_short port = 0; int client_sock = -1; struct sockaddr_in client_name; /* 客户端通讯地址 */ socklen_t client_name_len = sizeof(client_name);// pthread_t newthread; server_sock = startup(&amp;port); /* 建立一个socket并开始监听,返回的server_sock是这个socket的描述符 */ printf("httpd running on port %d\n", port); while (1) &#123; /* 与客户端建立连接,获得连接套接字 */ /* accept默认会阻塞进程,直到有一个客户连接建立后返回,它返回的是一个新可用的套接字,这个套接字是连接套接字 */ client_sock = accept(server_sock, (struct sockaddr *)&amp;client_name, &amp;client_name_len); if (client_sock == -1) error_die("accept"); accept_request(client_sock); /* if (pthread_create(&amp;newthread , NULL, accept_request, client_sock) != 0) */ /* perror("pthread_create"); */ &#125; close(server_sock); return(0);&#125; 编译Tiny HTTPd在源码文件httpd.c的开头，作者已经对如何在Linux上编译做了详细说明： 12345678/* This program compiles for Sparc Solaris 2.6. * To compile for Linux: * 1) Comment out the #include &lt;pthread.h&gt; line. * 2) Comment out the line that defines the variable newthread. * 3) Comment out the two lines that run pthread_create(). * 4) Uncomment the line that runs accept_request(). * 5) Remove -lsocket from the Makefile. */ 做完上面说的5步之后，执行编译命令make，会抛出两个warning，不过不影响编译结果: 1234567891011121314151617cheung@ubuntu:~/documents/tinyhttpd-0.1.0$ makegcc -W -Wall -lpthread -o httpd httpd.chttpd.c: In function ‘startup’:httpd.c:444:52: warning: pointer targets in passing argument 3 of ‘getsockname’ differ in signedness [-Wpointer-sign] if (getsockname(httpd, (struct sockaddr *)&amp;name, &amp;namelen) == -1) ^In file included from httpd.c:16:0:/usr/include/x86_64-linux-gnu/sys/socket.h:127:12: note: expected ‘socklen_t * restrict &#123;aka unsigned int * restrict&#125;’ but argument is of type ‘int *’ extern int getsockname (int __fd, __SOCKADDR_ARG __addr, ^httpd.c: In function ‘main’:httpd.c:503:24: warning: pointer targets in passing argument 3 of ‘accept’ differ in signedness [-Wpointer-sign] &amp;client_name_len); ^In file included from httpd.c:16:0:/usr/include/x86_64-linux-gnu/sys/socket.h:243:12: note: expected ‘socklen_t * restrict &#123;aka unsigned int * restrict&#125;’ but argument is of type ‘int *’ extern int accept (int __fd, __SOCKADDR_ARG __addr, 抛出警告是因为httpd.c中的namelen和client_name_len的类型不严格匹配:将 1int namelen = sizeof(name); 修改为 1socklen_t namelen = sizeof(name); 将 1int client_name_len = sizeof(client_name); 修改为 1socklen_t client_name_len = sizeof(client_name); 可以修正这两个警告。 编译通过后，目录下会生成可执行文件httpd，运行这个程序: 1$ ./httpd 程序会在命令行打印随机获得的http端口(port)，在浏览器打开ip:port就可以打开一个简单的网页。这个网页有一个输入框，输入一个颜色(例如purple/blue/yellow/pink)，可以跳转到指定颜色的网页(ip:port/color.cgi)。如下图(输入purple): 如果在输入一个颜色之后，跳转的网页是空的，这是由于脚本指定的perl解析器的路径有问题。打开htdoc目录下的check.cgi和color.cgi，可以看到第一行都是 1#!/usr/local/bin/perl -Tw 在我使用的Linux主机环境下，perl解析器的路径并不是/usr/local/bin/perl，使用whereis perl查看perl路径: 12$ whereis perlperl: /usr/bin/perl /usr/bin/perl5.22-x86_64-linux-gnu /etc/perl /usr/share/perl /usr/share/man/man1/perl.1.gz 把check.cgi和color.cgi的第一行修改为: 1#!/usr/bin/perl -Tw]]></content>
      <categories>
        <category>源码</category>
      </categories>
      <tags>
        <tag>Http</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群监控]]></title>
    <url>%2F2018%2F07%2F28%2Fhadoop-cluster-monitoring%2F</url>
    <content type="text"><![CDATA[本文的控制台输出都基于一个四台虚拟主机的Hadoop集群，这个集群的搭建过程参考 Hadoop完全分布式集群部署，集群概况如下： 角色 主机名(hostname) IP地址 Master sun 192.168.118.101 Slave mercury 192.168.118.102 Slave venus 192.168.118.103 Slave earth 192.168.118.104 监控HDFS通过控制台监控HDFSHDFS的命令都是通过${HADOOP_HOME}/bin/hdfs脚本调用的。在控制台执行hdfs会打印hdfs包含的参数命令的用法，看着一目了然，如下： 123456789101112131415161718192021222324252627282930313233343536cheung@sun:~$ hdfsUsage: hdfs [--config confdir] [--loglevel loglevel] COMMAND where COMMAND is one of: dfs run a filesystem command on the file systems supported in Hadoop. classpath prints the classpath namenode -format format the DFS filesystem secondarynamenode run the DFS secondary namenode namenode run the DFS namenode journalnode run the DFS journalnode zkfc run the ZK Failover Controller daemon datanode run a DFS datanode dfsadmin run a DFS admin client haadmin run a DFS HA admin client fsck run a DFS filesystem checking utility balancer run a cluster balancing utility jmxget get JMX exported values from NameNode or DataNode. mover run a utility to move block replicas across storage types oiv apply the offline fsimage viewer to an fsimage oiv_legacy apply the offline fsimage viewer to an legacy fsimage oev apply the offline edits viewer to an edits file fetchdt fetch a delegation token from the NameNode getconf get config values from configuration groups get the groups which users belong to snapshotDiff diff two snapshots of a directory or diff the current directory contents with a snapshot lsSnapshottableDir list all snapshottable dirs owned by the current user Use -help to see options portmap run a portmap service nfs3 run an NFS version 3 gateway cacheadmin configure the HDFS cache crypto configure HDFS encryption zones storagepolicies list/get/set block storage policies version print the versionMost commands print help when invoked w/o parameters. 其中hdfs dfsadmin命令可以获取关于集群的有用信息。例如下面的命令可以获取hadoop集群目前运行的概况(例如集群容量、使用率和live datanodes) 1$ hdfs dfsadmin -report 下面是在四台虚拟主机搭建的hadoop集群执行report命令的返回样例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566cheung@sun:~$ hdfs dfsadmin -reportConfigured Capacity: 56669491200 (52.78 GB)Present Capacity: 29869539328 (27.82 GB)DFS Remaining: 29869441024 (27.82 GB)DFS Used: 98304 (96 KB)DFS Used%: 0.00%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0-------------------------------------------------Live datanodes (3):Name: 192.168.118.102:50010 (mercury)Hostname: mercuryDecommission Status : NormalConfigured Capacity: 18889830400 (17.59 GB)DFS Used: 32768 (32 KB)Non DFS Used: 7826771968 (7.29 GB)DFS Remaining: 10079883264 (9.39 GB)DFS Used%: 0.00%DFS Remaining%: 53.36%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Sat Jul 28 21:35:44 CST 2018Name: 192.168.118.104:50010 (earth)Hostname: earthDecommission Status : NormalConfigured Capacity: 18889830400 (17.59 GB)DFS Used: 32768 (32 KB)Non DFS Used: 7826591744 (7.29 GB)DFS Remaining: 10080063488 (9.39 GB)DFS Used%: 0.00%DFS Remaining%: 53.36%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Sat Jul 28 21:35:44 CST 2018Name: 192.168.118.103:50010 (venus)Hostname: venusDecommission Status : NormalConfigured Capacity: 18889830400 (17.59 GB)DFS Used: 32768 (32 KB)Non DFS Used: 8197160960 (7.63 GB)DFS Remaining: 9709494272 (9.04 GB)DFS Used%: 0.00%DFS Remaining%: 51.40%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Sat Jul 28 21:35:44 CST 2018 通过help命令可以获取dfsadmin的更多用法: 1$ hdfs dfsadmin -help 通过浏览器(WebUI)监控HDFS可视化真是个好东西，通过WebUI监控HDFS直观多了。master(namenode)的WebUI通过在hdfs-site.xml中设置dfs.namenode.http-address来设置，默认namenode的50070端口。浏览器打开地址(hadoop-master-ip替换为hadoop集群master的ip): http://hadoop-master-ip:50070 我们可以看到页面顶部菜单栏有如下几个监控目标:|Overview|DataNodes|Datanode Volumn Failures|Snapshot|Startup Progress|Utilities|Overview: 集群概况，与通过hdfs dfsadmin -report获取的信息类似；DataNodes: 各个DN(Datanode)的状态、容量、Blocks数量等信息；Datanode Volumn Failures: 记录DN的异常；Snapshot: Snapshot SummaryStartup Progress: 记录HDFS启动过程和状态，包括装载fsimage和edits，checkpoint信息；Utilities: 这个很方便实用！下拉有两个选项，”Browse the file system”和”Logs”，”Browse the file system”以图形界面的方式查看HDFS文件系统存储的内容，”Logs”可以查看hadoop-master节点的日志。 监控YARN通过控制台监控YARN控制台输入yarn会输出yarn命令的用法。下面的命令输出yarn集群的节点信息。 1cheung@sun:~$ yarn node -list 12345618/07/28 23:58:07 INFO client.RMProxy: Connecting to ResourceManager at sun/192.168.118.101:8032Total Nodes:3 Node-Id Node-State Node-Http-Address Number-of-Running-Containers earth:40373 RUNNING earth:8042 0 venus:46595 RUNNING venus:8042 0 mercury:33455 RUNNING mercury:8042 0 [贴心提醒：hdfs命令和yarn命令不需要在master上执行也可以获得一样的结果。] 通过浏览器(WebUI)监控YARNYARN默认http端口为8088，浏览器打开地址(hadoop-master-ip替换为hadoop集群master的ip): http://hadoop-master-ip:8088 日志监控通过jps/hdfs/yarn这些命令行工具，我们可以快速查看集群的状态，但如果某些节点出问题了或者是发现某些进程没有正常启动(总之就是发现异常吧)，还是要通过日志打印来了解异常的详情然后寻求解决办法。Hadoop的日志在${HADOOP_HOME}/logs目录下。 参考文献:[1] https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/[2] http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml[3] http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux的一些基础知识]]></title>
    <url>%2F2018%2F07%2F25%2Flinux-tutorial-for-beginners%2F</url>
    <content type="text"><![CDATA[为了避免在写Linux相关的文章时重复啰嗦，在这里整理一些比较重要的Linux基础知识(Based on Ubuntu)。 关于不同的操作系统不同的操作系统有不一样的使命，也导致了用户群体的不一样。如微软的Windows和苹果的MacOS是目前用户十分广泛的桌面操作系统，两者都带有非常友好易用的用户界面，对于娱乐和办公，这类操作系统占有绝对领先的份额。但是对于服务于众多软件后台功能的服务器，图形界面就显得不这么重要了，这类操作系统更看重软件支持程度、安全性、相对较少的系统资源占用等，而开源的Linux是目前最广泛使用的服务器操作系统。Linux的发新版很多，比较常见的有Ubuntu(上手简单，界面看着也很舒服)、CentOS(服务器用得比较多，但不适合桌面机)、Debian和ArchLinux没用过，不过听说比较适合高级用户，可能由于系统可定制程度和系统优化比较好而成为这部分用户的心头好吧；也有一个国内的Linux发行版得到了很多用户的喜爱——Deepin，界面很好看可以当桌面操作系统把玩一下，看新闻说今年Deepin团队创始人离职了，希望这个项目还能好好运营下去吧。嗯，我一直都选Ubuntu，一方面是它的图形界面算得上比较好看的了，并且网上关于Ubuntu可以找到的资料也比较丰富。曾经我也热衷于折腾Linux的图形界面，搜集各种Ubuntu主题美化的教程，后来发现这些主题或多或少都对系统性能造成了影响，有的还可能导致死机…后来就都用原生的了。 那些常用的配置文件/etc/profile与.bash_profile环境变量这个词大概不陌生，它的作用是告诉操作系统，对用的程序的摆放位置。添加环境变量后，要执行这些路径下的程序不需要再输入完整路径。$ echo $PATH命令可以输出当前的环境变量。在Linux上安装过JDK、Scala、Hadoop、Spark之类软件的朋友，大概都给Linux配置过环境变量。这个与Windows上安装软件并配置环境变量是类似的。Windows上有对应的图形界面可以让用户添加环境变量，而Linux的环境变量则从文件中读取，具体来说，是/etc/profile和~/.bash_profile这两个文件。这两个位置的环境变量区别在于作用域不一样。在/etc/profile中添加的环境变量对主机上的所有用户有效，属于全局环境变量；在~/.bash_profile中添加的环境变量只对当前用户有效，属于用户环境变量。此外，Linux中~/.bashrc与~/.bash_profile作用类似，不过~/.bashrc在每次交互式shell启动时都会重新加载。(注：Linux中~指向当前用户的主目录，即/home/user_name目录)要使环境变量修改立刻生效，需要使用source命令： 12$ source /etc/profile$ source ~/.bash_profile hostname与hostsLinux中，文件/etc/hostname文件保存着当前机器的机器名。安装Ubuntu虚拟机的话，hostname就是ubuntu，打开命令行可以看到前缀cheung@ubuntu:~$，其中cheung是用户名，ubuntu就是机器名了，~表示目前命令行在用户主目录。Linux中的/etc/hosts是IP和主机名配置文件，负责IP地址与域名(主机名)的快速解析，与Windows中的hosts文件作用是一样的。例如，你在当前主机经常要访问另外一个IP为192.168.118.130的主机，例如你常常要确认这台主机的网络是否还正常，你可以用$ ping 192.168.118.130来做这个测试。但这个IP地址这么长，实在不好记，你就可以在/etc/hosts中添加一行192.168.118.130 server1，这样主机就会把server1解析为192.168.118.130，以后做只需要使用ping server1就可以了。 Linux的权限管理Linux是一个多用户系统，用户/用户组/文件权限等概念和相关的命令要好好掌握。需要着重掌握的命令有chmod和chown，chmod用于更改文件或文件夹的读写许可设置(change mode的缩写)，chown用于更改文件所有者(change owner的缩写)，命令的具体使用方法可以参考这里chmod | chown SSHSSH是Secure Shell的缩写，为主机之间的远程通讯和管理提供安全服务。SSH安装： 1$ sudo apt-get install openssh-server 生成公钥和私钥： 1$ ssh-keygen -t rsa -P "" SSH具体的机制可以看看这个Understanding the SSH Encryption and Connection Process (文章挺长…) 下面简单说明一下SSH的原理。用户user在主机A和B上都有账号，常常需要在主机B上连接主机A进行通信，为了确保通讯连接的安全，就想了一个办法：主机B造了一把锁(公钥)和配套的钥匙(私钥)，然后把这锁复制了一个给到A(放在authorized_keys里，这是A添加的白名单)。之后每次B尝试与A建立连接，A就随机生成一个字符串并用这把锁加密然后发送给B，B收到这段密文之后用钥匙打开就得到了明文，然后B就把解密的明文发送给A，A收到后检查是否与发送的字符串一致，一致的话就可以确认B是认证的用户，连接就确认了。最近搭建Hadoop集群，遇到一个比较诡异的问题：添加authorized_keys却不生效，后来发现竟然是权限问题，.ssh和authorized_keys权限过高导致！GitHub有对应的问题并给出了解决方案(chmod修改一下权限就可以了)，传送门 Adding public key to ~/.ssh/authorized_keys does not log me in automatically ，这里也有一些说明 I copied my public key to authorized_keys but public-key authentication still doesn’t work.要避免ssh的权限问题，可以使用ssh-copy-id命令来添加公钥。ssh-copy-id命令可以把本地主机的公钥复制到远程主机的authorized_keys文件上，ssh-copy-id命令也会给远程主机的用户主目录（home）和/.ssh, 和/.ssh/authorized_keys设置合适的权限。详情参考这里 ssh-copy-id命令 Linux一些常用命令除了上面的一些Linux知识，使用Linux最重要的是要掌握常用的命令，例如文件复制cp, 文件移动mv, 罗列文件ls, 压缩解压tar,进行远程拷贝文件的命令scp等，命令详情可以通过这个网站查询： Linux命令大全 Linux桌面系统软件软推荐使用Linux桌面的话，有几个软件比较推荐的：PDF阅读器：Okular系统设置工具：Unity Tweak Tool截图软件: Shutter音乐播放器：Audacious中文输入法：不推荐。搜狗拼音是目前在Linux上做的最好的，但貌似资源占用挺多的，偶尔导致系统卡顿…]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop完全分布式集群部署]]></title>
    <url>%2F2018%2F07%2F25%2Finstall-apache-hadoop-on-multi-node-cluster%2F</url>
    <content type="text"><![CDATA[Hadoop支持以下三种部署模式： Local (Standalone) Mode: 本地模式Pseudo-Distributed Mode: 伪分布模式Fully-Distributed Mode: 完全分布模式 前两种模式只适合用作Hadoop相关实验和测试，也难以从中体会Hadoop的框架和理念，Hadoop官网有简单的部署教程，传送门 Hadoop: Setting up a Single Node Cluster。 本文讨论Hadoop完全分布模式集群的搭建。 VMware Workstation安装Ubuntu虚拟机安装Ubuntu虚拟机前断网(或者待进入安装界面后，在虚拟机内部断开网络，避免影响宿主机器的网络)，否则Ubuntu在安装过程中可能会下载更新，导致安装速度缓慢。可以先安装虚拟机，然后在每个虚拟机环境下安装Hadoop；也可以先在一台虚拟机上搭建好Hadoop的软件环境，然后克隆该虚拟机得到集群。本文采用第一种方法。Hadoop集群需要统一的用户，因此新建虚拟机时，填写的用户名保持一致，可以避免安装后再新建统一用户。下面全部用cheung作为统一的用户名。密码也使用统一的密码。 搭建Hadoop集群集群主机列表如下： 角色 主机名(hostname) IP地址 Master sun 192.168.118.101 Slave mercury 192.168.118.102 Slave venus 192.168.118.103 Slave earth 192.168.118.104 概述：在集群的每台机器上安装vim、ssh、jdk，每台机器都要配置jdk和hadoop环境变量，先在master(即sun)上配置好hadoop，把配置好的hadoop发送到其他主机完成部署。 更改软件源(可选)这一步可选，选择合适的软件源可以提高软件的下载速度。开机进入Ubuntu后，先更改软件源为阿里云镜像源。(打开Ubuntu Software，菜单栏选择Software &amp; Updates，在打开面板的”Download from:”中依次选择”Others…”, China, http://mirrors.aliyun.com/ubuntu。稍等片刻，让Ubuntu Software更新source.list。打开命令行(快捷键Ctrl+Alt+T)，运行sudo apt-get update确保更新软件源。 安装vim1cheung@ubuntu:~$ sudo apt-get install vim 修改主机名为了用主机名代替IP进行Hadoop集群内主机之间的通讯，需要修改各个主机的host以及hostname/etc/hostname是Ubuntu系统的主机名配置文件，按照上述过程安装完Ubuntu，主机名默认是ubuntu。打开命令行可以观察到前缀为cheung@ubuntu:~$。使用vim(或者gedit,nano等编辑器)打开/etc/hostname，将主机名其修改为自己需要的名称，四台机器的主机名分别改为sun,mercury,venus,earth。[注：太阳Sun及距离其最近的三大行星(水星-Mercury、金星-Venus、地球-Earth)，主机名一律使用小写。] 文件/etc/hostname 1ubuntu 修改为 1sun 文件/etc/hosts 123456789127.0.0.1 localhost127.0.1.1 ubuntu# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 修改为(注释掉第二行) 123456789127.0.0.1 localhost#127.0.1.1 ubuntu# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 上面127.0.1.1 ubuntu那一行必须注释掉或者直接删除，否则会导致hadoop集群的slave主机群无法连接上master。这个问题困扰我很久，没想到竟然是这里出了问题。在hadoop官网找到了对应的issue竟然是2010年的 Distributed hadoop setup 0 live datanode problem in cluster 注意，其他三台主机(水星-Mercury、金星-Venus、地球-Earth)的/etc/hostname文件内容分别修改为mercury,venus,earth。 同时，/etc/hosts文件的第二行亦作同样的修改。重启即可使更改生效，再次打开命令行，可以看到控制台命令的前缀已经变为cheung@sun:~$ 添加IP与主机名的映射在每个虚拟机的/etc/hosts中添加Hadoop集群中每个虚拟机的ip地址和主机名(hostname)的映射：[注：Linux查看IP的命令是$ ifconfig] 1234192.168.118.101 sun192.168.118.102 mercury192.168.118.103 venus192.168.118.104 earth 安装并配置ssh Hadoop集群的master与slave之间，slave与slave之间都存在互相通信，因此要保证集群的所有机器之间能通过ssh无密码登陆。例如，master向slave派发任务，slave上报block信息给master，以及slave之间的文件互传(冗余备份)。 1$ sudo apt-get install openssh-server 生成公钥和私钥： 1$ ssh-keygen -t rsa -P "" 将集群中每一台机器的~/.ssh/id_rsa.pub(公钥)的内容复制到所有节点(包括自己)的.ssh/authorized_keys文件末尾。对于这一步，有很多方法，ssh有个专用命令ssh-copy-id可以将本机的公钥复制到远程机器的authorized_keys文件中： 1234ssh-copy-id -i ~/.ssh/id_rsa.pub cheung@sunssh-copy-id -i ~/.ssh/id_rsa.pub cheung@mercuryssh-copy-id -i ~/.ssh/id_rsa.pub cheung@venusssh-copy-id -i ~/.ssh/id_rsa.pub cheung@earth 注意，上面的四条添加公钥到authorized_keys的命令要在每台机器上执行！再次重申“Hadoop集群的master与slave之间，slave与slave之间都存在互相通信，因此要保证集群的所有机器之间能通过ssh无密码登陆。”通过以下命令验证是否能无密码登陆： 1234$ ssh sun$ ssh mercury$ ssh venus$ ssh earth 配置正确的话，上面的ssh命令无需输入密码就可以直接登陆对应的虚拟机。 安装Hadoop运行环境所需的软件分别到官网下载JDK、Hadoop安装包 jdk-8u152-linux-x64.tar.gz hadoop-2.7.6.tar.gz 上述两个安装包全部放在Downloads目录下。 安装JDK准备把软件都安装到/opt目录下，为避免一直需要管理员权限，修改/opt目录的所有者为cheung: 1$ sudo chown -R cheung:cheung /opt 命令行cd ~/Downloads/进入Downloads文件夹。 解压： 1$ tar -zxvf jdk-8u152-linux-x64.tar.gz 移动至/opt目录下并同步修改目录名为jdk： 1$ sudo mv jdk1.8.0_152/ /opt/jdk ls /opt可以看到/opt目录下的jdk文件夹。 配置jdk环境变量： 1$ sudo vim /etc/profile 在末尾添加jdk环境变量： 12345# JDKexport JAVA_HOME=/opt/jdkexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 运行以下命令让jdk环境变量生效： 1$ source /etc/profile 检查java jdk是否安装成功： 1$ java -version 出现类似输出表示jdk安装成功： 1234$ java -versionjava version "1.8.0_152"Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode) 安装Hadoop进入hadoop安装包所在文件夹，解压： 1$ tar -zxvf hadoop-2.7.6.tar.gz 移动至/opt目录下并同步修改文件名为spark： 1$ mv hadoop-2.7.6 /opt/hadoop ls /opt可以看到/opt目录下的hadoop文件夹。 配置hadoop环境变量： 1$ sudo vim /etc/profile 在末尾添加spark环境变量： 123# Hadoopexport HADOOP_HOME=/opt/hadoopexport PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH 运行以下命令让hadoop环境变量生效： 1$ source /etc/profile 修改Hadoop配置hadoop的配置文件都在${HADOOP_HOME}/etc/hadoop目录下，命令行进入这个目录修改以下配置文件： 1$ cd $&#123;HADOOP_HOME&#125;/etc/hadoop hadoop-env.sh1$ vim hadoop-env.sh 将其中的 1export JAVA_HOME=$&#123;JAVA_HOME&#125; 替换为JDK的安装位置： 1export JAVA_HOME=/opt/jdk core-site.xml1$ vim core-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://sun:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1$ vim hdfs-site.xml 12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/nameNode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/dataNode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意，上面我把namenode和datanode的路径分别设置在了/data/hadoop/nameNode和/data/hadoop/dataNode，hadoop格式化的时候会尝试创建这两个目录，我们要确保这个目录能被成功创建。但是/data位于系统根目录，用户cheung运行的hadoop程序是无权创建这个目录的，因此需要我们手动以管理员权限创建并将这个目录的所有者更改为cheung，之后的就可以交给hadoop。当然了，如果把这个路径改为/home/cheung下的某个路径，那么hadoop就有权创建所需的文件夹。我们舍近求远设置到/data路径下只是为了把数据统一放到一个显著的位置。我们需要在集群的每一台主机上执行以下创建/data目录并更改所有权的命令。 12$ sudo mkdir /data$ sudo chown -R cheung:cheung /data mapred-site.xml1$ vim mapred-site.xml ${HADOOP_HOME}/opt/etc/hadoop目录下可能没有mapred-site.xml，但有一个mapred-site.xml.template，复制该文件得到mapred-site.xml。 12$ cp mapred-site.xml.template mapred-site.xml$ vim mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;sun&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.acl.enable&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slave把原有的localhost那一行删除，添加 123mercuryvenusearth 这里是设置集群哪些机器作为slave节点的地方，当然了，也可以再添加一行sun，让sun作为master管理集群的同时也作为slave执行任务。但这样会让master的负载过高，一般不建议这样做。 打包hadoop并发送到集群其他机器在master(即主机sun)上完成hadoop的安装后，压缩打包配置好的hadoop安装文件，然后通过scp发送至机器其他机器，在这些机器上把压缩包解压到/opt目录下，然后按照前面的做法添加hadoop环境变量，完成在所有机器上的hadoop的部署。 压缩: 12$ cd /opt$ tar -czvf hadoop.tar.gz /opt/hadoop/ 将压缩包发送到集群其他主机: 123$ scp hadoop.tar.gz cheung@mercury:/opt$ scp hadoop.tar.gz cheung@venus:/opt$ scp hadoop.tar.gz cheung@earth:/opt 然后在这些主机上解压这个压缩包到/opt目录下: 1$ tar -zxvf /opt/hadoop.tar.gz -C /opt 最后的检查 ssh是否按照上面的说明在每台主机上验证过了 如果按照上面hdfs-site.xml的设定的话，先确认每台主机都新建了/data目录并更改owner为集群的统一用户(cheung) 确认每台主机上的hadoop压缩包都已经解压到指定的位置，可通过ls /opt查看 在每台主机上确认Hadoop的环境变量设置正常，在集群的每台机器上执行: 12$ java -version$ hadoop version 输出如下图： 格式化HDFS格式化Hadoop文件系统。在集群master主机sun上执行格式化命令: 1$ hdfs namenode -format 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273cheung@sun:~$ hdfs namenode -format18/07/27 18:07:59 INFO namenode.NameNode: STARTUP_MSG: /************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host = sun/192.168.118.101STARTUP_MSG: args = [-format]STARTUP_MSG: version = 2.7.6STARTUP_MSG: classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.6.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.6.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.6-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.7.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.6.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.6.jar:/opt/hadoop/contrib/capacity-scheduler/*.jarSTARTUP_MSG: build = https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8; compiled by &apos;kshvachk&apos; on 2018-04-18T01:33ZSTARTUP_MSG: java = 1.8.0_152************************************************************/18/07/27 18:07:59 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]18/07/27 18:07:59 INFO namenode.NameNode: createNameNode [-format]Formatting using clusterid: CID-602d7447-71b0-4809-b633-f29c2652a28a18/07/27 18:08:00 INFO namenode.FSNamesystem: No KeyProvider found.18/07/27 18:08:00 INFO namenode.FSNamesystem: fsLock is fair: true18/07/27 18:08:00 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false18/07/27 18:08:00 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=100018/07/27 18:08:00 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true18/07/27 18:08:00 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.00018/07/27 18:08:00 INFO blockmanagement.BlockManager: The block deletion will start around 2018 Jul 27 18:08:0018/07/27 18:08:00 INFO util.GSet: Computing capacity for map BlocksMap18/07/27 18:08:00 INFO util.GSet: VM type = 64-bit18/07/27 18:08:00 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB18/07/27 18:08:00 INFO util.GSet: capacity = 2^21 = 2097152 entries18/07/27 18:08:00 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false18/07/27 18:08:00 INFO blockmanagement.BlockManager: defaultReplication = 218/07/27 18:08:00 INFO blockmanagement.BlockManager: maxReplication = 51218/07/27 18:08:00 INFO blockmanagement.BlockManager: minReplication = 118/07/27 18:08:00 INFO blockmanagement.BlockManager: maxReplicationStreams = 218/07/27 18:08:00 INFO blockmanagement.BlockManager: replicationRecheckInterval = 300018/07/27 18:08:00 INFO blockmanagement.BlockManager: encryptDataTransfer = false18/07/27 18:08:00 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 100018/07/27 18:08:00 INFO namenode.FSNamesystem: fsOwner = cheung (auth:SIMPLE)18/07/27 18:08:00 INFO namenode.FSNamesystem: supergroup = supergroup18/07/27 18:08:00 INFO namenode.FSNamesystem: isPermissionEnabled = true18/07/27 18:08:00 INFO namenode.FSNamesystem: HA Enabled: false18/07/27 18:08:00 INFO namenode.FSNamesystem: Append Enabled: true18/07/27 18:08:00 INFO util.GSet: Computing capacity for map INodeMap18/07/27 18:08:00 INFO util.GSet: VM type = 64-bit18/07/27 18:08:00 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB18/07/27 18:08:00 INFO util.GSet: capacity = 2^20 = 1048576 entries18/07/27 18:08:00 INFO namenode.FSDirectory: ACLs enabled? false18/07/27 18:08:00 INFO namenode.FSDirectory: XAttrs enabled? true18/07/27 18:08:00 INFO namenode.FSDirectory: Maximum size of an xattr: 1638418/07/27 18:08:00 INFO namenode.NameNode: Caching file names occuring more than 10 times18/07/27 18:08:00 INFO util.GSet: Computing capacity for map cachedBlocks18/07/27 18:08:00 INFO util.GSet: VM type = 64-bit18/07/27 18:08:00 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB18/07/27 18:08:00 INFO util.GSet: capacity = 2^18 = 262144 entries18/07/27 18:08:00 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.999000012874603318/07/27 18:08:00 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 018/07/27 18:08:00 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 3000018/07/27 18:08:00 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 1018/07/27 18:08:00 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 1018/07/27 18:08:00 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,2518/07/27 18:08:00 INFO namenode.FSNamesystem: Retry cache on namenode is enabled18/07/27 18:08:00 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis18/07/27 18:08:00 INFO util.GSet: Computing capacity for map NameNodeRetryCache18/07/27 18:08:00 INFO util.GSet: VM type = 64-bit18/07/27 18:08:00 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB18/07/27 18:08:00 INFO util.GSet: capacity = 2^15 = 32768 entries18/07/27 18:08:00 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1090642709-192.168.118.101-153268608073218/07/27 18:08:00 INFO common.Storage: Storage directory /data/hadoop/nameNode has been successfully formatted.18/07/27 18:08:00 INFO namenode.FSImageFormatProtobuf: Saving image file /data/hadoop/nameNode/current/fsimage.ckpt_0000000000000000000 using no compression18/07/27 18:08:00 INFO namenode.FSImageFormatProtobuf: Image file /data/hadoop/nameNode/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds.18/07/27 18:08:00 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 018/07/27 18:08:00 INFO util.ExitUtil: Exiting with status 018/07/27 18:08:00 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at sun/192.168.118.101************************************************************/cheung@sun:~$ HDFS格式化完成，查看/data目录，可以看到hadoop创建的目录和文件。 启动HDFS1cheung@sun:~$ start-dfs.sh 命令行输出如下: 1234567Starting namenodes on [sun]sun: starting namenode, logging to /opt/hadoop/logs/hadoop-cheung-namenode-sun.outvenus: starting datanode, logging to /opt/hadoop/logs/hadoop-cheung-datanode-venus.outmercury: starting datanode, logging to /opt/hadoop/logs/hadoop-cheung-datanode-mercury.outearth: starting datanode, logging to /opt/hadoop/logs/hadoop-cheung-datanode-earth.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /opt/hadoop/logs/hadoop-cheung-secondarynamenode-sun.out 在sun(hadoop集群master)上使用java的jps命令查看jvm进程(输出内容前面的数字是进程号)： 1cheung@sun:~$ jps 1231942 NameNode2151 SecondaryNameNode2411 Jps 在各个slave(mercury/venus/earth)节点上执行jps可看到以下类似输出: 121970 Jps1812 DataNode 启动YARN1cheung@sun:~$ start-yarn.sh 12345starting yarn daemonsstarting resourcemanager, logging to /opt/hadoop/logs/yarn-cheung-resourcemanager-sun.outmercury: starting nodemanager, logging to /opt/hadoop/logs/yarn-cheung-nodemanager-mercury.outearth: starting nodemanager, logging to /opt/hadoop/logs/yarn-cheung-nodemanager-earth.outvenus: starting nodemanager, logging to /opt/hadoop/logs/yarn-cheung-nodemanager-venus.out 再次在sun(hadoop集群master)上使用java的jps命令查看jvm进程，可看到多了ResourceManager进程，如下: 1cheung@sun:~$ jps 12342752 Jps1942 NameNode2151 SecondaryNameNode2489 ResourceManager 在各个slave(mercury/venus/earth)节点上执行jps可看到以下类似输出: 1232035 NodeManager1812 DataNode2136 Jps 停止HDFS与YARN在sun(hadoop集群master)上执行 12$ stop-dfs.sh$ stop-yarn.sh 至此，Hadoop集群搭建完毕。 参考文献：[1] https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2018%2F06%2F19%2Fregularization%2F</url>
    <content type="text"><![CDATA[问题描述$$E(w)=\frac{1}{N}\sum_{n=1}^{N} {y(x_n, w)-t_n}^2 + \lambda {\lVert w \rVert}^2$$ 关于机器学习的书很多都会讲到结构风险最小化，它是为了防止过拟合而提出来的策略。很多的讲解都以上面的公式作为例子，公式第一项是经验风险，第二项是表示模型复杂度的正则化项(或者说是对复杂模型的惩罚项)。 经验风险表示模型表达数据的误差(上面的公式使用均方误差)；而正则化项的加入是为了惩罚过于复杂的模型。对于这一点，早就深深刻入脑子里了，后来细看PRML，才发现以前太过于囫囵吞枣。一个需要理解的关键问题是，上面公式的正则化项中${\lVert w \rVert}^2$(即$L_2范数$)为什么可以表示模型复杂度，从而使正则化具有惩罚复杂模型的作用呢？这是一篇PRML读书笔记，着重于对知识的整理，下面的图都标注了来源(主要来自PRML一书)。 多项式曲线拟合下面通过PRML中多项式曲线拟合的例子来理解这一问题。首先，如图1所示，通过对函数$t=\sin(2 \pi x)$加入随机噪声来生成若干训练样本。图1中的蓝色小圈表示训练样本，绿色曲线是用于生成数据的函数$t=\sin(2 \pi x)$ 图1(PRML Figure 1.2)：加入噪声的10个数据点 我们使用下面的多项式函数来做拟合 $$y(x,w)=w_0+w_{1}x+w_{2}x^2+\dots+w_{M}x^M=\sum_{j=0}^{M}{w_{j}x^j}$$ 其中M是多项式最高次项的次数(order)。我们也可以把$w_0,\dots,w_M$合起来表示为$w$，这些参数($w$)可以通过最小化误差函数来决定，一个常用的误差函数是样本的均方误差之和，即下式 $$E(w)=\sum_{n=1}^{N} {y(x_n,w)-t_n}^2$$ 最小化上式就是常见的最小二乘法，对于给定的M(多项式的order)可以通过对各个参数求导来求解，这里就不继续展开了。现在问题来了，M的值取多少好呢？若取$M=0$，$y(x,w)=w_0$就是一条与$x$轴平行的直线；若取$M=1$，$y(x,w)=w_0+w_{1}x$就是一条斜率为$w_{1}$的直线；若取$M=2$就是一条二次曲线等~ 我们用不同的$M$值(不同的多项式模型)做实验，得到下面的结果 图2(PRML Figure 1.4)：不同次数的多项式拟合的结果 从上图可以看到，取多项式次数$M&lt;2$时，拟合效果相当差；而取$M=9$时，10个训练样本点全都落在了曲线上(完美拟合了训练样本)。嗯，知道真相的我们肯定不会认为$M=9$这个看着完美的结果是最好的。为了验证这些结果是否理想，我们用同样的方法生成了一些测试样本，测试的均方误差(对方差取平均)结果如下 图3(PRML Figure 1.5)：不同模型的均方误差 由图3可以看到，当$M=0,1,2$，参数也分别只有$1,2,3$个，代表简单的模型，这几个模型对训练样本和测试样本的拟合效果都不好(误差大)，当$M$取值越大，参数越多，模型越来越复杂，模型对训练样本的拟合效果越来越好(蓝色线)，足够复杂的模型可以完全拟合训练样本($M \geq 9$)；另一方面，选择越来越复杂的模型，测试误差经历了先下降再升高的过程，当$3 \leq M \leq 8$时，模型的测试结果比较好；当模型复杂度很高时($M \geq 9$)，测试误差剧增，表明该模型对未知(unseen)样本的拟合非常差。 正则化的作用誒，从上面的训练和测试结果来看，我们直接把不同的$M$值都尝试一遍，然后选择测试误差最小的模型不就OK了么，这样得到的模型泛化能力并不差(在测试样本上体现)，哪需要多整一个正则化项？先看看上面的实验表现出来的看似矛盾的地方：低次多项式实际上是更高次多项式的子集(令多出的高次项的系数为0可得)，也就是说$M=9$的多项式可以实现的拟合结果至少不会比$M=3$的多项式差；另一方面，学过微积分的同学大概记得我们生成数据的函数$\sin(2\pi x)$的泰勒展开式包含所有次项，我们的测试结果理应随着$M$值增大而不断改善。带着疑惑，我们看看当多项式最高次数$M$变化时，通过最小化均方误差得到的模型的各个参数的值 图4(PRML Table 1.1)：不同多项式模型的参数值 通过上图可以看到，采用更大的$M$值，多项式的系数的值(绝对值)变得越来越大。当$M=9$时，参数值变得异常大，曲线完全拟合了测试样本，但却偏离实际的$\sin(2\pi x)$甚远，得到的结果的系数与$\sin(2\pi x)$的泰勒展开式的系数差异巨大。单纯地采取使均方误差(MSE)最小化的策略，会使得模型不断地去适应训练样本，当模型越来越复杂，就像贪食蛇游戏一样，曲线剧烈抖动力求经过每一个训练样本点，导致模型的参数(magnitude)非常大。也就是说，复杂的模型过度拟合训练样本会使得参数值很大。要限制模型这种过度适应训练样本的行为，于是引入了正则化，通过惩罚模型过大的参数值来防止模型对训练样本的过拟合。也就是开篇的公式 $$E(w)=\frac{1}{N}\sum_{n=1}^{N} {y(x_n, w)-t_n}^2 + \lambda {\lVert w \rVert}^2$$ 通过正则化，$M\geq 9$的多项式模型可以获得更好的结果，这样一来，“低次多项式模型拟合效果比高次多项式更好”的矛盾就解决了。由于高次多项式是包含低次多项式的，这样的模型的表达能力更强，而正则化正好可以实现这种模型复杂度的缩放变化，复杂模型中提高罚项(penalty term)的权重可以让一些参数变为零(或接近零)，降低模型的实际复杂度，从而可以退化为更简单的模型。上面提到一个问题，我们直接把不同的$M$值都尝试一遍，然后选择测试误差最小的模型不就OK了么，这样得到的模型泛化能力并不差(但并非让人满意)，哪需要多整一个正则化项？这样做不好的原因是，通过经验风险最小化获得的模型，模型复杂度高时(模型参数多)，由于高度拟合训练样本，测试结果往往很差，就如图2所示。结果就是，这样的策略倾向于选择参数数量少($M$值小)的模型，以至于得到的过于简单的模型根本无法还原真实数据的原貌[缺点与Akaike information criterion(AIC)惊人地相似)]。而采取正则化的结构风险最小化方法避免了这些问题。 增加样本数量可以缓解过拟合问题如果真要不加正则化项的话(直接采用经验风险最小化策略，使均方误差MSE最小)，同等模型复杂度下，增加训练样本数量可以缓解过拟合的问题。 图4(PRML Figure 1.6)：$M=9$的多项式在不同训练样本数量下的结果比较 正则化与奥卡姆剃刀原则正则化是符合奥卡姆剃刀原则的，在可选的模型中，我们选择简单易解释的模型。模型简单与否，我们通过规则函数来判断。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>结构风险最小化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark连接MySQL数据库]]></title>
    <url>%2F2018%2F04%2F08%2Fconnect-spark-to-mysql%2F</url>
    <content type="text"><![CDATA[Spark官方的spark-assembly包集合里面没有连接MySQL需要的jar包，报错java.lang.ClassNotFoundException: com.mysql.jdbc.Driver，需要先去下载一个MySQL Connector解压后把mysql-connector-java-5.1.46.jar包含到Project的Libraries中。连接所需要的参数用option(&quot;name&quot;, &quot;value&quot;)来指定，详情参照以下代码。 1234567891011121314151617181920import org.apache.spark.sql.SQLContextimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by WahCheung on 2018/4/8. */object SparkDemo &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(this.getClass.getSimpleName).setMaster("local[*]") val sc = new SparkContext(conf) val sqlContext: SQLContext = new SQLContext(sc) val df = sqlContext.read.format("jdbc").option("url", "jdbc:mysql://your_ip_address/your_database").option("driver", "com.mysql.jdbc.Driver") .option("dbtable", "your_table").option("user", "your_name").option("password", "your_password").load() // df是DataFrame类型 println(s"number of records fetched: $&#123;df.count()&#125;") df.printSchema() // 打印结构 sc.stop() &#125;&#125; 如果连接信息出错，会出现java.lang.NullPointerException，检查val df = ...那一行看看有没有什么地方写错了。]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[层次聚类]]></title>
    <url>%2F2018%2F01%2F30%2Fhierarchical-clustering%2F</url>
    <content type="text"><![CDATA[层次聚类输出一个具有层次结构的簇集合，因此能够比扁平聚类输出的无结构簇集合提供更丰富的信息，代价是，层次聚类的时间复杂度至少是文档数目的平方级，较线性时间复杂度的K-均值算法和EM算法要差。 凝聚式层次聚类层次聚类可以是自顶向下或自底向上的一个过程。自底向上的算法一开始将每篇文档都看成是一个簇，然后不断地对簇进行两两合并，直到所有文档都聚成一类为止。自底向上的聚类方法也因此被称为$HAC$。而自顶向下的方法则首先将所有文档看成一个簇，然后不断利用某种方法对簇进行分裂直到每篇文档都成为一个簇为止。在信息检索领域，自底向上的$HAC$方法更普遍。合并策略：每次选取相似度最大的两个簇进行合并。不同的层次聚类方法的差异在于簇的相似度计算方法不同，因此也就具备不同的性质。IR中介绍了四种层次聚类方法，分别是单连接聚类，全连接聚类，组平均聚类，质心聚类。 层次聚类不需要事先指定簇的数目，但在一些应用中，我们希望聚类得到不想交的多个簇。此时，需要在聚类的层次结构中的某一点进行截断（停止聚类），常用截断方式有以下几种。 在某个实现给定的相似度水平上进行截断。例如，当待结合的两个簇的结合相似度低于设定的阈值（如0.4）时，聚类终止。 当两个连续的聚类结果的结合相似度之差最大时进行截断。这种策略相当于寻找聚类的拐点，较大的差值意味着多增加一个簇之后的聚类结果质量会显著下降。 结构风险最小化。$$K = \mathop{\arg\min}_{K’}[RSS(K’) + \lambda K’]$$其中，$K’$指的是对层次结构进行截断后产生的结果簇的数目，$RSS$是残差平方和，而$\lambda$是每额外增加一个簇时的惩罚量。 和扁平聚类一样，也可以事先指定结果簇的数目K，在产生K个簇时进行截断。 单连接聚类及全连接聚类算法在单连接聚类（single-link clustering或single-linkage clustering）中，两个簇之间的相似度定义为两个最相似的成员之间的相似度。这种单连接的合并准则是局部的，它仅仅关注两个簇互相邻近的区域，而不考虑簇中更远的区域和簇的总体结构。在全连接聚类（complete-link clustering 或complete-linkage clustering）中，两个簇之间的相似度定义为两个最不相似的成员之间的相似度，这相当于选择两个簇进行聚类，使得合并结果具有最短路径。全连接聚类准则是非局部的，聚类结果中的整体结构信息会影响合并的结果。这种聚类实际上相当于优先考虑具有较短直径的紧凑簇，而不是具有长直径的松散簇，当然这种做法可能会对离群点较为敏感，比如某个远离中心的文档会显著增加候选簇的直径从而完全改变最后的聚类结果。 算法的图论解释单连接和全连接聚类算法都可以用图论来解释。令$S_k$为第$k$步中两个簇合并后的结合相似度，$G(S_k)$为任意两点间相似度不低于$S_k$的点的连接图。对于单连接聚类，第$k$步后得到的簇其实是$G(S_k)$的连通分支；而采用全连接聚类在第$k$步后得到的簇则构成$G(S_k)$的最大团。上述基于图论的解释方法也是单连接和全连接聚类方法得名的由来。单连接方法中第k步的所谓单连接簇指的是这样一些点的集合：它们当中至少存在两个点之间的相似度满足$ \geq s_k$；而全连接方法中第$k$步的簇中所有点之间的相似度均满足$s \geq s_k$。 组平均凝聚式聚类GAAC（Group-average Agglomerative Clustering，组平均凝聚式聚类）通过计算所有文档($w_i \cup w_j$)之间的相似度来对簇的质量进行计算，因此可以避免在单连接和全连接准则中只计算一对文档相似度的缺陷。组平均聚类计算所有文档之间相似度的平均值，其中包括来自同一簇的文档，但不使用自相似度（文档与文档自身的相似度）。因此，簇$\omega_i$和$\omega_j$的组平均相似度($\verb|SIM-GA|$)计算公式如下： $$\verb|SIM-GA|(\omega_i, \omega_j) = \frac{1}{(N_i+N_j)(N_i+N_j-1)}\sum_{d_m \in \omega_i\cup\omega_j},\sum_{d_n \in \omega_i\cup\omega_j, 且d_n \neq d_m} \vec d_m \cdot \vec d_n \tag{1}$$ 其中，$\vec d$是文档$d$的长度归一化向量，$\cdot$是内积运算符，$N_i$和$N_j$分别是$\omega_i$和$\omega_j$中的文档数目。 组平均聚类的动机是要选择两个整体上一致性较高的簇进行合并。这个一致性，通过合并的簇$\omega_k=\omega_i\cup\omega_j$中两两文档之间相似度的均值来评估。 由于组平均聚类需要计算任意两个文档之间的相似度，其时间复杂度是$\Theta(N_i N_j)$。这个计算在某些条件下可以得到优化：当文档用单位向量表示(归一化)，并且相似度采用余弦相似度时，向量相似度之和等于向量和的相似度： $$\sum_{d_m \in \omega_i},\sum_{d_n \in \omega_j}(\vec d_m \cdot \vec d_n) = \left(\sum_{d_m \in \omega_i} \vec d_m\right)\cdot\left(\sum_{d_n \in \omega_j} \vec d_n\right) \tag{2}$$ 于是，有 $$\verb|SIM-GA|(\omega_i, \omega_j) = \frac{1}{(N_i+N_j)(N_i+N_j-1)}\left[\left(\sum_{d_m \in \omega_i\cup\omega_j}\vec d_m\right)^2 - (N_i+N_j)\right] \tag{3}$$ 其中，式子右边的$(N_i+N_j)$是$N_i+N_j$个值为1的自相似度之和。假设两个向量和$\sum_{d_m \in \omega_i}\vec d_m$及$\sum_{d_n \in \omega_j}\vec d_n$已知，利用上述技巧可以在常数时间而不是$\Theta(N_i N_j)$内计算两个簇的相似度。而计算两个向量和$\sum_{d_m \in \omega_i}\vec d_m$及$\sum_{d_n \in \omega_j}\vec d_n$的时间复杂度是$\Theta(N_i+N_j)$ 补充说明关于上面说到的“当文档用单位向量表示(归一化)，并且相似度采用余弦相似度时，向量相似度之和等于向量和的相似度”，这个性质(即公式2)是由于单位向量的内积计算满足加法分配律。下面给出公式2的证明过程。假设每篇文档有两个特征，用$x$和$y$表示，文档$d_m$的归一化表示为$(x_m, y_m)$。文档$d_m$和文档$d_n$的余弦相似度为 $$sim(d_m,d_n) = \frac{\vec d_m \cdot \vec d_n}{|\vec d_m||\vec d_n|} = \vec d_m \cdot \vec d_n = (x_m, y_m)\cdot(x_n, y_n) = x_mx_n+y_my_n$$ 对于只有一篇文档($d_0$)的簇$w_j$，它与包含$N$篇文档的簇$w_i={d_1,d_2,\cdots,d_{N}}$的相似度的和为（根据定义，即$d_0$与$w_i$中所有文档的相似度的和）：$$\begin{align}Sum(sim) &amp;= \sum_{d_n \in w_i} \vec d_0 \cdot \vec d_n \ &amp;= \sum_{1\leq n \leq N} (x_0, y_0)\cdot(x_n, y_n) \ &amp;= \sum_{1\leq n \leq N} (x_0x_n+y_0y_n) \ &amp;= \left(x_0\sum_{1\leq n \leq N}x_n + y_0\sum_{1\leq n \leq N}y_n \right) \ &amp;= \left(\vec d_0 \sum_{d_n \in w_i} \vec d_n\right)\end{align}$$ 即： $$\sum_{d_n \in w_i} \vec d_0 \cdot \vec d_n = \left(\vec d_0 \sum_{d_n \in w_i} \vec d_n\right) \tag{*}$$ 若簇$w_i$包含多篇文档$(d_{1},d_{2},\cdots,d_{M})$，根据定义，计算簇$w_i$与簇$w_j$的相似度的和(包含$MN$个文档相似度，$M,N$分别是簇$w_i$与簇$w_j$的文档数目)： $$\begin{align}Sum(sim)&amp;= \sum_{d_m \in w_i}\sum_{d_n \in w_j}(\vec d_m \cdot \vec d_n) \&amp;= \left[\left(\vec d_{1} \sum_{d_n \in w_j} \vec d_n\right) + \left(\vec d_{2} \sum_{d_n \in w_j} \vec d_n\right) + \cdots + \left(\vec d_{M} \sum_{d_n \in w_j} \vec d_n\right)\right] \tag{**} \end{align}$$ 上式记为[**]。$\sum_{d_n \in w_j} \vec d_n$也是一个向量，记为$\vec d_s$，代入[**]：[注：下式推导的第二到第三行利用了公式[*]] $$\begin{align}Sum(sim) &amp;= (\vec d_{1} \vec d_s + \vec d_{2} \vec d_s + \cdots + \vec d_{M} \vec d_s) \ &amp;= \sum_{d_m \in w_i} (\vec d_m \cdot \vec d_s) \ &amp;= \vec d_s \sum_{d_m \in w_i} \vec d_m \ &amp;= \left(\sum_{d_n \in \omega_j} \vec d_n\right)\cdot\left(\sum_{d_m \in \omega_i} \vec d_m\right)\end{align}$$ $Sum(sim)$是向量相似度之和，上面等式右边即是向量和的相似度。即，向量相似度之和等于向量和的相似度，得证。 质心聚类在质心聚类中，将通过两个簇的质心相似度来定义这两个簇的相似度： $$\begin{align}\verb|SIM-CENT|(w_i,w_j)&amp;= \vec\mu(w_i) \cdot \vec\mu(w_j) \tag{5} \&amp;= \left(\frac{1}{N_i} \sum_{d_m \in \omega_i} \vec d_m\right)\cdot\left(\frac{1}{N_j} \sum_{d_n \in \omega_j} \vec d_n\right) \&amp;= \frac{1}{N_iN_j}\sum_{d_m \in w_i}\sum_{d_n \in w_j}(\vec d_m \cdot \vec d_n) \tag{6}\end{align}$$ 公式(5)就是质心相似度。公式(6)则表明质心相似度等价于不同簇文档之间的平均相似度。注意与组平均聚类的区别。与其他三种HAC算法相比，质心聚类方法不是单调的，可能会发生相似度的颠倒现象，也就是说聚类过程中相似度值有可能会下降。 层次聚类的最优性]]></content>
      <tags>
        <tag>聚类</tag>
        <tag>信息检索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文维基百科语料处理]]></title>
    <url>%2F2018%2F01%2F06%2Fwikipedia%2F</url>
    <content type="text"><![CDATA[中文维基百科语料下载中文维基百科语料下载地址：https://dumps.wikimedia.org/zhwiki/latest/选择zhwiki-latest-pages-articles.xml.bz2 安装gensimgensim基于numpy和scipy，先安装这两个库。 1$ pip install numpy 1$ pip install scipy 安装scipy时报错pip._vendor.requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host=&#39;pypi.python.org&#39;, port=443): Read timed out.后面安装gensim是也报同样的错误，改为使用wheel文件安装。scipy下载地址：https://pypi.python.org/pypi/scipygensim下载地址：https://pypi.python.org/pypi/gensim 1$ pip install scipy-1.0.0-cp36-none-win_amd64.whl 1$ pip install gensim-3.2.0-cp36-cp36m-win_amd64.whl 中文维基百科语料提取12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import timeimport loggingfrom gensim.corpora import WikiCorpusfrom codecs import openlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)if __name__ == '__main__': logging.info("running start at %s" % time.time()) inp = '../data/zhwiki-latest-pages-articles.xml.bz2' outp = '../data/zhwiki' cnt = 0 output = open(outp, 'w', 'utf-8') wiki = WikiCorpus(inp, lemmatize=False, dictionary=&#123;&#125;) for text in wiki.get_texts(): output.write(" ".join(text) + "\n") cnt = cnt + 1 if cnt % 10000 == 0: logging.info("saved %s articles" % cnt) output.close() logging.info("saved all %s articles" % cnt) logging.info("running end at %s" % time.time()) 123456789101112131415C:\Software\Python36\python.exe D:/Workspace/Wikipedia/src/process_wiki.py2018-01-07 17:50:42,994 : INFO : running start at 1515318642.9940503C:\Software\Python36\lib\site-packages\gensim\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;) # 这个UserWarning多次出现，不必理会2018-01-07 17:51:07,901 : INFO : saved 10000 articles2018-01-07 17:51:29,050 : INFO : saved 20000 articles2018-01-07 17:51:47,826 : INFO : saved 30000 articles...2018-01-07 18:06:38,324 : INFO : saved 290000 articles2018-01-07 18:07:14,165 : INFO : saved 300000 articles2018-01-07 18:07:37,254 : INFO : finished iterating over Wikipedia corpus of 306129 documents with 69298936 positions (total 3067015 articles, 82501223 positions before pruning articles shorter than 50 words)2018-01-07 18:07:37,405 : INFO : saved all 306129 articles2018-01-07 18:07:37,405 : INFO : running end at 1515319657.4056065Process finished with exit code 0 使用结巴分词对语料进行分词如果还没安装jieba分词的话，通过以下命令安装。 1$ pip install jieba 使用结巴分词，对语料进行分词，每篇文章输出依旧是一行。程序耗时差不多一个小时。 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import timeimport loggingfrom codecs import openimport jiebalogging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO)if __name__ == &apos;__main__&apos;: logging.info(&quot;running start at %s&quot; % time.time()) inp = &apos;../data/zhwiki&apos; outp = &apos;../data/zhwiki_seg&apos; cnt = 0 output = open(outp, &apos;w&apos;, &apos;utf-8&apos;) with open(inp, &apos;r&apos;, &apos;utf-8&apos;) as inp: for text in inp: words = jieba.cut(text) words = filter(lambda w: w != &quot; &quot;, words) output.write(&quot; &quot;.join(words)) cnt += 1 if cnt % 10000 == 0: logging.info(&quot;saved %s articles&quot; % cnt) output.close() logging.info(&quot;saved all %s articles&quot; % cnt) logging.info(&quot;running start at %s&quot; % time.time()) 参考文献：[1] 中英文维基百科语料上的Word2Vec实验]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>维基百科</tag>
        <tag>Gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Windows C++开发环境]]></title>
    <url>%2F2018%2F01%2F05%2Fsetup-cpp-development-environment-on-windows%2F</url>
    <content type="text"><![CDATA[Windows下可用的C++ IDE有不少，毕业前一直用的都是微软的Visual Studio，从VS2010到VS2017，调试特别简单，功能也非常丰富。缺点是不支持多平台。其安装包也很大，高达几个GB，业余时间用来写点小代码感觉就是杀鸡用牛刀…换了不少的IDE，都觉得不太满意，Code::Blocks有集成了mingw的版本，安装和使用都特别简单，但感觉界面风格有点老气…换来换去，最终选择了Eclipse CDT + MinGW的组合。 关于MinGW百度搜索mingw大概会指向http://mingw.org/，但这个网站的mingw已经很久没更新了，被坑了很多次，发现下载的版本竟然是10年前的。后来在贴吧发现了mingw现在的官网，地址是 http://www.mingw-w64.org首页介绍mingw-w64如下： Mingw-w64 is an advancement of the original mingw.org project, created to support the GCC compiler on Windows systems. It has forked it in 2007 in order to provide support for 64 bits and new APIs. It has since then gained widespread use and distribution. 看来旧版mingw网站已经停更10年了。mingw吧提供了新版mingw网站的mingw版本介绍和下载方法。下载方式：前往 https://sourceforge.net/projects/mingw-w64/files/?source=navbar页面下拉，选择需要的版本，然后点击链接，跳转之后自动下载。 MinGW Distro贴吧发现的，推荐这个发行版，简单易用，对新手友好。MinGW Distro是gcc的一个发行版本，集成了boost，由Stephan T. Lavavej个人进行维护，STL的个人网站 https://nuwen.net/mingw.html 有关于MinGW Distro的详细介绍和下载链接。下载后，直接解压到C盘根目录，会得到一个MinGW的解压文件夹。 Eclipse C++开发下载Eclipse CDT版本，解压安装。新建CPP Project，编译器路径选择C:\MinGW。若遇到“Launch failed. Binary not found”，依次选择Project / Properties / C/C++ Build / Settings /Binary parsers:勾选“PE Windows Parser”。每次run/debug之前记得先build project。]]></content>
      <categories>
        <category>开发环境</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>MinGW</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MMSEG中文分词]]></title>
    <url>%2F2017%2F12%2F11%2Fmaximum-matching-segmentation-for-chinese-text%2F</url>
    <content type="text"><![CDATA[对经典的MMSEG中文分词做一下笔记。论文地址: MMSEG: A Word Identification System for Mandarin Chinese Text Based on Two Variants of the Maximum Matching Algorithm 词典（The Lexicon）词典包含两部分，第一部分是中文的词汇表，这部分是从网络搜集的中文词汇。第二部分是中文词汇及其附加的使用频率，用于MMSEG分词的最后一条规则。 歧义消除规则chunk：几个词组成的块，例如句子“晚上/一起/吃饭/吧”，“晚上/一起/吃饭”就是三个词组成的chunk，“吃饭/吧”是两个词组成的chunk。本文讨论的最大匹配规则中的chunk的最大长度设置为3。 最大匹配（Maximum matching）从最长的chunk中选取第一个单词，若有多个chunk的长度都是最长的，则应用下一条规则。 例如对于“眼看就要来了”，从“眼”开始的chunk有以下几个，“眼看 就要 来了”这个chunk最长，选取这个chunk的第一个词“眼看”作为分词结果。 123451. 眼看 就要 来了2. 眼看 就要 来3. 眼看 就 要4. 眼 看 就要5. 眼 看 就 最大化平均词长（Largest average word length）这个规则基于“多字词比单字词更常见”的假设。由于chunk的单词数是定长的（一般设置为3），在文本的结尾很可能只剩下不足3个单词。例如，下面的chunks的长度是一样的，但平均长度不同。这个规则会选取平均长度最大的chunk的第一个单词。 121. _C1_ _C2_ _C3_2. _C1C2C3_ 1231. 国际化2. 国际 化3. 国 际 化 在上面的例子中，选取的是_C1C2C3_。国际化的分词是这个规则的一个应用实例。这个规则只适用于chunk的单词数不足预设的值的情况（一般设定chunk的单词数为3），常见于文本末尾的分词，或者是短文本。所有三个词组成的chunk，如果总长度是一样的话，平均长度也会是一样的。此时，需要进一步的歧义消除。 最小化词长方差（Smallest variance of word lengths）这个规则基于“单词长度更倾向于均匀分布”的假设。 121. _C1C2_ _C3C4_ _C5C6_2. _C1C2C3_ _C4_ _C5C6_ 12研究 生命 起源研究生 命 起源 选取词长变化最小的chunk的第一个词。词长变化一般用词长的方差来衡量。在上面的例子中，这个规则会选取_C1C2_作为分词结果。 最大化单字词语素自由度 （Largest sum of degree of morphemic freedom of one-character words）下面两个chunk的长度、词长方差、平均长度都一样： 121. _C1_ _C2_ _C3C4_2. _C1_ _C2C3_ _C4_ 12主要 是 因为主 要是 因为 这个规则只考虑仅包含一个汉字的单字词。一个词的使用频率越高则它的自由度也越高，一个被高频使用的汉字更可能是一个单字词。计算chunk的自由度的方法是对单字词的词频的对数值求和。]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>中文分词</tag>
        <tag>MMSEG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类算法的评价指标]]></title>
    <url>%2F2017%2F12%2F09%2Fevaluation-of-clustering%2F</url>
    <content type="text"><![CDATA[聚类是无监督学习（unsupervised learning）的一种最普遍的形式。通过比较聚类算法得到的聚类结果与人工判定的标准分类结果的吻合程度，可以评价一个聚类算法的好坏。 纯度纯度是聚类效果最简单的评价指标之一。计算纯度时，每个簇被分配给该簇当中出现数目最多的文档所在的类别，然后可以通过正确分配的文档数除以文档集中的文档总数$N$来得到该分配的精度。形式地有 $$purity(\Omega,C) = \frac{1}{N}\sum_k\max_j|\omega_k \cap c_j|$$ 其中，$\Omega = {\omega_1,\omega_2,\cdots,\omega_K}$是聚类结果，$C = {c_1,c_2,\cdots,c_J}$是类别集合。每个$w_k$和$c_j$都是指一些样本组成的集合。 如图，每个簇的样本数分别为：$|w_1| = 6$，$|w_2| = 6$，$|w_3| = 5$，总样本数$N = 17$。每个簇所对应的主类以及这个主类对应的样本数分别是：簇1-三角形-5，簇2-圆形-4，簇3-正方形-3。纯度为 $$purity = \frac{1}{17}(5+4+3) = \frac{12}{17}$$ 归一化互信息NMI（Normalized Mutual Information，归一化互信息）是一种能在聚类质量和簇数目之间维持均衡的指标。定义如下 $$NMI(\Omega,C)=\frac{I(\Omega,C)}{[H(\Omega)+H(C)]/2}$$其中，$I$是互信息 $$\begin{align}I(\Omega,C)&amp;=\sum_k \sum_jP(w_k \cap c_j)\log{\frac{P(w_k \cap c_j)}{P(w_k)P(c_j)}} \&amp;=\sum_k \sum_j \frac{|w_k \cap c_j|}{N} \log{\frac{N|w_k \cap c_j|}{|w_k||c_j|}}\end{align}$$ $I(\Omega,C)$度量的是在知道簇的情况下关于类的知识所增加的信息量。因为由互信息定义$I(X;Y) = H(X) + H(Y) - H(X, Y)$，且$H(X, Y) \leq 0$，因此有$I(X;Y) \geq H(X) + H(Y)$。公式的分母$[H(\Omega)+H(C)]/2$是$I(\Omega,C)$的紧上界，因此NMI的值在0到1之间。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>纯度</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马尔科夫不等式]]></title>
    <url>%2F2017%2F12%2F06%2Fmarkov-inequality%2F</url>
    <content type="text"><![CDATA[定义马尔科夫不等式(Markov’s inequality)，其形式如下$X$是一个非负值随机变量，则对于任意$a &gt; 0$，有 $$P(X \geq a) \leq \frac {E(X)}{a}$$ 令$k = \frac {a}{E(X)}$，(显然有 $k &gt; 0$)，上面的不等式可以重写为： $$P(X \geq k \cdot E(X)) \leq \frac {1}{k} $$ 举个例子，假设收入都是非负的，马尔科夫不等式表明，拥有多于($\geq$)五倍($k=5$)人均收入的人口不超过总人口数的五分之一。[Wikipedia-Markov’s inequality] 证明$X$可以被切分成$X \geq a$和$X &lt; a$两部分。假设$N_{x \geq a}$和$N_{x &lt; a}$分别是$X \geq a$和$X &lt; a$的元素数量。则$X$的元素总量$N = N_{x \geq a} + N_{x &lt; a}$。 $$\begin{aligned}E(X) &amp;= E(X_{x \geq a} \cup X_{x &lt; a}) \&amp;= \frac {\sum_{x \geq a} x + \sum_{x &lt; a} x}{N} \&amp;= \frac {\sum_{x \geq a} x}{N} + \frac {\sum_{x &lt; a} x}{N} \&amp;\geq \frac {\sum_{x \geq a} x}{N} \&amp;\geq \frac {aN_{x \geq a}}{N} \&amp;= aP(X \geq a)\end{aligned}$$ 即有$$E(X) \geq aP(X \geq a)$$ 不等式两边同时除以$a$， $$\frac {E(X)}{a} \geq P(X \geq a)$$ 原式得证。]]></content>
      <categories>
        <category>概率统计</category>
      </categories>
      <tags>
        <tag>马尔科夫不等式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客测试专用]]></title>
    <url>%2F2017%2F11%2F30%2Fblog-test%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[理解信息熵]]></title>
    <url>%2F2017%2F11%2F29%2Finformation-entropy%2F</url>
    <content type="text"><![CDATA[如果$X$是一个离散型随机变量，取值空间为$R$，其概率分布为$p(x)=P(X=x),x \in R$。那么，$X$的熵$H(X)$定义为： $$H(X)=H(p)=- \sum_{x \in R} p(x)\log p(x)$$ 先假想第一个场景，先知(prophet)把一枚均匀的骰子🎲放在黑盒子里，摇了一摇，问你“骰子点数$x$是多少？”。你只能瞎猜了，因为你知道骰子1-6个点朝上的可能性都一样（$\frac 1 6$）。这时候，你随便猜一个点数，结果是正确的概率都是$\frac 1 6$。但是，如果场景中的骰子并不是均匀的，例如有一个不均匀的骰子1,2,3,4,5,6点朝上的概率分别为$\frac {1} {2}, \frac {1} {4}, \frac {1} {8}, \frac {1} {16}, \frac {1} {32}, \frac {1} {32}$，这时候，你应该会比较有把握地猜点数是1，而不是像上一个场景那样瞎猜了。从这里可以看出，一个概率分布均匀的变量具有的不确定性是最大的，而概率分布越不均匀则不确定性越小。假如骰子1-6点朝上的概率是$\frac {2} {3}, \frac {3} {30}, \frac {3} {30}, \frac {2} {30}, \frac {1} {30}, \frac {1} {30}$，那么或许你就会更有把握说黑盒里的骰子1点朝上了。通过刚才的几个概率分布的例子，可能会让你觉得，变量$x$的不确定性(熵)只取决于最高的概率值。而事实上，系统整体的不确定性由全体概率一起决定。在上述场景中，我们尚没考量猜骰子点数这个过程的全貌。好了，先知现在告诉你，你刚才猜错了，让你再猜，直到猜中为止。那么现在，你就需要考虑余下的概率了（最佳的策略就是每一步都猜余下概率最大的点数）。同样，评估一个系统的不确定性，也需要考虑系统的所有可能性(概率)，而不只是考虑某一个猜测的可能性(概率)。系统的不确定性是由其概率分布决定的。而信息论中度量这种不确定性的方法正是信息熵。信息的作用在于消除不确定性。那么，变量的概率分布与信息有什么联系呢？ We begin by considering a discrete random variable $x$ and we ask how much information is received when we observe a specific value for this variable. The amount of information can be viewed as the ‘degree of surprise’ on learning the value of $x$. If we are told that a highly improbable event has just occurred, we will have received more information than if we were told that some very likely event has just occurred, and if we knew that the event was certain to happen we would receive no information. Our measure of information content will therefore depend on the probability distribution $p(x)$, and we therefore look for a quantity $h(x)$ that is a monotonic function of the probability $p(x)$ and that expresses the information content. 信息熵是事件不确定性的度量(measurement of unknown)，一个事件(随机变量)的不确定性越大，它的信息熵越大。观察一个随机变量的样本时，如果观察到一个小概率事件发生了，那么我们就获得了较多的信息；与此相反，从一个高频事件中获得到的信息则相对较少。举个例子，考虑明天撒哈拉沙漠是否会下雨这件事，先知告诉你，“明天撒哈拉沙漠会下雨”，你会很惊讶；而如果先知告诉你，“明天撒哈拉沙漠不会下雨”，你觉得这个信息没什么作用，因为你已经相当有把握撒哈拉沙漠不会下雨了。换句话说，发生频率越高的事件，它的不确定性程度就越低，观察到这个事件发生时所获得的信息量也就越少，反之亦然。通过上面的分析，信息量依赖于变量的概率分布，并且熵$h(x)$的函数定义应该是单调的(发生的可能性越大，信息量越小)。 The form of $h(·)$ can be found by noting that if we have two events x and y that are unrelated, then the information gain from observing both of them should be the sum of the information gaine dfrom each of them separately, so that $h(x,y)=h(x)+h(y)$. Two unrelated events will be statistically independent and so $p(x,y)=p(x)p(y)$. From these two relationships, it is easily shown that h(x) must be given by the logarithm of $p(x)$ and so we have $$h(x) = -\log p(x)$$ where the negative sign ensures that information is positive or zero. 假设现在发送一个随便变量$x$的值，那么发送的平均信息量就是$x$的基于其分布$p(x)$的期望， $$H(X) = - \sum_x {p(x) \log p(x)}$$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>信息熵</tag>
        <tag>信息论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark完全分布式集群部署]]></title>
    <url>%2F2017%2F11%2F27%2Finstall-apache-spark-on-multi-node-cluster%2F</url>
    <content type="text"><![CDATA[VMware Workstation安装Ubuntu虚拟机安装Ubuntu虚拟机前断网，否则Ubuntu在安装过程中可能会下载更新，导致安装速度缓慢。可以先安装虚拟机，然后在每个虚拟机环境下重复部署安装Spark的操作，也可以先在一台虚拟机上搭建好Spark环境，然后克隆该虚拟机得到集群。Spark集群(Hadoop集群亦类似)需要统一的用户，因此新建虚拟机时，填写的用户名保持一致，可以避免安装后再新建统一用户。下面全部用cheung作为统一的用户名。密码也使用统一的密码。 搭建Spark集群更改软件源安装vim1cheung@ubuntu:~$ sudo apt-get install vim 修改主机名为了用主机名代替IP进行Hadoop集群内主机之间的通讯，需要修改各个主机的host以及hostname/etc/hostname是Ubuntu系统的主机名配置文件，按照上述过程安装完Ubuntu，主机名默认是ubuntu。打开命令行可以观察到前缀为cheung@ubuntu:~$。使用vim(或者gedit,nano等编辑器)打开/etc/hostname，将主机名其修改为自己需要的名称，四台机器的主机名分别改为sun,mercury,venus,earth。[注：太阳Sun及距离其最近的三大行星(水星-Mercury、金星-Venus、地球-Earth)，主机名一律使用小写。] 文件/etc/hostname 1ubuntu 修改为 1sun 文件/etc/hosts 123456789127.0.0.1 localhost127.0.1.1 ubuntu# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 修改为(注释掉第二行) 123456789127.0.0.1 localhost#127.0.1.1 ubuntu# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 上面127.0.1.1 ubuntu那一行必须注释掉或者直接删除，否则会导致集群的worker主机群无法连接上master。注意，其他三台主机(水星-Mercury、金星-Venus、地球-Earth)的/etc/hostname文件内容分别修改为mercury,venus,earth。 同时，/etc/hosts文件的第二行亦作同样的修改。重启即可使更改生效，再次打开命令行，可以看到控制台命令的前缀已经变为cheung@sun:~$ 添加IP与主机名的映射在每个虚拟机的/etc/hosts中添加Spark集群中每个虚拟机的ip地址和主机名(hostname)的映射：[注：Linux查看IP的命令是$ ifconfig] 1234192.168.118.130 sun192.168.118.129 mercury192.168.118.131 venus192.168.118.132 earth 安装并配置ssh Hadoop集群的master与slave之间，slave与slave之间都存在互相通信，因此要保证集群的所有机器之间能通过ssh无密码登陆。例如，master向slave派发任务，slave上报block信息给master，以及slave之间的文件互传(冗余备份)。 1$ sudo apt-get install openssh-server 生成公钥和私钥： 1$ ssh-keygen -t rsa -P "" 将集群中每一台机器的~/.ssh/id_rsa.pub(公钥)的内容复制到所有节点(包括自己)的.ssh/authorized_keys文件末尾。对于这一步，有很多方法，ssh有个专用命令ssh-copy-id可以将本机的公钥复制到远程机器的authorized_keys文件中： 1234ssh-copy-id -i ~/.ssh/id_rsa.pub cheung@sunssh-copy-id -i ~/.ssh/id_rsa.pub cheung@mercuryssh-copy-id -i ~/.ssh/id_rsa.pub cheung@venusssh-copy-id -i ~/.ssh/id_rsa.pub cheung@earth 注意，上面的四条添加公钥到authorized_keys的命令要在每台机器上执行！再次重申“Hadoop集群的master与slave之间，slave与slave之间都存在互相通信，因此要保证集群的所有机器之间能通过ssh无密码登陆。”通过以下命令验证是否能无密码登陆： 1234$ ssh sun$ ssh mercury$ ssh venus$ ssh earth 配置正确的话，上面的ssh命令无需输入密码就可以直接登陆对应的虚拟机。 安装Spark运行环境所需的软件 jdk-8u152-linux-x64.tar.gz scala-2.11.12.tgz spark-2.2.0-bin-hadoop2.7.tgz 上述三个文件全部放在Downloads目录下。 安装JDK准备把软件都安装到/opt目录下，为避免一直需要管理员权限，修改/opt目录的所有者为cheung: 1$ sudo chown -R cheung:cheung /opt 命令行cd ~/Downloads/进入Downloads文件夹。 解压： 1$ tar -zxvf jdk-8u152-linux-x64.tar.gz 移动至/opt目录下并同步修改目录名为jdk： 1$ sudo mv jdk1.8.0_152/ /opt/jdk ls /opt可以看到/opt目录下的jdk文件夹。 配置jdk环境变量： 1$ sudo vim /etc/profile 在末尾添加jdk环境变量： 12345# JDKexport JAVA_HOME=/opt/jdkexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 运行以下命令让jdk环境变量生效： 1$ source /etc/profile 检查java jdk是否安装成功： 1$ java -version 出现类似输出表示jdk安装成功： 1234$ java -versionjava version "1.8.0_152"Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode) 安装Scala步骤与安装java jdk如出一辙。 命令行cd ~/Downloads/进入Downloads文件夹。 解压： 1$ tar zxvf scala-2.11.12.tgz 移动至/opt目录下并同步修改文件名为jdk： 1$ sudo mv scala-2.11.12/ /opt/scala ls /opt可以看到/opt目录下的scala文件夹。 配置scala环境变量： 1$ sudo vim /etc/profile 在末尾添加scala环境变量： 123# Scalaexport SCALA_HOME=/opt/scalaexport PATH=$&#123;SCALA_HOME&#125;/bin:$PATH 运行以下命令让scala环境变量生效： 1$ source /etc/profile 检查scala是否安装成功： 1$ scala -version 出现类似输出表示scala安装成功： 12$ scala -versionScala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL 安装Spark命令行cd ~/Downloads/进入Downloads文件夹。 解压： 1$ tar zxvf spark-2.2.0-bin-hadoop2.7.tgz 移动至/opt目录下并同步修改文件名为spark： 1$ $ sudo mv spark-2.2.0-bin-hadoop2.7 /opt/spark ls /opt可以看到/opt目录下的spark文件夹。 配置spark环境变量： 1$ sudo vim /etc/profile 在末尾添加spark环境变量： 123# Sparkexport SPARK_HOME=/opt/sparkexport PATH=$&#123;SPARK_HOME&#125;/bin:$PATH 运行以下命令让spark环境变量生效： 1$ source /etc/profile 配置Spark修改配置文件slaves1$ cd /opt/spark/conf 在conf目录下新建slaves文件： 1$ vim slaves 在slaves中输入Spark所有的slave节点的hostname： 123mercuryvenusearth 修改配置文件spark-env.sh1$ cd /opt/spark/conf 通过复制spark自带的模板来创建spark-env.sh： 1$ cp spark-env.sh.template spark-env.sh 在spark-env.sh末尾添加 1export JAVA_HOME=/opt/jdk 将程序打包发送到各个主机12345$ cd /opt$ sudo tar zcvf pack.tar.gz jdk/ scala/ spark/$ sudo scp pack.tar.gz cheung@mercury:/home/cheung/Documents/$ sudo scp pack.tar.gz cheung@venus:/home/cheung/Documents/$ sudo scp pack.tar.gz cheung@earth:/home/cheung/Documents/ 在各个slave节点解压程序解压到/opt目录 12$ cd /home/cheung/Documents/$ sudo tar zxvf pack.tar.gz -C /opt/ [注：别忘了在slave节点虚拟机上添加Java、Scala和Spark的环境变量] Spark运行环境启动与关闭Spark启动Spark12$ cd /opt/spark/sbin$ ./start-all.sh 检查 1cheung@sun:/opt/spark/sbin$ jps 4369 Jps4289 Master 关闭Spark12$ cd /opt/spark/sbin$ ./stop-all.sh SparkUI在浏览器打开网页http://master-ip:8080/可以看到Spark集群的信息。[master-ip用Spark集群Master的IP替换] 参考文献：https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯公式]]></title>
    <url>%2F2017%2F11%2F25%2Fbayes-rule%2F</url>
    <content type="text"><![CDATA[全概率定理设$A_1,A_2,\cdots,A_n$是一组互不相容的事件，形成样本空间的一个分割（每一个实验结果必定使得其中一个事件发生）。又假定对每一个$i$，$P(A_i) &gt; 0$。则对于任何事件$B$，下列公式成立$$P(B) = P(A_1 \cap B) + \cdots + P(A_n \cap B) = P(A_1)P(B \mid A_1) + \cdots + P(A_n)P(B \mid A_n)$$ 贝叶斯定理条件概率$P(A \mid B)$表示在事件$B$发生的条件下$A$发生的概率。并且有如下的链式法（联合概率和条件概率的关系）： $$P(AB) = P(A \cap B) = P(A \mid B)P(B) = P(B \mid A)P(A)$$ 由$P(A \mid B)P(B) = P(B \mid A)P(A)$可以推导出贝叶斯定理： $$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}$$ 或者， $$P(A \mid B) = \frac{P(AB)}{P(B)} = \frac{P(B \mid A)P(A)}{P(B)}$$ 如果对上面贝叶斯公式的分母$P(B)$根据全概率公式分割（$A$和$\bar{A}$是样本空间的一个分割）， $$P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)} = \frac{P(B \mid A)P(A)}{P(B \mid A)P(A) + P(B \mid \bar{A})P(\bar{A})}$$ 更一般地（$A_1, \cdots, A_n$是样本空间的一个分割）， $$P(A_i \mid B) = \frac{P(B \mid A_i)P(A_i)}{P(B)} = \frac{P(B \mid A_i)P(A_i)}{P(B \mid A_1)P(A_1) + \cdots + P(B \mid A_n)P(A_n)} = \frac{P(B \mid A_i)P(A_i)}{\sum_{k=1}^{n}P(B \mid A_k)P(A_k)}$$ 这就是一般形式的贝叶斯公式。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯公式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows系统中搭建Spark开发环境]]></title>
    <url>%2F2017%2F11%2F13%2Fspark-development-with-intellij-idea%2F</url>
    <content type="text"><![CDATA[系统环境 Windows 10 jdk1.8 scala2.11.8 Intellij IDEA 2017.1.4 winutil.exe (下载地址: https://github.com/steveloughran/winutils) spark-2.2.0-bin-hadoop2.7.tgz (下载地址: http://spark.apache.org/downloads.html) 先安装JDK, Scala 和 Intellij IDEA。某些功能可能较新版本的Intellij IDEA才具备(如本文使用的IDEA-based Scala project)，建议升级使用2017.x之后的版本。此外，给JDK和Scala配置好环境变量能避免不少问题，通过下面两个命令检查是否已经配置好环境变量。 1java -version 1scala -version 注：scala版本需要与spark版本兼容，否则编译报错NoSuchMethodError。官网下载Spark时提示Spark2.0之后是使用Scala2.11编译的，因此选择使用Scala2.11.x。关于各个Spark版本适用的Scala可以到Spark官方文档目录 查询。该页面下，根据你选择的Spark版本，点击进入对应的目录，然后在副标题 Downloading 那一段查看该Spark版本支持的Scala版本。如下图： 新建Scala项目打开IEDA, File → New → Project → Scala → 项目类型选IEDA → Next → 选择Scala SDK位置 → FinishScala项目类型那一步选择IDEA，如下图，这个类型在较新版本的Intellij中才有（因此建议升级2017.x之后的版本），感觉比SBT好用很多（SBT要修改配置文件、写入库依赖关系，比较麻烦）： 新工程命名为SparkDemo。 新建立好的工程目录如下图： 测试程序在Project的src目录下新建Scala Class（选中src目录，鼠标右键，选择New → Scala Class），类型(kind)选择object，命名为HelloSpark，如下图： 在新建的HelloSpark.scala中贴入以下测试代码（统计HelloSpark.scala文件中字母a和字母b的出现次数）： 12345678910111213import org.apache.spark.&#123;SparkConf, SparkContext&#125;object HelloSpark &#123; def main(args: Array[String]) &#123; val logFile = "src/HelloSpark.scala" val conf = new SparkConf().setAppName("Simple Application").setMaster("local[*]") val sc = new SparkContext(conf) val logData = sc.textFile(logFile, 2).cache() val numAs = logData.filter(line =&gt; line.contains("a")).count() val numBs = logData.filter(line =&gt; line.contains("b")).count() println("Lines with a: %s, Lines with b: %s".format(numAs, numBs)) &#125;&#125; 通过上图可以看到，Intellij IDEA检查到了非常多的错误，此时的程序由于缺少Spark相关的jar包而无法编译通过。现在我们给当前这个工程导入Spark相关的jar包。菜单栏 File → Project Structure → 左侧Project Settings点击选中Libraries → 点击中间的”+”然后选择Java → 选中spark目录下的jars目录 → OK → Apply → OK 稍等片刻，程序运行所需要的jar包就会导入完毕，再次查看HelloSpark.scala，会发现编辑器中的红色错误提示都消失了。如果继续添加代码，会发现已经有Spark API的智能提示了。 现在，已经可以运行这个Scala-Spark程序了。在控制台输出中，你可能会看到这样一个错误：(这个错误并不会导致程序中断，也不会影响正确结果的输出，但程序出现Error很难受，下面有消除这个错误的方法) 12ERROR Shell: Failed to locate the winutils binary in the hadoop binary pathjava.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries. 这是因为HADOOP_HOME环境变量为空，我们需要添加HADOOP_HOME环境变量。下载 winutil.exe (下载地址: https://github.com/steveloughran/winutils) 将 winutil.exe 放置于路径 D:\winutil\bin\winutil.exe 下。在系统环境变量添加HADOOP_HOME变量，路径值设置为 D:\winutil在系统Path环境变量中添加;%HADOOP_HOME%\bin（分号“;”是为了与Path中已有的路径分隔）重启系统让环境变量刷新，再次运行程序就不会再出现上面的错误了。如果运行程序时还出现了以下错误， 123456789Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: scala.Predef$.$conforms()Lscala/Predef$$less$colon$less; at org.apache.spark.util.Utils$.getSystemProperties(Utils.scala:1749) at org.apache.spark.SparkConf.loadFromSystemProperties(SparkConf.scala:73) at org.apache.spark.SparkConf.&lt;init&gt;(SparkConf.scala:68) at org.apache.spark.SparkConf.&lt;init&gt;(SparkConf.scala:55) at HelloSpark$.main(HelloSpark.scala:10) at HelloSpark.main(HelloSpark.scala)Process finished with exit code 1 这是Scala与Spark版本不对应导致的，到 Spark官方文档目录 查看与所使用的Spark版本对应的Scala版本，安装相应的Scala版本。修正所有错误之后，观察输出，可以看到一行： 1Lines with a: 10, Lines with b: 3 程序没有因为异常而中断的话，Windows下Spark开发环境搭建成功。 生成jar包在开发环境中把spark scala代码写好后需要把程序部署到服务器，这时候需要打包程序。打包过程如下：菜单栏 File → Project Structure, 然后根据下图操作：步骤3中点击“+”，选择“JAR”，右侧展开栏选择“From modules with dependencies” 菜单栏 Build → Build Artifacts → Build稍等片刻，编译通过后，Intellij IDEA底部会出现“Compilation completed successfully in *s *ms”的提示。观察工程目录，会发现已经生成了HelloSpark.jar，如下图所示： 嗯，如果现在把程序部署到服务器，你会发现这个HelloSpark.jar包出乎意料的大，约200MB…这是因为编译的时候IEDA把Spark和Scala的相关库也打包了，因为服务器上已经安装了Scala并且搭建好了Spark运行环境，因此我们并不需要将Spark和Scala相关的库打包进目标程序的jar包。再次打开Project Structure作最后的修改：菜单栏 File → Project Structure → Artifacts，界面如下： 选中那些Spark和Scala相关的jar包，点击中间上方的“-”按钮将其删除，然后点击右下角的“Apply”使修改生效。最后，菜单栏 Build → Build Artifacts → Rebuild再次查看生成的HelloSpark.jar的文件大小，大约只有几KB，大功告成。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派安装指南]]></title>
    <url>%2F2017%2F10%2F27%2Fsetup-raspberrypi%2F</url>
    <content type="text"><![CDATA[硬件准备 树莓派3B Class 10 SD卡(容量不小于8G) 读卡器 树莓派3B主板如下图： 为了缓解小机器的散热问题，在CPU、GPU和内存区域贴上了散热片。为保证树莓派系统的运行速度，Class 10的SD卡非常重要。容量不小于8G，我用的是16G的。如果是新卡，直接进入下面的步骤就可以了。如果已经在卡上烧录过树莓派，那么其分区已经改变(多出一个boot分区)，可以使用Windows自带的磁盘管理将SD卡的分区删除再新建卷，实现格式化。 准备操作系统镜像和烧录工具2017-09-07-raspbian-stretch-lite.zip 这个解压会得到树莓派的光盘镜像文件树莓派系统下载地址 https://www.raspberrypi.org/downloads/raspbian/ win32diskimager-1.0.0-install.exe Win32磁盘映像工具win32diskimager下载地址 https://sourceforge.net/projects/win32diskimager/ 树莓派系统烧录在Windows中安装win32diskimager，并使用管理员权限打开。win32diskimager打开界面中“映像文件”一栏选中解压出来的树莓派镜像，设备栏选中SD卡(一般会默认选中，确认一下)。点击写入，等待写入完成。（点击写入后，会弹出提示“写入物理设备可能会造成该设备不可使用.是否继续”，不必理会，点击“yes”继续） 完成后会提示“写入成功”。写入成功后，可以看到SD卡被切分为两部分（boot就是树莓派的boot分区），如下图： 刚烧录好的SD卡直接插入树莓派上电开机的话，是无法对树莓派进行操作的，因为无法ssh登陆。先做完下面一步再插卡开机。 让树莓派开启SSH服务在的boot分区下新建一个空白文件，命名为ssh，不要有任何后缀。树莓派启动后检测到这个文件会自动启用ssh服务。随后就可以通过ssh连接到树莓派了。至于怎么找到树莓派的IP地址，下文再说。 开机、查找树莓派IP地址插入SD卡，接上网线，打开电源。看到树莓派几个指示灯正常： 这时候，我们需要找到树莓派的IP才能使用ssh连接到机器上。一般使用路由器查询。家用网络中，打开路由器的管理界面，可以看到连接在线的设备，找到raspberrypi（树莓派Raspbian系统的默认主机名是raspberrypi），查看对应的IP得知即可获取到树莓派的IP。 找到IP之后就可以使用熟悉的VNC客户端连接到树莓派了（如Xshell、SecureCRT、Putty等）。树莓派Raspbian系统默认登录用户名为pi，该账户默认密码是raspberry。登陆后会出现以下提示，提示更改密码。 12SSH is enabled and the default password for the &apos;pi&apos; user has not been changed.This is a security risk - please login as the &apos;pi&apos; user and type &apos;passwd&apos; to set a new password. 更改树莓派软件源树莓派默认的软件源在国外，速度不太理想，因此有必要更换为国内的软件源。可用的中国软件源可查看Raspbian中国软件源。 1pi@raspberrypi:~ $ sudo nano /etc/apt/sources.list 将原来sources.list中的内容注释掉（行首加#），添加后面两行。这里使用阿里云的软件源。 12345#deb http://mirrordirector.raspbian.org/raspbian/ stretch main contrib non-free rpi# Uncomment line below then 'apt-get update' to enable 'apt-get source'#deb-src http://archive.raspbian.org/raspbian/ stretch main contrib non-free rpideb http://mirrors.aliyun.com/raspbian/raspbian/ stretch main contrib non-free rpideb-src http://mirrors.aliyun.com/raspbian/raspbian/ stretch main contrib non-free rpi 更新软件源并升级软件： 12pi@raspberrypi:~ $ sudo apt-get updatepi@raspberrypi:~ $ sudo apt-get upgrade 至此，树莓派已经安装完成，利用它去开发好玩的东西吧！]]></content>
      <categories>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>树莓派</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量空间模型]]></title>
    <url>%2F2017%2F10%2F11%2Fvector-space-model%2F</url>
    <content type="text"><![CDATA[词项权重计算词项频率(term frequency)在布尔检索模型中，只考虑了词项在文档中出现与否，给定一个布尔查询，一篇文档要么满足查询要求要么不满足，返回的文档没有排序。对于Web搜索引擎，用户往往需要浏览非常多的网页才能找到需要的信息。如何才能对检索的文档进行评分和排序呢，一个合理的想法是，如果一篇文档包含的查询词的数目越多，那么这篇文档与查询相关的可能性就越高，就意味着更可能是用户所需要的文档。[注：如果只是考虑词频，那么长文本会更可能包含更多的查询词而获得评分优势，我们需要消除文档长度对评分的影响，这也是向量空间模型采用余弦相似度的原因，从而实现文档长度归一化]在向量空间模型中，我们先基于每个查询词项与文档的匹配情况对文档打分，然后对所有查询词项在文档中的得分求和，作为文档对于这个查询的总得分，得分高的文档在返回结果中排在前面。 那么该如何根据相应的查询给文档打分呢？在向量空间模型中，我们根据文档的词项频率(term frequency)给文档打分。一个简单的做法是，对于一个查询（包含一个或者多个查询词），我们将查询词t在文档中出现的次数作为t在文档中的权重，对所有查询词在文档中的权重求和作为文档对于该查询的得分。也就是说，将词项频率(term frequency)作为权重。我们将词项频率（term frequency, 简写tf）记为$tf_{t,d}$，意为词项t在文档d中出现的次数。 这种忽略词项在文档中的次序关系，将文档看作词项的集合的模型，称为词袋模型(bag of words model)。 逆文档频率(inverse document frequency)使用词项频率作为权重有一个严重缺陷，它无区别地对每一个词项计算权重，而事实上，文档中两个词频相同的词极有可能具有不一样的重要性(亦即权重)。例如，在一个人的日记文档集查询“我 AND 小猫”，像上面所说得简单地通过词频来计算权重的话，统计每一篇日记中“我”出现的频率($tf_{我}$)和“小猫”出现的频率($tf_{小猫}$)，然后把这两个权重相加得到每一篇日记的权重得分，得分越高的日记被认为与查询越有关联。然而，不可忽略的事实是，大量的日记中都会出现“我”，而“小猫”则比较少见。这说明“小猫”会比“我”更有区分度。如果一篇日记中出现了三次“我”的话，即使这篇日记一个“小猫”都没有，也会因为“我”的词频得分高而排在出现了一次“我”和一次“小猫”的日记前面。由于词项“我”在大量文档中出现，会导致最终搜索返回的文档中，排在前面的都是仅仅包含多个“我”的日记文档，而包含“小猫”的文档则淹没在这数不清的“我”的文档中了。 为此，需要一种机制来降低这些在大量文档中都出现的词项在查询得分计算中的重要性。也就是说，文档频率(document frequency, 出现词项t的文档数目)较高的词项（导致区分度较低）给予较低的权重，反之，文档集频率较低的词项（由于区分度较高）给予较高的权重。那么一个合理的词项t的权重公式为$\frac{N}{df_t}$，其中$N$为所有文档的数量，$df_t$为词项t的文档频率(document frequency)。由于$\frac{N}{df_t}$的数值往往比较大，通常会用取对数的方法把它映射到一个较小的取值范围内。最后，我们得到的词项t的idf(inverse document frequency, 逆文档频率)的定义如下：$$idf_t = \log {\frac{N}{df_t}}$$ tf-idf权重计算结合tf（词项频率）和idf（逆文档频率），文档d中词项t的权重得分为：$$tf\text{-}idf_{t,d} = tf_{t,d} \times idf_t$$那么，对于查询q，文档d的得分为每个查询词($t \in q$)在文档中的得分之和：$$Score(q,d) = \sum_{t \in q} tf\text{-}idf_{t,d}$$ 向量空间模型把文档映射为向量我们可以把文档看作一个向量(vector)，假设这个向量有$M$维，把它表示为 $\vec V(d) = [w_1,w_2,\cdots,w_M]$，这个向量的每个分量$w_i$对应词典中的一个词项的权重得分，这个权重得分通过$tf\text{-}idf_{t,d} = tf_{t,d} \times idf_t$来计算。当某词项在文档中没有出现时，其对应的分量值为0。假设文档d对应的向量用$\vec V(d)$表示，每个分量对应一个词项的权重得分(如tf-idf)。为了修正文档长度给相似度计算带来的影响，采用余弦相似度来评估文档$d_1$和$d_2$的相似度：$$sim(d_1,d_2) = \frac {\vec V(d_1) \cdot \vec V(d_2)} {|\vec V(d_1)| |\vec V(d_2)|}$$通过余弦相似度，我们可以判断两篇文档的近似程度了，搜索引擎可以根据这个给出与用户当前浏览的文档相似的文档，这个在论文或专利搜索系统中特别常见。我们分析余弦相似度的计算公式：$$sim(d_1,d_2) = \frac {\vec V(d_1) \cdot \vec V(d_2)} {|\vec V(d_1)| |\vec V(d_2)|} = \frac {\vec V(d_1)}{|\vec V(d_1)|} \cdot \frac {\vec V(d_1)}{|\vec V(d_1)|}$$分母$|\vec V(d_1)|$和$|\vec V(d_2)|$的效果实际上是对向量$\vec V(d_1)$和$\vec V(d_2)$的长度进行归一化，从而得到两个文档向量对应的单位向量：$\vec v(d_1) = \frac {\vec V(d_1)}{|\vec V(d_1)|}$，$\vec v(d_2) = \frac {\vec V(d_2)}{|\vec V(d_2)|}$。用单位向量的形式，我们把上面计算余弦相似度的公式重写为：$$sim(d_1,d_2) = \vec v(d_1) \cdot \vec v(d_2)$$ 查询向量用户输入的查询也可以表示为向量。查询向量对应的查询词$q_i$分量的计算过程如下：$$\vec v_{i}(q) = \frac {tf_{q_i}}{\sum_{i=0}^m tf_{q_i}^2}$$ 考虑查询q=“cat dog”。我们将查询语句也看作一篇文档，长度为2，只有两个词(cat和dog，记为$q_1$和$q_2$)，分别都只出现一次，那么可以查询向量计算公式将该查询转化为单位向量。因为词典中其他词都没有在查询中出现，tf为0，对公式计算没有影响，因此不必考虑未出现在查询中的词项。$$\vec v_{q_1}(q) = \vec v_{q_2}(q) = \frac {tf_{q_1}} {\sqrt{\sum_{i=0}^m tf_{q_i}^2}} = \frac {tf_{q_1}} {\sqrt{tf_{q_1}^2 + tf_{q_2}^2}} = \frac {1} {\sqrt{1^2 + 1^2}} = \frac {\sqrt{2}}{2}$$ 查询q与文档d的余弦相似度计算：$$sim(q,d) = score(q,d) = \frac {\vec V(q) \cdot \vec V(d)} {|\vec V(q)| |\vec V(d)|} = \vec v(q) \cdot \vec v(d)$$ 对于给定的查询，通过计算查询向量与文档向量的余弦相似度来对所有文档进行相似度打分、排名，进而从结果中选择排名靠前的一些文档展示给用户。]]></content>
      <categories>
        <category>信息检索</category>
      </categories>
      <tags>
        <tag>向量空间模型</tag>
        <tag>信息检索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo搭建个人博客]]></title>
    <url>%2F2017%2F10%2F09%2Fcreate-a-blog-with-hexo%2F</url>
    <content type="text"><![CDATA[预装软件先安装好Git, Node.js: 1$ sudo apt-get install git 到Node.js官方下载页面下载Linux版本的二进制文件, 这里下载的版本为node-v10.13.0-linux-x64.tar.xz解压后放到/opt目录下并重命名为nodejs: 123$ xz -d node-v10.13.0-linux-x64.tar.xz$ tar xvf node-v10.13.0-linux-x64.tar$ sudo mv node-v10.13.0-linux-x64 /opt/nodejs 在/etc/profile或者~/.bash_profile文件末尾处添加环境变量: 1$ sudo vim /etc/profile 12export NODEJS_HOME=/opt/nodejsexport PATH=$&#123;NODEJS_HOME&#125;/bin:$&#123;PATH&#125; source命令读入/etc/profile使得环境变量在当前shell窗口立即生效: 1$ source /etc/profile 使用Linux作为宿主操作系统的话, 要确保当前用户对主站所在目录具有读写相关权限。否则会出现类似下面的错误: 123456felix@thinkpad:/cheung.site$ hexo initINFO Cloning hexo-starter to /cheung.siteCloning into '/cheung.site'.../cheung.site/.git: Permission deniedWARN git clone failed. Copying data insteadWARN Failed to install dependencies. Please run 'npm install' manually! 建站Hexo中文网提供了很详细的说明, 首次折腾Hexo的话建议前往看看, Hexo中文网传送门。 1$ npm install -g hexo-cli [npm install -g xxx中的-g参数表示全局安装对应的xxx包, xxx会被安装到Nodejs的安装目录下] 123$ cd your-hexo-site$ hexo init$ npm install 把your-hexo-site替换为你自己的为站点新建的目录路径, 下文把这个目录称为站点根目录。上面几条命令之后, 站点根目录下会有不少文件, 如下: 12345678felix@thinkpad:/cheung.site$ ls -l-rw-rw-r-- 1 felix felix 1765 10月 8 21:54 _config.ymldrwxrwxr-x 286 felix felix 12288 10月 8 21:55 node_modules-rw-rw-r-- 1 felix felix 443 10月 8 21:54 package.json-rw-rw-r-- 1 felix felix 135766 10月 8 21:55 package-lock.jsondrwxrwxr-x 2 felix felix 4096 10月 8 21:54 scaffoldsdrwxrwxr-x 3 felix felix 4096 10月 8 21:54 sourcedrwxrwxr-x 3 felix felix 4096 10月 8 21:54 themes 现在, 在站点根目录下执行hexo s, 本地浏览器打开http://localhost:4000/就能进行预览了。[非本地浏览器可以通过http://ip-address:4000/访问] 站点目录介绍站点根目录下的_config.yml为站点配置文件, source目录存放站点的资源(如博客的md源文件), themes目录存放Hexo站点的主题, 默认的landscape主题, 比较简陋。主题目录下也有一个_config.yml配置文件, 我们把这个叫主题配置文件, 与站点根目录下的站点配置文件区别开来。主题配置文件主要用于设置主题相关的资源摆放、页面内容展示、启用何种插件等内容。 站点部署安装hexo-deployer-git插件作为部署站点的工具: 12$ cd your-hexo-site$ npm install hexo-deployer-git --save 生成静态文件并部署到Github, 中间会提示输入Github用户名和密码: 12$ hexo g$ hexo d 打开https://wahcheung.github.io/就可以看到线上的主站了(将wahcheung替换为你的Github用户名). 安装Next主题Next主站给出了详细的攻略, Next使用文档传送门, 这里只是简单地复述安装过程。下载(克隆)Next主题: 12$ cd your-hexo-site$ git clone https://github.com/theme-next/hexo-theme-next themes/next 下载完成后, 打开站点配置文件, 找到theme字段, 并将其值更改为next。Next主题可供自定义的设置非常丰富, 具体查看主题配置文件以及查阅 Next使用说明 创建页面创建”关于我”页面, 参考 创建”关于我”页面。 1$ hexo new page "about" hexo-site/source/about/index.md的markdown头要添加并设置属性type为about。 123456---title: 关于作者date: 2018-01-05 13:28:26type: aboutcomments: true--- 创建”分类”页面, 参考 创建”分类”页面。 1$ hexo new page "categories" hexo-site/source/categories/index.md的markdown头要添加并设置属性type为categories。 123456---title: 分类date: 2018-01-05 13:29:37type: categoriescomments: false--- 创建”标签云”页面, 参考 创建”标签云”页面。 1$ hexo new page "tags" hexo-site/source/tags/index.md的markdown头要添加并设置属性type为tags。 123456---title: 标签云date: 2018-01-05 13:29:30type: tagscomments: false--- 最后, 别忘了在Next主题配置文件中打开Menu Settings的相关设置: 123456789menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 个性签名在网站侧边栏头像下方添加个性签名, 方法很简单, 在站点配置文件开头的description设置喜欢的个性签名。 设置RSS订阅源安装hexo-generator-feed插件: 12$ cd hexo-site$ npm install hexo-generator-feed --save 安装完成后hexo clean &amp;&amp; hexo server就可以看到侧边栏出现RSS订阅源。 启用Mathjax新版本的Next主题集成了Mathjax, 在主题配置文件中打开Mathjax支持: 12345678# Math Equations Render Supportmath: enable: true per_page: true engine: mathjax mathjax: # Use 2.7.1 as default, jsdelivr as default CDN, works everywhere even in China cdn: //cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML 由于Hexo默认使用的marked.js在解析markdown时对_和\\的处理与Latex的这两个符号冲突了, 导致使用mathjax渲染带下标和换行的数学公式时会出错, 常用的解决办法就是修改marked.js对这两个字符的渲染方式, 具体修改如下: 编辑hexo-site/node_modules/marked/lib/marked.js, 找到下面的代码: 12345678910111213141516171819/** * Inline-Level Grammar */var inline = &#123; escape: /^\\([\\`*&#123;&#125;\[\]()#+\-.!_&gt;])/, autolink: /^&lt;(scheme:[^\s\x00-\x1f&lt;&gt;]*|email)&gt;/, url: noop, tag: /^&lt;!--[\s\S]*?--&gt;|^&lt;\/?[a-zA-Z0-9\-]+(?:"[^"]*"|'[^']*'|\s[^&lt;'"&gt;\/\s]*)*?\/?&gt;/, link: /^!?\[(inside)\]\(href\)/, reflink: /^!?\[(inside)\]\s*\[([^\]]*)\]/, nolink: /^!?\[((?:\[[^\[\]]*\]|\\[\[\]]|[^\[\]])*)\]/, strong: /^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/, em: /^_([^\s_](?:[^_]|__)+?[^\s_])_\b|^\*((?:\*\*|[^*])+?)\*(?!\*)/, code: /^(`+)\s*([\s\S]*?[^`]?)\s*\1(?!`)/, br: /^ &#123;2,&#125;\n(?!\s*$)/, del: noop, text: /^[\s\S]+?(?=[\\&lt;!\[`*]|\b_| &#123;2,&#125;\n|$)/&#125;; 将 1escape: /^\\([\\`*&#123;&#125;\[\]()#+\-.!_&gt;])/, 替换为 1escape: /^\\([`*\[\]()#+\-.!_&gt;])/, 将 1em: /^_([^\s_](?:[^_]|__)+?[^\s_])_\b|^\*((?:\*\*|[^*])+?)\*(?!\*)/, 替换为 1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 展示一个数学公式的示例: $$P(A_i \mid B) = \frac{P(B \mid A_i)P(A_i)}{P(B)} = \frac{P(B \mid A_i)P(A_i)}{P(B \mid A_1)P(A_1) + \cdots + P(B \mid A_n)P(A_n)} = \frac{P(B \mid A_i)P(A_i)}{\sum_{k=1}^{n}P(B \mid A_k)P(A_k)}$$ 添加搜索功能需要安装以下两个插件: 12$ npm install hexo-generator-search --save$ npm install hexo-generator-searchdb --save 在主题配置文件中找到local search, 将enable设置为true: 1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: 1 # unescape html strings to the readable one unescape: false hexo clean &amp;&amp; hexo server就可以看到站点顶部出现搜索图标, 点击就会出现搜索框。如果点击之后有个表示正在加载的圈圈一直在转, 大概率是没装hexo-generator-searchdb。 评论系统在Hexo用过最好用的评论系统是Gitment, 但作者已经不维护这个项目了, 前段时间突然发现博客的评论没法使用了。目前还没看到好用的评论系统, 不打算折腾评论功能了。 更多个性化功能特别推荐一个博主总结的一个帖子: hexo的next主题个性化配置教程, 本站的功能都从这里学习得到。 添加域名解析我是在阿里云(万网)买的域名cheung.site, 在阿里云域名控制台中添加域名解析: 记录类型 主机记录 记录值 CNAME www wahcheung.github.io A @ 192.30.252.153 A @ 192.30.252.154 192.30.252.153和192.30.252.154是GitHub提供的两个服务器IP。我们还需要在hexo-site/source/目录下新建CNAME文件, 输入你的域名: 1cheung.site hexo clean &amp;&amp; hexo deploy部署到GitHub之后就可以通过域名访问了。]]></content>
      <categories>
        <category>自娱自乐</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[布尔检索模型]]></title>
    <url>%2F2017%2F10%2F06%2Fboolean-retrieval%2F</url>
    <content type="text"><![CDATA[最近在看《Introduction to Information Retrieval》（中文版为《信息检索导论》，下文简称为“IR”），是最经典的信息检索书籍之一了。由于淞姐要求我细读这本书然后跟同事分享，就有了这个版块，之后会陆续添加后续章节内容。即使是站在巨人的肩膀上了（看了中文版和英文版IR，也从网上搜集了不少内容），但很多细节往往还是需要自己用心体会。从一个读者到一个讲解人，在第一次做分享的时候已经感觉很不容易了，有些东西原来只是一知半解，能自己想清楚但却很难表述。这些内容就是个人的一些读书笔记，希望能给刚好想了解搜索引擎的你带来一些启发。文中会尽量备注英文原版中的术语以免丢失本意。英文电子书可以从官网获取 Introduction to Information Retrieval。 线性扫描和倒排索引从线性扫描讲起如果需要从文档集合$D$中搜索包含某个关键词$k$的文档，最直接的方法就是从头到尾扫描文档集$D$，对每个文档$d_i$都查看是否包含关键词$k$。这种线性扫描的方式最为直观易懂，在Unix/Linux系统中的文本扫描命令grep做的就是这种工作。然而，当需要检索的文档规模非常大时，这种线性扫描的方式的效率会变得非常低下。线性扫描的时间复杂度与文档集大小成正比，在大规模文本检索的场景下，线性扫描不再适用。大型的Web搜索引擎需要检索千亿级别数量的网页，如果采用类似grep的线性扫描方式，就需要依次扫描这么多的文本来判断每一个网页是否符合查询要求，这样的检索慢如蜗牛，用户显然无法接受。目前，搜索引擎通过事先给文档建立索引（index）的方法来避免这种线性扫描，使得搜索过程非常快速，这种技术称为倒排索引。 IR中，以《莎士比亚全集》为例子来说明倒排索引的基本知识。这里笔者就重新举个例子吧。假设某文档集中存在这样的三篇文档（还有其他文档不列举），分别是$d_1$为“Apple and cat”， $d_2$为“I like cat”，$d_3$为“I have an apple”，用矩阵表示，当文档$d$中存在词项$t$时，矩阵元素$(d, t)$为1，否则为0： 文档\词项 apple and cat I like have an $…$ $d_1$ 1 1 1 0 0 0 0 $…$ $d_2$ 0 0 1 1 1 0 0 $…$ $d_3$ 1 0 0 1 0 1 1 $…$ $…$ $…$ $…$ $…$ $…$ $…$ $…$ $…$ $…$ 表1：文档-词项关联矩阵 倒排索引使用类似grep命令的线性扫描方法的话，如需查询cat相关的文档，需要遍历所有文档（把每篇文档从头到尾扫描一遍），返回包含cat关键词的文档集。观察上面的矩阵表格，查询包含cat关键词的文档，其实只需要查看cat的那一列都有哪些元素大于0的文档就行。利用这种技巧，词项成为索引的基本单位，如果事先建立了类似上表(表1)的矩阵(或者其他形式的数据结构，如链表)，那么就可以快速地找到与查询的关键词相关的文档。我们把上面的矩阵转置一下，把词项(term)作为横坐标，文档(document)作为纵坐标，然后词项按照字母表排序，得到更直观的矩阵： 词项\文档 $d_1$ $d_2$ $d_3$ $…$ an 0 0 1 $…$ and 1 0 0 $…$ apple 1 0 1 $…$ cat 1 1 0 $…$ have 0 0 1 $…$ I 0 1 1 $…$ like 0 1 0 $…$ $…$ $…$ $…$ $…$ $…$ 表2：词项-文档关联矩阵 对于上表(表2-词项-文档关联矩阵)，从行来看，可以表示词项在哪些文档中出现或者不出现；从列来看，可以得到每个文档都有哪些词项或者没有哪些词项。 看到这里，倒排索引的概念已经呼之欲出了。不着急，我们先看看如何利用上面的表2来做搜索查询。布尔检索模型接受布尔表达式查询，用户可以通过使用AND，OR以及NOT等逻辑操作符来构建查询表达式。假如用户提交查询“cat AND NOT apple”（表示寻找带有cat但不含有apple的文档），我们从表2分别取出cat和apple对应的行向量，(110)和(101)，[更规范的写法应该是(1,1,0)和(1,0,1)，此处简写]，然后根据查询做对应的逻辑处理，如下：(110) AND NOT (101) = (110) AND (010) = (010)结果向量(010)中第二个元素为1，表明该查询对应的结果是文档$d_2$。我们来确认一下，文档$d_2$的内容为“I like cat”，确实为查询所需。 读者可能奇怪为什么这种基于位(bit)的逻辑操作可以得到正确的结果。我们举个简单的例子，如果另外一个用户提交了查询“cat AND apple”，我们需要从表2取出cat和apple对应的行向量。读者观察一下表2的那些行向量都是什么形式？cat的行向量(110)的第一位的1表示文档$d_1$包含词项cat，第二位的1表示文档$d_2$包含词项cat，第三位是0，表示文档$d_3$不包含词项cat。词项对应的行向量的第$i$位表明这个词项是否包含于第$i$个文档($d_i$)中！对于查询“cat AND apple”，需要返回同时包含cat和apple的文档。我们取出cat和apple对应的行向量，分别为(110)和(101)，根据“一个词项对应的行向量的第i位表明这个词项是否包含于第i个文档($d_i$)中”，词项cat和apple的行向量的第i位如果都是1表明$d_i$同时包含cat和apple，否则不同时包含cat和apple。因此cat和apple对应的行向量做AND操作可以获得同时包含cat和apple的文档都有哪些。(110) AND (101) = (100)，只有文档$d_1$符合查询要求。我们再看一个例子，对于查询“NOT apple”，需要返回不包含apple的文档。我们从表2取出apple对应的行向量(101)，我们已经知道对应位为1表示文档包含这个词项，那么，现在我们需要查询不包含这个词项的文档，当然就是取反了(NOT操作)。NOT (101) = (010)，结果(010)表示第一个和第三个文档都包含apple，只有第二个文档$d_2$不包含apple。至此，通过这几个简单的例子，读者应该能够理解这种布尔表达式查询的工作原理了。 为了向倒排索引的概念平滑过渡，现在我们考虑一个更真实的场景(使用IR中的例子)。假如我们有100万篇文档(document)，每篇文档大约1000个词，那么，通常这些文档大概会有50万个不同的词项。此时，回头看看表2(词项-文档关联矩阵)，这个矩阵的规模会变得非常巨大，会有50万行(因为有50万的词汇量)和100万列(因为有100万篇文档)，元素数量为5000亿(50万×100万)，存放这张表需要的内存远远大于一台计算机内存的容量！此外，不难发现，这个50万×100万规模的矩阵，大部分元素都是0，可以从某一列来估算，文档$d_i$平均大约有1000个单词，但$d_i$那一列却有50万个元素(因为全局有50万的词汇量)。这么一算，这个词项-文档关联矩阵大约有99.8%的元素都为0。显然，只记录原始矩阵中1的位置的话，所需的存储空间会大大地减少，倒排索引(inverted index)正是起到这样的作用！ 我们观察表1和表2，表1以文档为基本考察要素，而表2以词项为基本考察要素，表2由原始的表1通过inverted(转置，引申为倒排)而来，这正是倒排这个概念的由来。在倒排索引中，我们只存储那些不为0的项。倒排索引的基本结构如下图： 图1(IR图1-3)：倒排索引基本结构 倒排索引带有一个词典(dictionary)，词典中的每个词项(term)都有对应的一个list，保存了这个词项在哪些文档中出现过。词项对应的list中的每一个元素我们称之为一个倒排记录(posting)，而把一个词项对应的整个list称为倒排表(posting list/inverted list)。所有词项的倒排表一起构成postings。[读者对于这几个定义不必太纠结，中文翻译过来感觉怪怪的] 我们对上图中的倒排索引做一个基本说明。以图1中的倒排索引为例，词项Brutus指向的list称为Brutus的posting list，posting list中的每一个元素都称之为posting（如Brutus指向的1,2,4,11,31,45,173,174等，这些posting都是文档ID，表示Brutus在这些文档中出现）。之后我们将会看到，更实用的倒排索引的posting除了存储文档ID，还会保存词项在文档中出现的位置以及词项在该文档中出现的频率，这些都是后话了。 构建倒排索引为获得由索引(indexing)带来的检索速度的提升，我们需要事先构建索引。索引构建分为四个步骤： （1） Collect the documents to be indexed（收集构建索引的文档集，也就是我们检索系统需要检索的所有文档）（2） Tokenize the text, turning each document into a list of tokens（把文本转化为token，就是把文本分割成一个一个的词，中文版的信息检索导论中将token译为词条）（3） Do linguistic preprocessing, producing a list of normalized tokens, whichare the indexing terms（语言学预处理，利用词干还原(stemming)和词形归并(lemmatization)的方法，将token转为normalized token，例如将不同时态的单词转为其词根，将单复数名词统一转为单数形式，还原token的本来形式来获得统一的表示[注：更专业的说法叫“归一化”]，这些还原的token就是将要索引的词项(term)）（4） Index the documents that each term occurs in by creating an inverted index,consisting of a dictionary and postings（对文档中的词项构建倒排索引，最终倒排索引的形式可以参考图1） 步骤1~3是索引构建的基础，我们做一个简要的说明。步骤（1）中收集文档的工作，在Web搜索引擎中就是通过网络爬虫(web crawler)来搜集分布于Web各处的网页。搜集的网页还不能直接用于后面的步骤中，还需要对网页文本做诸如去除标签，过滤广告等处理。步骤（2）就是tokenization(词条化)，例如，对于英文文本而言，就是根据空格把单词一个一个地提取出来，把原始文本分割成token。当然了，如果只是这样简单的分割的话，会把“New York University”分成三个token了，因此英文也会存在短语划分问题。中文文本的话涉及的主要方法就是分词(word segmentation)了，因为汉语字与字之间没有空格，并且由于文法的特殊性，词与词组的边界模糊，这让tokenization的问题更加复杂。中文分词技术属于自然语言处理的范畴，有兴趣的读者可以参考相关的专业文献。步骤（3）就是将步骤（2）产生的token转为更加统一规范的词项(term),例如在文本中可能出现“apple”、“apples”、“Apple”这类token，但我们知道这几个token都是表达苹果(apple)的意思，因此，在构建索引的时候通常会把这几个token统一还原为“apple”，只为“apple”建立索引项，那么“apple”就是一个term(词项)了。倒排索引里面所有的term组成的集合我们称为词典(dictionary,也有称为词汇表(vocabulary)的) 现在，假设前三步都已完成，原始的网页已经被我们分割成一个个的词项(term)。下面，我们体验一下构建基本的倒排索引的过程。 给定一个文档集，给每个文档分配一个唯一的标识符(docID),通过步骤1~3，我们会得到很多的词项，用二元组的形式表示为(词项，文档ID)，表示词项在文档ID对应的文档中出现。布尔检索只考虑词项在文档中有没有出现，因此，即使一个词项在某个文档中多次出现，我们也只记录一个这样的(词项，文档ID)。建立倒排索引的图形化表示如下图2所示： 图2（IR图1-4）：通过排序和合并构建索引的过程 图2中左边部分是由原始文档经过步骤1~3产生的(词项，文档ID)二元组；将这些二元组按照词项的字母顺序进行排序，就会把词项相同的二元组排在一起（图2中部）；最后，把同一词项的多个二元组合并在一起，这个词项出现的文档ID用list存放(也就是posting list)，并且根据文档ID排序。图中所示的倒排索引还存储了词项在文档中出现的次数(在同一个文档中多次出现算一次)。最终，我们形成了一个包含许多词项(term)的字典(dictionary)，每个词项都有一个倒排记录表(posting list)。 至此，倒排索引已经建立好了，那么，如何利用这个倒排索引做布尔查询呢？ 处理布尔查询使用IR中的例子，查询为“Brutus AND Calpurnia”。我们分别从倒排索引表中取出词项Brutus和Calpurnia的posting list，然后对这两个词项的posting list求交集，即可得到查询需要输出的文档集合。如下图， 图3（IR图1-3）：处理布尔查询时对两个倒排记录表求交集 然而，求交集这个操作的时间复杂度应该认真考量！一个显而易见的方法就是在外循环中遍历其中一个posting list，然后在内循环中对另外一个posting list的元素进行匹配查找。然而，这种求交集的方式需要的比较次数为$O(xy)$，其中x、y分别是两个posting list的长度。考虑前面提到过的100万篇文档(document)，每篇文档大约1000个词，这些文档大概会有50万个不同的词项的例子，那么每个posting list的长度大约为2000（$100万×1000÷50万=2000$）。那么就需要做大约400万(2000×2000)次比较才能返回查询结果。在Web搜索引擎中，文档集规模比例子要大得多，查询需要的比较次数就更多了。那么，有没有更优的合并算法呢？回忆我们构建倒排索引的时候，对posting list中的posting根据文档ID排序了，这个时候就可以派上用场了，下面的算法只需要$O(xy)$次操作！ 12345678910111213/* 合并倒排记录表，p1, p2是根据docID排序的有序list，对p1和p2求交集（合并） */Intersect(p1, p2) answer = &lt;&gt; // 初始化取交集的结果 while p1 ≠ NIL and p2 ≠ NIL // list不为空 if docID(p1) == docID(p2) // 两个posting list遇到一样的docID Add(answer, docID(p1)) p1 = p1.next p2 = p2.next else if docID(p1) &lt; docID(p2) // docID小的posting list的指针前移(p1的小) p1 = p1.next else // docID小的posting list的指针前移(p2的小) p2 = p2.next return answer 上面的伪代码是我根据IR书中的伪代码重写的，因为原书中的涉及if-else判断语句的代码缩进非常奇怪，附上原图： 图4（IR图1-6）：两个倒排记录表的合并算法 要使用上述的合并posting list的算法，那么所有的posting list必须按照统一的标准进行排序。这里，我们使用的排序方法是根据全局统一的文档ID(docID)来对posting list排序。对上述代码稍作修改就可以写出对p1,p2求并集的代码(最后需要把非空的list添加到answer中)。而对于NOT查询，只需要在结果中去掉相应的postings。 查询优化通过恰当地组织查询的处理过程可以进一步降低上述合并倒排记录表过程的比较次数。考虑查询： Brutus AND Caesar AND Calpurnia 直接的想法是，我们依次取出Brutus，Caesar和Calpurnia的倒排记录表，然后先合并Brutus和Caesar得到一个中间结果tmp_answer（调用Intersect(Brutus.list, Caesar.list)），然后再把tmp_answer与Calpurnia合并得到最终结果（调用Intersect(tmp_answer, Calpurnia.list)）。然而，一个启发式的算法可能取得更好的效果，降低比较次数。根据词项的文档频率（亦即泡排记录表的长度）从小到大依次进行处理。先合并两个最短的倒排记录表，那么所有中间结果的长度都不会超过最短的倒排记录表，最终需要的比较次数也就很可能最少。 图5（IR图1-7）：多查询词的查询优化-启发式算法 这种启发式算法大部分情况下能取得很好的效果，但在某些情况下并不是最优的，最后，有兴趣的读者可以思考一下如何合并以下的三个倒排记录表才能最优（可见这种启发式算法并不一定能保证比较次数最少）。 123apple → [5,6,10]cat → [3,5,6,10]luffy → [6,11,12,13,14]]]></content>
      <categories>
        <category>信息检索</category>
      </categories>
      <tags>
        <tag>信息检索</tag>
        <tag>布尔检索模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python向MySQL插入特殊字符相关错误的解决办法]]></title>
    <url>%2F2017%2F06%2F30%2Finsert-special-characters-into-mysql%2F</url>
    <content type="text"><![CDATA[背景：这几天在用Python抓取网易云音乐华语歌手的歌曲（歌手-歌曲-专辑），每抓取一个歌手的内容就插入到MySQL数据库中。但歌手的歌名和专辑名常常带有特殊字符（例如单引号、双引号），不做任何处理的话，遇到带特殊字符的数据就会抛出异常，导致数据插入失败。 例如这样一条数据(庾澄庆, 勇气, Lady’s Night)，如果使用下面的插入语句： 1insert into music.songs(singer, song, album) values ('庾澄庆', '心动', 'Lady's Night') 就会抛出如下的异常： 1(1064, &quot;You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;s Night&apos;)&apos; at line 1&quot;) 观察歌曲的专辑名“Lady’s Night”，中间有个引号，导致sql语句被错误解释了。当然你可以写个函数，把单引号、双引号重新转义，但这样太麻烦了。其实，python已经提供了转移函数，可以直接调用。 遇到特殊字符会出错的sql语句形式为： 1sql = 'insert into songs(singer, song, album) values ('%s', '%s', '%s')' % (music[0], music[1], music[2]) 使用MySQLdb.escape_string修正这种错误： 1"insert into songs(singer, song, album) values ('%s', '%s', '%s')" % (MySQLdb.escape_string(music[0]), MySQLdb.escape_string(music[1]), MySQLdb.escape_string(music[2])) 下面附上相关的代码段： 123456789101112db = MySQLdb.connect(MYSQL_HOST, MYSQL_USER, MYSQL_PWD, MYSQL_DB, charset='utf8')cursor = db.cursor()for music in music_set: try: cursor.execute("insert into songs(singer, song, album) values ('%s', '%s', '%s')" % (MySQLdb.escape_string(music[0]), MySQLdb.escape_string(music[1]), MySQLdb.escape_string(music[2]))) db.commit() except Exception, e: print edb.close()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
